From ajk@mds.rmit.edu.au  Sat Mar  2 02:39:30 2002
From: ajk@mds.rmit.edu.au (Alan Kent)
Date: Sat, 2 Mar 2002 13:39:30 +1100
Subject: [OAI-implementers] <datestamp> format please
Message-ID: <20020302133930.K1467@io.mds.rmit.edu.au>

I am having trouble working out the legal format for <datestamp> from
the spec. The spec says in requests use YYYY-MM-DD and in responses
use the ISO format with a time as well. <datestamp> is in a reponse
but seems to use the request format in all the examples.

sceti returns '2001-09-28T11:27:13-04:00' (ISO format).

scout return '1997-09-17 00:00:00' which does not match any format.

How forgiving should I be on date formats? At present I mark the
server as bad and ignore it from then on. I can be more forgiving,
but are people more interested in fixing their problems?

Thanks,
Alan

From ajk@mds.rmit.edu.au  Sat Mar  2 04:07:22 2002
From: ajk@mds.rmit.edu.au (Alan Kent)
Date: Sat, 2 Mar 2002 15:07:22 +1100
Subject: [OAI-implementers] New OAI crawl results in progress...
Message-ID: <20020302150722.L1467@io.mds.rmit.edu.au>

I have started up my new OAI crawler which does a few more sanity
checks on data formats etc, automatically disables crawls from
bad sites, and so on. The results page has moved (but is linked
from the old manually generated page). The new page is:

    http://www.mds.rmit.edu.au/~ajk/oai/interop/summary.htm

If you site has been disbled, its because my end things you did
something wrong. If you click on the link for your repository name,
it will take you to a log of status messages and errors.

At present, I regenerate the HTML pages using a script I run by
hand so the results might not be 100% up to date when you view
them. But they should be pretty accurate.

I am now doing things like correctly supporting aggregators
(sites where records coming back do not belong to that site).
For example, I think the 'anu' site returns 'caltechEERL'
records - even though I don't have any details about such
a repository.

After the crawler is a bit more robust, I may move the whole database
outside our firewall so others can query it.

Alan
-- 
Alan Kent (mailto:ajk@mds.rmit.edu.au, http://www.mds.rmit.edu.au)
Postal: Multimedia Database Systems, RMIT, GPO Box 2476V, Melbourne 3001.
Where: RMIT MDS, Bld 91, Level 3, 110 Victoria St, Carlton 3053, VIC Australia.
Phone: +61 3 9925 4114  Reception: +61 3 9925 4099  Fax: +61 3 9925 4098 

From ajk@mds.rmit.edu.au  Sat Mar  2 09:04:13 2002
From: ajk@mds.rmit.edu.au (Alan Kent)
Date: Sat, 2 Mar 2002 20:04:13 +1100
Subject: [OAI-implementers] New OAI crawl results in progress...
In-Reply-To: <20020302150722.L1467@io.mds.rmit.edu.au>; from Alan Kent on Sat, Mar 02, 2002 at 03:07:22PM +1100
References: <20020302150722.L1467@io.mds.rmit.edu.au>
Message-ID: <20020302200412.S1467@io.mds.rmit.edu.au>

On Sat, Mar 02, 2002 at 03:07:22PM +1100, Alan Kent wrote:
> I have started up my new OAI crawler ... The new page is:
> 
>     http://www.mds.rmit.edu.au/~ajk/oai/interop/summary.htm

Its got through most sites now. In case you are interested in
a quick summary, I failed to talk to the following sites:

    8657690236
    AIM25
    aisri
    anlc
    CEIAT
    celebration
    cimi
    citebase
    cogprints
    conoze
    CPS
    dfki
    dlpscoll
    EKUTuebingen
    ELibBSU
    elra
    ENUMERATE
    ethnologue
    hsss
    ibiblio
    in2p3
    lacito
    LDC
    mit.etheses
    ndltd
    NSDL-DEV-CU
    ota
    perseus
    RIACS
    sceti
    scout
    SUUB
    tkn
    UDLAthesis
    USF
    yea

The good news is, that while the above 36 sites failed for various reasons,
there were another 39 sites that worked. (I will let other people decide
whether a 52% success rate is really that good!)

Alan

From ajk@mds.rmit.edu.au  Sat Mar  2 09:25:57 2002
From: ajk@mds.rmit.edu.au (Alan Kent)
Date: Sat, 2 Mar 2002 20:25:57 +1100
Subject: [OAI-implementers] Friends and Neighbours scheme ... locating OAI repositories
Message-ID: <20020302202557.T1467@io.mds.rmit.edu.au>

Another mailing list I am on related to Z39.50 is talking about the
concept of "Friends and Neighbours". It may be a bit early for
OAI (not that many sites yet!), but the basic idea is that sites
keep track of what other sites they know of. That way, there does
not need to be a central registry as such. Theoretically someone
can write a crawler, start at one site, find out what other
sites it knows of, and work its way on from there.

This allows major institutions with a real commitment to OAI to
maintain a list of other sites under their juristiction, with links
off to other places where similar lists are kept.

One way this could be done in OAI for example is to define a new
XML schema for information on repositories. You then ask your first
site (you have to be given a starting point) what sites it knows of.
It will probably return itself (not mandatory for simple data provider
implementations), but it may return other sites it knows of.
The crawler can then go off and ask those sites what sites they know
of, and so on.

Two things are needed to achieve this:

(1) A definition of a new schema describing a OAI site. Probably only
    needs repository id, BaseURL, and protocol version. The Identify
    verb can be used on the site to get other information.

(2) One or more of the data provider software implementors to support
    more than one record syntax (ie, not only dublin core).
    
I am assuming that in OAI if you ask for data in a particular record
schema, you will only get records that were in that syntax.

It seemed a better way of getting an up to date list of sites compared
to the current ListFriends.pl XML document at the www.openarchives.org
site. It allows the OAI protocol to maintain lists of OAI sites
in a distributed way. Aggregators just aggregate OAI site records
like any other record they would aggregate.

I thought it was a cute idea.

Alan
-- 
Alan Kent (mailto:ajk@mds.rmit.edu.au, http://www.mds.rmit.edu.au)
Postal: Multimedia Database Systems, RMIT, GPO Box 2476V, Melbourne 3001.
Where: RMIT MDS, Bld 91, Level 3, 110 Victoria St, Carlton 3053, VIC Australia.
Phone: +61 3 9925 4114  Reception: +61 3 9925 4099  Fax: +61 3 9925 4098 

From hvdsomp@yahoo.com  Sat Mar  2 09:26:44 2002
From: hvdsomp@yahoo.com (herbert van de sompel)
Date: Sat, 02 Mar 2002 09:26:44 +0000
Subject: [OAI-implementers] OAI 2.0? (XML namespaces change request)
References: <20020301100527.B12806@io.mds.rmit.edu.au>
Message-ID: <3C809AD4.A8DAD867@yahoo.com>

hi Alan,

Consider it done.  There will be a single namespace for OAI-PMH version 2.0 .

This issue has come up before, and now is the time to address it
appropriately.

BTW: release date of version 2.0 is expected May 1st 2002.

Thanks,

herbert van de sompel

Alan Kent wrote:

> Is OAI 2.0 still planned to be released end of March (or is my
> memory faulty)? I am not sure if this is the forum for change
> requests, but I have one hopefully minor thing I would like to
> see changed. This may be a known issue, but I just hit it in
> some code I was writing, so thought I would mail again just
> in case.
>
> At present, a separate namespace is defined per verb (GetRecords,
> ListRecords, etc). Could this be changed for one namespace for
> an OAI version? I can see no benefit in having one namespace
> per verb, and there are real disadvantages when tryiing to
> write namespace aware code.
>
> At the risk of preaching to the knowledgable, when you use a
> namespace aware tool, the element name is not just the name
> in the tag (<record>, <metadata> etc), its the element name
> (local name) plus the namespace name. For convenience, some
> people write this as {http://www.openarchives.org/OAI/1.1/GetRecord}record
> (I might have the namespace wrong, but you get the idea).
>
> I am in the process of reworking my harvester code based on my
> better knoweldge and experience with OAI. It turns out my old
> program discarded some information that was important - so I
> have to recrawl everything <:-(. Oh well. So the new code I
> am trying to do correctly with namespaces etc.
>
> The problem I am hitting is that all the full names of elements
> change depending on whether its a GetRecord request or a ListRecords
> request. I cannot write generic code (easily) to process a
> <record> element, because the name is one of
>
>     {http://www.openarchives.org/OAI/1.1/GetRecord}record
>     {http://www.openarchives.org/OAI/1.1/ListRecords}record
>
> It is not correct to ignore the namespace name, so everywhere
> I check element names, I have to check for multiple element names.
>
> With OAI 2.0 coming along, it would make life easier if a single
> namespace URI was used for the whole protocol meaning the <record>
> full name in both GetRecord and ListRecords would become
>
>     {http://www.openarchives.org/OAI/2.0/}record
>
> Thanks!
> Alan
> --
> Alan Kent (mailto:ajk@mds.rmit.edu.au, http://www.mds.rmit.edu.au)
> Postal: Multimedia Database Systems, RMIT, GPO Box 2476V, Melbourne 3001.
> Where: RMIT MDS, Bld 91, Level 3, 110 Victoria St, Carlton 3053, VIC
> Australia.
> Phone: +61 3 9925 4114  Reception: +61 3 9925 4099  Fax: +61 3 9925 4098
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
>
> *********************************************************************
> The information contained in this e-mail is confidential and may be
> legally privileged. It is intended for the addressee(s) only. If you
> are not the intended recipient, please delete this e-mail and notify
> the postmaster@bl.uk : The contents of this e-mail must not be
> disclosed or copied without the sender's consent.
>
> The statements and opinions expressed in this message are those of
> the author and do not necessarily reflect those of the British
> Library. The British Library does not take any responsibility for
> the views of the author.
> *********************************************************************


_________________________________________________________
Do You Yahoo!?
Get your free @yahoo.com address at http://mail.yahoo.com


From hvdsomp@yahoo.com  Sat Mar  2 09:32:34 2002
From: hvdsomp@yahoo.com (herbert van de sompel)
Date: Sat, 02 Mar 2002 09:32:34 +0000
Subject: [OAI-implementers] <datestamp> format please
References: <20020302133930.K1467@io.mds.rmit.edu.au>
Message-ID: <3C809C32.120019FC@yahoo.com>

Alan,

Alan Kent wrote:

> I am having trouble working out the legal format for <datestamp> from
> the spec. The spec says in requests use YYYY-MM-DD and in responses
> use the ISO format with a time as well. <datestamp> is in a reponse
> but seems to use the request format in all the examples.

datestamps in 1.1 are YYYY-MM-DD, which is ISO8601
responseDates in 1.1 are YYYY-MM-DDThh:mm:ssTZD which is ISO8601

>
> sceti returns '2001-09-28T11:27:13-04:00' (ISO format).

correct

> scout return '1997-09-17 00:00:00' which does not match any format.

incorrect

> How forgiving should I be on date formats? At present I mark the
> server as bad and ignore it from then on. I can be more forgiving,
> but are people more interested in fixing their problems?

The above problem is one of those instances where it should be trivial
for a repository to make a fix.  Why not just send an e-mail to the
admin, and report the problem?

greetings

herbert


_________________________________________________________
Do You Yahoo!?
Get your free @yahoo.com address at http://mail.yahoo.com


From ajk@mds.rmit.edu.au  Sat Mar  2 09:43:20 2002
From: ajk@mds.rmit.edu.au (Alan Kent)
Date: Sat, 2 Mar 2002 20:43:20 +1100
Subject: [OAI-implementers] <datestamp> format please
In-Reply-To: <3C809C32.120019FC@yahoo.com>; from herbert van de sompel on Sat, Mar 02, 2002 at 09:32:34AM +0000
References: <20020302133930.K1467@io.mds.rmit.edu.au> <3C809C32.120019FC@yahoo.com>
Message-ID: <20020302204320.V1467@io.mds.rmit.edu.au>

On Sat, Mar 02, 2002 at 09:32:34AM +0000, herbert van de sompel wrote:
> datestamps in 1.1 are YYYY-MM-DD, which is ISO8601
> responseDates in 1.1 are YYYY-MM-DDThh:mm:ssTZD which is ISO8601
> 
> > sceti returns '2001-09-28T11:27:13-04:00' (ISO format).
> 
> correct

So sceti is incorrect since the above was returned in a <datestamp>
element (not responseDates) and <datestamp> requires only YYYY-MM-DD?

> The above problem is one of those instances where it should be trivial
> for a repository to make a fix.  Why not just send an e-mail to the
> admin, and report the problem?

I wanted to make sure who was wrong first! :-)

The spec is ambiguous because it says 'all dates in response packets
are ISO date/time strings', but the <datestamp> element in a record is
not an ISO date/time stamp - just the date. So the spec should have
said responseDates are in the ISO format (ie, been more specific).

Alan

From hvdsomp@yahoo.com  Sat Mar  2 09:39:41 2002
From: hvdsomp@yahoo.com (herbert van de sompel)
Date: Sat, 02 Mar 2002 09:39:41 +0000
Subject: [OAI-implementers] New OAI crawl results in progress...
References: <20020302150722.L1467@io.mds.rmit.edu.au>
Message-ID: <3C809DDD.57E21DB@yahoo.com>

Alan,

This looks like wonderful work!  A significant addition to the toolset
available for repositories to inspect their implementations.

greetings

herbert van de sompel

Alan Kent wrote:

> I have started up my new OAI crawler which does a few more sanity
> checks on data formats etc, automatically disables crawls from
> bad sites, and so on. The results page has moved (but is linked
> from the old manually generated page). The new page is:
>
>     http://www.mds.rmit.edu.au/~ajk/oai/interop/summary.htm
>
> If you site has been disbled, its because my end things you did
> something wrong. If you click on the link for your repository name,
> it will take you to a log of status messages and errors.
>
> At present, I regenerate the HTML pages using a script I run by
> hand so the results might not be 100% up to date when you view
> them. But they should be pretty accurate.
>
> I am now doing things like correctly supporting aggregators
> (sites where records coming back do not belong to that site).
> For example, I think the 'anu' site returns 'caltechEERL'
> records - even though I don't have any details about such
> a repository.
>
> After the crawler is a bit more robust, I may move the whole database
> outside our firewall so others can query it.
>
> Alan
> --
> Alan Kent (mailto:ajk@mds.rmit.edu.au, http://www.mds.rmit.edu.au)
> Postal: Multimedia Database Systems, RMIT, GPO Box 2476V, Melbourne 3001.
> Where: RMIT MDS, Bld 91, Level 3, 110 Victoria St, Carlton 3053, VIC
> Australia.
> Phone: +61 3 9925 4114  Reception: +61 3 9925 4099  Fax: +61 3 9925 4098
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
>
> *********************************************************************
> The information contained in this e-mail is confidential and may be
> legally privileged. It is intended for the addressee(s) only. If you
> are not the intended recipient, please delete this e-mail and notify
> the postmaster@bl.uk : The contents of this e-mail must not be
> disclosed or copied without the sender's consent.
>
> The statements and opinions expressed in this message are those of
> the author and do not necessarily reflect those of the British
> Library. The British Library does not take any responsibility for
> the views of the author.
> *********************************************************************


_________________________________________________________
Do You Yahoo!?
Get your free @yahoo.com address at http://mail.yahoo.com


From hvdsomp@yahoo.com  Sat Mar  2 09:53:38 2002
From: hvdsomp@yahoo.com (herbert van de sompel)
Date: Sat, 02 Mar 2002 09:53:38 +0000
Subject: [OAI-implementers] Friends and Neighbours scheme ... locating
 OAIrepositories
References: <20020302202557.T1467@io.mds.rmit.edu.au>
Message-ID: <3C80A122.7E0CB232@yahoo.com>

Alan,

I have proposed the introduction of a ListFriends verb, which was supposed to
do exactly what you describe, for inclusion in version 2.0 of the OAI-PMH.  I
felt it would have been an interesting addition to the spec because it
supported service providers in locating data poviders in a non-centralised
manner.    However, the OAI-Tech committee has decided not to pursue it,
mainly because we could not figure out what the incentives would be for a
repository to do the additional work of keeping a list of "befriended" repos.

I wrote a white paper on it, in October 2001.  If you are interested please
send me a private mail.

greetings

herbert van de sompel





Alan Kent wrote:

> Another mailing list I am on related to Z39.50 is talking about the
> concept of "Friends and Neighbours". It may be a bit early for
> OAI (not that many sites yet!), but the basic idea is that sites
> keep track of what other sites they know of. That way, there does
> not need to be a central registry as such. Theoretically someone
> can write a crawler, start at one site, find out what other
> sites it knows of, and work its way on from there.
>
> This allows major institutions with a real commitment to OAI to
> maintain a list of other sites under their juristiction, with links
> off to other places where similar lists are kept.
>
> One way this could be done in OAI for example is to define a new
> XML schema for information on repositories. You then ask your first
> site (you have to be given a starting point) what sites it knows of.
> It will probably return itself (not mandatory for simple data provider
> implementations), but it may return other sites it knows of.
> The crawler can then go off and ask those sites what sites they know
> of, and so on.
>
> Two things are needed to achieve this:
>
> (1) A definition of a new schema describing a OAI site. Probably only
>     needs repository id, BaseURL, and protocol version. The Identify
>     verb can be used on the site to get other information.
>
> (2) One or more of the data provider software implementors to support
>     more than one record syntax (ie, not only dublin core).
>
> I am assuming that in OAI if you ask for data in a particular record
> schema, you will only get records that were in that syntax.
>
> It seemed a better way of getting an up to date list of sites compared
> to the current ListFriends.pl XML document at the www.openarchives.org
> site. It allows the OAI protocol to maintain lists of OAI sites
> in a distributed way. Aggregators just aggregate OAI site records
> like any other record they would aggregate.
>
> I thought it was a cute idea.
>
> Alan
> --
> Alan Kent (mailto:ajk@mds.rmit.edu.au, http://www.mds.rmit.edu.au)
> Postal: Multimedia Database Systems, RMIT, GPO Box 2476V, Melbourne 3001.
> Where: RMIT MDS, Bld 91, Level 3, 110 Victoria St, Carlton 3053, VIC
> Australia.
> Phone: +61 3 9925 4114  Reception: +61 3 9925 4099  Fax: +61 3 9925 4098
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
>
> *********************************************************************
> The information contained in this e-mail is confidential and may be
> legally privileged. It is intended for the addressee(s) only. If you
> are not the intended recipient, please delete this e-mail and notify
> the postmaster@bl.uk : The contents of this e-mail must not be
> disclosed or copied without the sender's consent.
>
> The statements and opinions expressed in this message are those of
> the author and do not necessarily reflect those of the British
> Library. The British Library does not take any responsibility for
> the views of the author.
> *********************************************************************


_________________________________________________________
Do You Yahoo!?
Get your free @yahoo.com address at http://mail.yahoo.com


From ajk@mds.rmit.edu.au  Sat Mar  2 10:44:51 2002
From: ajk@mds.rmit.edu.au (Alan Kent)
Date: Sat, 2 Mar 2002 21:44:51 +1100
Subject: [OAI-implementers] Friends and Neighbours scheme ... locating  OAIrepositories
In-Reply-To: <Pine.SGI.3.95.1020302104106.13656C-100000@coglit.ecs.soton.ac.uk>; from Stevan Harnad on Sat, Mar 02, 2002 at 10:44:50AM +0000
References: <3C80A122.7E0CB232@yahoo.com> <Pine.SGI.3.95.1020302104106.13656C-100000@coglit.ecs.soton.ac.uk>
Message-ID: <20020302214451.W1467@io.mds.rmit.edu.au>

On Sat, Mar 02, 2002 at 10:44:50AM +0000, Stevan Harnad wrote:
> (1) Archives have been remiss in registering themselves; this may make a
> registry unnecessary, or automatic.

Registry is still required - at least one other person must know of
your existance. It only saves having to go to a central place for
registration. Whether this is good or bad is not yet clear to me
since OAI wants globally unique repository identifiers (if they are
not registered centrally, how to make sure they are globally unique?)

Another cute thought I had was that maybe a new schema is not required.
Maybe just use the current Identify reponse as a record schema: oai_Identify.

Alan

From hvdsomp@yahoo.com  Fri Mar  1 17:10:04 2002
From: hvdsomp@yahoo.com (herbert van de sompel)
Date: Fri, 01 Mar 2002 17:10:04 +0000
Subject: [OAI-implementers] OAI 2.0? (XML namespaces change request)
References: <20020301100527.B12806@io.mds.rmit.edu.au>
Message-ID: <3C7FB5EC.682F93BC@yahoo.com>

hi Alan,

Consider it done.  There will be a single namespace for OAI-PMH version 2.0 .

This issue has come up before, and now is the time to address it
appropriately.

BTW: release date of version 2.0 is expected May 1st 2002.

Thanks,

herbert van de sompel

Alan Kent wrote:

> Is OAI 2.0 still planned to be released end of March (or is my
> memory faulty)? I am not sure if this is the forum for change
> requests, but I have one hopefully minor thing I would like to
> see changed. This may be a known issue, but I just hit it in
> some code I was writing, so thought I would mail again just
> in case.
>
> At present, a separate namespace is defined per verb (GetRecords,
> ListRecords, etc). Could this be changed for one namespace for
> an OAI version? I can see no benefit in having one namespace
> per verb, and there are real disadvantages when tryiing to
> write namespace aware code.
>
> At the risk of preaching to the knowledgable, when you use a
> namespace aware tool, the element name is not just the name
> in the tag (<record>, <metadata> etc), its the element name
> (local name) plus the namespace name. For convenience, some
> people write this as {http://www.openarchives.org/OAI/1.1/GetRecord}record
> (I might have the namespace wrong, but you get the idea).
>
> I am in the process of reworking my harvester code based on my
> better knoweldge and experience with OAI. It turns out my old
> program discarded some information that was important - so I
> have to recrawl everything <:-(. Oh well. So the new code I
> am trying to do correctly with namespaces etc.
>
> The problem I am hitting is that all the full names of elements
> change depending on whether its a GetRecord request or a ListRecords
> request. I cannot write generic code (easily) to process a
> <record> element, because the name is one of
>
>     {http://www.openarchives.org/OAI/1.1/GetRecord}record
>     {http://www.openarchives.org/OAI/1.1/ListRecords}record
>
> It is not correct to ignore the namespace name, so everywhere
> I check element names, I have to check for multiple element names.
>
> With OAI 2.0 coming along, it would make life easier if a single
> namespace URI was used for the whole protocol meaning the <record>
> full name in both GetRecord and ListRecords would become
>
>     {http://www.openarchives.org/OAI/2.0/}record
>
> Thanks!
> Alan
> --
> Alan Kent (mailto:ajk@mds.rmit.edu.au, http://www.mds.rmit.edu.au)
> Postal: Multimedia Database Systems, RMIT, GPO Box 2476V, Melbourne 3001.
> Where: RMIT MDS, Bld 91, Level 3, 110 Victoria St, Carlton 3053, VIC
> Australia.
> Phone: +61 3 9925 4114  Reception: +61 3 9925 4099  Fax: +61 3 9925 4098
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
>
> *********************************************************************
> The information contained in this e-mail is confidential and may be
> legally privileged. It is intended for the addressee(s) only. If you
> are not the intended recipient, please delete this e-mail and notify
> the postmaster@bl.uk : The contents of this e-mail must not be
> disclosed or copied without the sender's consent.
>
> The statements and opinions expressed in this message are those of
> the author and do not necessarily reflect those of the British
> Library. The British Library does not take any responsibility for
> the views of the author.
> *********************************************************************


_________________________________________________________
Do You Yahoo!?
Get your free @yahoo.com address at http://mail.yahoo.com


From Steven Bird <sb@ldc.upenn.edu>  Sat Mar  2 19:54:57 2002
From: Steven Bird <sb@ldc.upenn.edu> (Steven Bird)
Date: Sat, 02 Mar 2002 14:54:57 EST
Subject: [OAI-implementers] Friends and Neighbours scheme ... locating OAIrepositories
In-Reply-To: Your mail dated Saturday 2 March, 2002.
Message-ID: <200203021954.g22Jswb13084@unagi.cis.upenn.edu>

Alan Kent wrote:
> Registry is still required - at least one other person must know of
> your existance. It only saves having to go to a central place for
> registration. Whether this is good or bad is not yet clear to me
> since OAI wants globally unique repository identifiers (if they are
> not registered centrally, how to make sure they are globally unique?)

I favor a central registry to control the oai:... namespace.

How else can a data provider be sure of controlling their own oai:xyz:...
namespace, and avoid collisions (accidental or intentional) on oai
identifiers?  The chief incentive for data providers to register would be
to protect their own identifier namespace.

The central registry would also list which registered data providers are
valid (and for which protocol version), both as a service to the service
providers, and as a stimulus to data providers to be correct and
up-to-date.

-Steven

--
Steven.Bird@ldc.upenn.edu  http://www.ldc.upenn.edu/sb
Assoc Director, LDC; Adj Assoc Prof, CIS & Linguistics
Linguistic Data Consortium, University of Pennsylvania
3615 Market St, Suite 200, Philadelphia, PA 19104-2608



From liu_x@cs.odu.edu  Sun Mar  3 02:23:23 2002
From: liu_x@cs.odu.edu (Xiaoming Liu)
Date: Sat, 2 Mar 2002 21:23:23 -0500 (EST)
Subject: [OAI-implementers] Friends and Neighbours scheme ... locating
 OAI repositories
In-Reply-To: <20020302202557.T1467@io.mds.rmit.edu.au>
Message-ID: <Pine.SOL.4.10.10203022043110.29289-100000@defiant.cs.odu.edu>

Several replies combined in one message.

On Sat, 2 Mar 2002, Alan Kent wrote:

> Another mailing list I am on related to Z39.50 is talking about the
> concept of "Friends and Neighbours". It may be a bit early for
> OAI (not that many sites yet!), but the basic idea is that sites
> keep track of what other sites they know of. That way, there does

This looks very like the concepts in Peer-to-Peer, especially Gnutella.
But I am not sure how well it fits in here. In Gnutella/Fasttrack, people
want to get a music file, they don't care  much about how precise the
result is, as long as they can get something. But here we may want a
definite answer, like: how many OAI data providers and what contents they
share so far?  

Another way is to expose the list of data provider in an OAI interface,
that's what we did in Arc:

http://arc.cs.odu.edu:8080/oaisp/servlet/OAI-SP?verb=ListRecords&from=&until=&set=&metadataPrefix=oai_dc

This might be an easier way to implement a friend/neighbor functions. For
a data provider, it might support two OAI interface, one for naming
service, another for real data.

On Sat, 2 Mar 2002, Steven Bird wrote:
> I favor a central registry to control the oai:... namespace.  

I agree a strong central control will be helpful, considering somebody
wants to build a service provider, he really needs a point to start with.
Of course, that won't prevent any internal use. 

On Sat, 2 Mar 2002, herbert van de sompel wrote:
> manner.    However, the OAI-Tech committee has decided not to pursue it,
> mainly because we could not figure out what the incentives would be for
>a repository to do the additional work of keeping a list of "befriended"
> repos.

One incentives will be the peer-2-peer like Kepler ;-), in the scenario of
Kepler, each client is very unstable, so their contents must be cached in
somewhere, it might be in the service provider side, like we did now. Or
it might be distributed to its neighbor or anybody who is more stable or
has better Internet communcation. So it would be helpful if such a list
of "befriended" exists. That's the the approach we are exploring.

Another incentives will be kind of mirror problem like oaia, for one
service provider, it doesn't want to harvest same records multiple times
from different data providers. So one way to identify the duplication is
necessary, I believe that's also where a "befriended" verb will play.


Regards,
liu


From ajk@mds.rmit.edu.au  Mon Mar  4 04:14:44 2002
From: ajk@mds.rmit.edu.au (Alan Kent)
Date: Mon, 4 Mar 2002 15:14:44 +1100
Subject: [OAI-implementers] How to verify a download worked?
Message-ID: <20020304151444.M1467@io.mds.rmit.edu.au>

Hi All,

I was wondering if anyone has good schemes for verifying if a download
of metadata 'worked'. For example, I crawled the arXiv site and got
around 60,000 records. However, it turns out the site actually has
190,000 or so records. So I only got 1/3 of the site!

Has anyone used any clever tricks to verify how well a crawl worked?
I now have to work out if my crawler has been discarding one in three
records! :-(

Alan
-- 
Alan Kent (mailto:ajk@mds.rmit.edu.au, http://www.mds.rmit.edu.au)
Postal: Multimedia Database Systems, RMIT, GPO Box 2476V, Melbourne 3001.
Where: RMIT MDS, Bld 91, Level 3, 110 Victoria St, Carlton 3053, VIC Australia.
Phone: +61 3 9925 4114  Reception: +61 3 9925 4099  Fax: +61 3 9925 4098 

From simeon@cs.cornell.edu  Mon Mar  4 15:41:58 2002
From: simeon@cs.cornell.edu (Simeon Warner)
Date: Mon, 4 Mar 2002 10:41:58 -0500 (EST)
Subject: [OAI-implementers] How to verify a download worked?
In-Reply-To: <20020304151444.M1467@io.mds.rmit.edu.au>
Message-ID: <Pine.LNX.4.44.0203040952560.22277-100000@ice.cs.cornell.edu>

Alan,

The reason you get just 60k records from arXiv is probably linked with the
problem of specifying a date too early for my implementation to understand
correctly (now fixed, someone else pointed it out last week too). I don't
know about ways to verify successful harvesting but I would suggest that
doing a harvest with no 'from' and 'until' parameters is more robust than
picking an arbitrary 'from' date.

Cheers,
Simeon.

On Mon, 4 Mar 2002, Alan Kent wrote:
> Hi All,
> 
> I was wondering if anyone has good schemes for verifying if a download
> of metadata 'worked'. For example, I crawled the arXiv site and got
> around 60,000 records. However, it turns out the site actually has
> 190,000 or so records. So I only got 1/3 of the site!
> 
> Has anyone used any clever tricks to verify how well a crawl worked?
> I now have to work out if my crawler has been discarding one in three
> records! :-(
> 
> Alan
> 



From hussein@vt.edu  Mon Mar  4 15:52:17 2002
From: hussein@vt.edu (Hussein Suleman)
Date: Mon, 04 Mar 2002 10:52:17 -0500
Subject: [OAI-implementers] How to verify a download worked?
References: <Pine.LNX.4.44.0203040952560.22277-100000@ice.cs.cornell.edu>
Message-ID: <3C839831.4080700@vt.edu>

hi

also, i should mention that we expect OAI-PMH v2.0 will have a "full 
list size" field of sorts that will let you know how many records there 
are in the full set.

ttfn
----hussein

Simeon Warner wrote:

> Alan,
> 
> The reason you get just 60k records from arXiv is probably linked with the
> problem of specifying a date too early for my implementation to understand
> correctly (now fixed, someone else pointed it out last week too). I don't
> know about ways to verify successful harvesting but I would suggest that
> doing a harvest with no 'from' and 'until' parameters is more robust than
> picking an arbitrary 'from' date.
> 
> Cheers,
> Simeon.
> 
> On Mon, 4 Mar 2002, Alan Kent wrote:
> 
>>Hi All,
>>
>>I was wondering if anyone has good schemes for verifying if a download
>>of metadata 'worked'. For example, I crawled the arXiv site and got
>>around 60,000 records. However, it turns out the site actually has
>>190,000 or so records. So I only got 1/3 of the site!
>>
>>Has anyone used any clever tricks to verify how well a crawl worked?
>>I now have to work out if my crawler has been discarding one in three
>>records! :-(
>>
>>Alan
>>
>>
> 
> 
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 


-- 
======================================================================
hussein suleman - hussein@vt.edu - vtcs - http://www.husseinsspace.com
======================================================================


From mln@ils.unc.edu  Mon Mar  4 22:43:37 2002
From: mln@ils.unc.edu (Michael L. Nelson)
Date: Mon, 4 Mar 2002 17:43:37 -0500 (EST)
Subject: [OAI-implementers] Friends and Neighbours scheme ... locating
 OAI repositories
In-Reply-To: <Pine.SOL.4.10.10203022043110.29289-100000@defiant.cs.odu.edu>
Message-ID: <Pine.GSO.4.21.0203041734080.17873-100000@ruby.ils.unc.edu>

> > Another mailing list I am on related to Z39.50 is talking about the
> > concept of "Friends and Neighbours". It may be a bit early for
> > OAI (not that many sites yet!), but the basic idea is that sites
> > keep track of what other sites they know of. That way, there does
> 
> This looks very like the concepts in Peer-to-Peer, especially Gnutella.
> But I am not sure how well it fits in here. In Gnutella/Fasttrack, people
> want to get a music file, they don't care  much about how precise the
> result is, as long as they can get something. But here we may want a
> definite answer, like: how many OAI data providers and what contents they
> share so far?  
> 

not necessarily...  the original purpose of "friends" was that it would
allow a loosely connected set of DPs w/o having to use a central
registery.  sort of like a bunch of http servers that have html pages that
point to each other.  and "friends" is very DP-centric:  DPs get to decide
who to list as their friends.

for example, the NASA DPs might list each other as "friends", and maybe
some of the other labs & institutes that they share data with (e.g., LANL)
or are affiliated with (e.g., RIACS).  a SP that finds one of these DPs
should be able to get a rough idea of the DPs that cooperate by design,
essentially inferring the "community" of federal research lab DPs.  apply
the standard hubs/authorities, etc. stuff here too.

"ListFriends" did not make it as a verb in OAI 2.0; but perhaps it could
be experimented with using specials sets: "oai_friends" or something
equally mnemonic.  or maybe just a separate metadata format (oai_friends):

http://www.foo.org/oai/?verb=ListRecords&metadataPrefix=oai_friends

its not hard to imagine a simple schema that delineates
who you're pointing to.  

> Another way is to expose the list of data provider in an OAI interface,
> that's what we did in Arc:
> 
> http://arc.cs.odu.edu:8080/oaisp/servlet/OAI-SP?verb=ListRecords&from=&until=&set=&metadataPrefix=oai_dc
> 

yeah, a separate interface is one way to do it too.  

> This might be an easier way to implement a friend/neighbor functions. For
> a data provider, it might support two OAI interface, one for naming
> service, another for real data.
> 
> On Sat, 2 Mar 2002, Steven Bird wrote:
> > I favor a central registry to control the oai:... namespace.  
> 
> I agree a strong central control will be helpful, considering somebody
> wants to build a service provider, he really needs a point to start with.
> Of course, that won't prevent any internal use. 
> 

yeah, a quick browse through Arc reveals that  most of the archive names
don't mean much, and there is going to be collision of obvious acronyms /
names.

> 
> Regards,
> liu
> 

---
Michael L. Nelson
NASA Langley Research Center		m.l.nelson@larc.nasa.gov
MS 158, Hampton, VA 23681		http://www.ils.unc.edu/~mln/
+1 757 864 8511				+1 757 864 8342 (f)


From harnad@cogprints.soton.ac.uk  Sat Mar  2 10:44:50 2002
From: harnad@cogprints.soton.ac.uk (Stevan Harnad)
Date: Sat, 2 Mar 2002 10:44:50 +0000 (GMT)
Subject: [OAI-implementers] Friends and Neighbours scheme ... locating  OAIrepositories
In-Reply-To: <3C80A122.7E0CB232@yahoo.com>
Message-ID: <Pine.SGI.3.95.1020302104106.13656C-100000@coglit.ecs.soton.ac.uk>

On Sat, 2 Mar 2002, herbert van de sompel wrote:

> I have proposed the introduction of a ListFriends verb, which was supposed to
> do exactly what you describe, for inclusion in version 2.0 of the OAI-PMH.  I
> felt it would have been an interesting addition to the spec because it
> supported service providers in locating data poviders in a non-centralised
> manner.    However, the OAI-Tech committee has decided not to pursue it,
> mainly because we could not figure out what the incentives would be for a
> repository to do the additional work of keeping a list of "befriended" repos.
> 
> I wrote a white paper on it, in October 2001.  If you are interested please
> send me a private mail.

Here are a few incentives, in the context of distributed Eprint Archives
for disclosing institutions' full-text refereed research output.

(1) Archives have been remiss in registering themselves; this may make a
registry unnecessary, or automatic.

(2) Potential archivers and users have expressed concerns about how they
can be sure that search engines will pick up all and only the contents
of full-text refereed research archives like themselves.

(3) Such distributed archives will need interconnectivity for backup
(twinning, mirroring, upgrades), citation-linking, and version control.
For all this it would help if they all systematically knew of and
tracked one another.

My technical grasp is not sufficient for me to judge whether any or all
of these objectives can be or are being realized in other ways too,
but I mention them just in case.

Stevan Harnad

> Alan Kent wrote:
> 
> > Another mailing list I am on related to Z39.50 is talking about the
> > concept of "Friends and Neighbours". It may be a bit early for
> > OAI (not that many sites yet!), but the basic idea is that sites
> > keep track of what other sites they know of. That way, there does
> > not need to be a central registry as such. Theoretically someone
> > can write a crawler, start at one site, find out what other
> > sites it knows of, and work its way on from there.
> >
> > This allows major institutions with a real commitment to OAI to
> > maintain a list of other sites under their juristiction, with links
> > off to other places where similar lists are kept.
> >
> > One way this could be done in OAI for example is to define a new
> > XML schema for information on repositories. You then ask your first
> > site (you have to be given a starting point) what sites it knows of.
> > It will probably return itself (not mandatory for simple data provider
> > implementations), but it may return other sites it knows of.
> > The crawler can then go off and ask those sites what sites they know
> > of, and so on.
> >
> > Two things are needed to achieve this:
> >
> > (1) A definition of a new schema describing a OAI site. Probably only
> >     needs repository id, BaseURL, and protocol version. The Identify
> >     verb can be used on the site to get other information.
> >
> > (2) One or more of the data provider software implementors to support
> >     more than one record syntax (ie, not only dublin core).
> >
> > I am assuming that in OAI if you ask for data in a particular record
> > schema, you will only get records that were in that syntax.
> >
> > It seemed a better way of getting an up to date list of sites compared
> > to the current ListFriends.pl XML document at the www.openarchives.org
> > site. It allows the OAI protocol to maintain lists of OAI sites
> > in a distributed way. Aggregators just aggregate OAI site records
> > like any other record they would aggregate.
> >
> > I thought it was a cute idea.
> >
> > Alan
> > --
> > Alan Kent (mailto:ajk@mds.rmit.edu.au, http://www.mds.rmit.edu.au)
> > Postal: Multimedia Database Systems, RMIT, GPO Box 2476V, Melbourne 3001.
> > Where: RMIT MDS, Bld 91, Level 3, 110 Victoria St, Carlton 3053, VIC
> > Australia.
> > Phone: +61 3 9925 4114  Reception: +61 3 9925 4099  Fax: +61 3 9925 4098
> > _______________________________________________
> > OAI-implementers mailing list
> > OAI-implementers@oaisrv.nsdl.cornell.edu
> > http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> >
> > *********************************************************************
> > The information contained in this e-mail is confidential and may be
> > legally privileged. It is intended for the addressee(s) only. If you
> > are not the intended recipient, please delete this e-mail and notify
> > the postmaster@bl.uk : The contents of this e-mail must not be
> > disclosed or copied without the sender's consent.
> >
> > The statements and opinions expressed in this message are those of
> > the author and do not necessarily reflect those of the British
> > Library. The British Library does not take any responsibility for
> > the views of the author.
> > *********************************************************************
> 
> 
> _________________________________________________________
> Do You Yahoo!?
> Get your free @yahoo.com address at http://mail.yahoo.com
> 
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 


From ajk@mds.rmit.edu.au  Tue Mar  5 09:00:22 2002
From: ajk@mds.rmit.edu.au ('Alan Kent')
Date: Tue, 5 Mar 2002 20:00:22 +1100
Subject: [OAI-implementers] Better resumption mechanism - more important than ever!
Message-ID: <20020305200021.R1467@io.mds.rmit.edu.au>

I just got some mail from Jeff at OCLC talking about ETDCat (hope
you don't mind me quoting some of your mail Jeff). In particular,
he just told me

    ETDCat contains a lot of records (over 4 million), all of
    which currently have the exact same datestamp from the initial load.

He also told me that there were no sets. So basically, its all
or nothing for this site because OAI has no standard way to resume
if a transfer fails.

If this has happened already, I think its likely to occur again.
(That is, one very large database all with the same time stamp.)
So any comments about having a single large collection like this
is beside the point. The point is OAI does not handle it well.

So I would like to resurrect the discussion again if people don't
mind on how to do support restarts. My understanding of the general
feeling so far is

(1) Mandating support is not going to be acceptable

(2) Mandating format of resumption tokens is not going to be acceptable

(3) Mandating resumption tokens be long lifed (eg: can try again the
    following day) is not acceptable

(4) In fact, mandating that resumption tokens be unique (allowing
    a token to be reused twice in quick succession to get the same
    data) is not acceptable

So any proposal needs to be optionally supported.

Question time:

Does anyone else think that this is a major hole in OAI? I personally
do. After trying to crawl sites, things go wrong. The larger the site,
the greater the probability that something will go wrong. The larger
the site, the greater the pain of starting all over again. I do not
think it is practical for anyone to harvest ETDCat if is really got
4,000,000 records. Any fault, and start downloading that 4gb again!
So I feel strongly on this one. In fact, I think this is the most
major problem OAI has.

Do people think its better to reuse resumption tokens for this purpose,
or introduce a different sort of token? ETDCat for example I think
allocates a session id in resumption tokens, meaning they cannot
be reused when the session times out in the server (similar semantics
anyway). This is a reasonable implementation decision to make.
So maybe its better for servers to return an additional token,
which is a <restartToken> which means a client can instead of
specifying from= and to= again, specify restartToken= instead where
the server then automatically works out whatever other parameters
it needs, creates a new session etc internally. The new 'session'
(ListXXX verb) then can use resumptionTokens to manage that new
transfer.

The idea is for a <restartToken> to be long lifed. It may be less
efficient to use than a resumptionToken, but its only purpose is
if the client fails the download. If a server does not support
restartToken, it simply never returns one. Large collections *should*
support restartTokens.

For my harvester, I can then remember (to disk) the restartToken for
every packet I get back, allowing me to recover much more easily
if anything crashes. If restartToken's are too hard for someone
to implement, then they don't. If you have a large data collection
on the other hand, to reduce network load, I think its probably worth
the extra effort of supporting restartTokens.

Any comments? Better suggesions?

Alan

From tim@tim.brody.btinternet.co.uk  Tue Mar  5 13:38:33 2002
From: tim@tim.brody.btinternet.co.uk (Tim Brody)
Date: Tue, 5 Mar 2002 13:38:33 -0000
Subject: [OAI-implementers] Inadequacy of Datestamps + repository IDs
Message-ID: <005a01c1c44b$0ba94b00$6400a8c0@Advocate>

Dear all,

Datestamps:

Following on from Alan's email regarding datestamps, I would add another
datestamp show-stopper:
Federators

My current understanding of how to build hierachical harvesting is to change
the record datestamp to the day of harvest. This means if a repository of
100,000 records is harvested, that will be 100,000 similarly dated records.

May I suggest the following (which would seem in keeping with OAI's aim of
DP flexibility):
Relax "datestamp" to be any positive number (not necessarily exclusive),
which the repository must be capable of applying a "from" and "to" filter.
New and changed records must have a number greater than the last record in
the repository.

If a repository can be required to provide records in "datestamp" order,
resuming from a broken response is simple (continue from last index
inclusive).

As a datestamp is already an ordered number, this won't lose anything, but
will enable repositories where a datestamp makes no sense to provide
incremental harvesting.

repository IDs:

Following from my email a while back, it seems the discussion on UIDing
repositories has come up with central vs. distributed listings. Simeon
Warner responded with:

> I don't see harm in allowing also '-' and '.'. (I wouldn't want to make it
> case insensitive.) However, without some enforceable policy about naming
> (avoiding the need for OAI registration which currently solves the
> uniqueness problem) does this really buy us anything? After all, I could
> use identifiers "http://arXiv.org/abs/hep-th/9901001" and such for arXiv
> but I choose to use the simpler oai scheme "oai:arXiv:hep-th/9901001".

I can't see a flat, short, centralised naming mechanism working in the
long-run. XML namespaces are UIDed by URL (which is guaranteed to be unique
in an honest system). Assuming that bandwidth isn't a problem, then it makes
sense to have identifiers based on some kind of URL structure - why restrict
the length of identifiers?:
oai:arXiv.org/oai1:hep-th/0001001
oai:cogprints.soton.ac.uk/perl/oai:cogprints/00001111

Another thought about naming authorities: What happens if someone sues for
trademark infringement (a univeristy archive on microsoft legal proceedings
called "oai:microsoft/...")? If archive's are based on DNS, they already
have well-established mechanisms for coping with name conflicts.


All the best,
Tim Brody


From mln@ils.unc.edu  Tue Mar  5 15:02:43 2002
From: mln@ils.unc.edu (Michael L. Nelson)
Date: Tue, 5 Mar 2002 10:02:43 -0500 (EST)
Subject: [OAI-implementers] Better resumption mechanism - more important
 than ever!
In-Reply-To: <20020305200021.R1467@io.mds.rmit.edu.au>
Message-ID: <Pine.GSO.4.21.0203050947320.29139-100000@ruby.ils.unc.edu>

actually, the way I see it is the protocol should not be complicated with
additional tokens and such to enforce what ETDCat (and similiarly
large-sized DPs) should do:

1.  partition their collection into sets
2.  use stateless (or very long lived) resumptionTokens

in 2.0, resumptionTokens will have optional attributes, including
"expirationDate", so this will take the guess work out of knowing how long
a resumptionToken will be valid.

IMO, introducing an optional restartToken is no different (from an
implementer's point of view) than making the resumptionToken last a long
time.  

at some point, you (as a harvester) are simply at the mercy of the
repository.  new features in the protocol won't change that.

regards,

Michael

On Tue, 5 Mar 2002, 'Alan Kent' wrote:

> I just got some mail from Jeff at OCLC talking about ETDCat (hope
> you don't mind me quoting some of your mail Jeff). In particular,
> he just told me
> 
>     ETDCat contains a lot of records (over 4 million), all of
>     which currently have the exact same datestamp from the initial load.
> 
> He also told me that there were no sets. So basically, its all
> or nothing for this site because OAI has no standard way to resume
> if a transfer fails.
> 
> If this has happened already, I think its likely to occur again.
> (That is, one very large database all with the same time stamp.)
> So any comments about having a single large collection like this
> is beside the point. The point is OAI does not handle it well.
> 
> So I would like to resurrect the discussion again if people don't
> mind on how to do support restarts. My understanding of the general
> feeling so far is
> 
> (1) Mandating support is not going to be acceptable
> 
> (2) Mandating format of resumption tokens is not going to be acceptable
> 
> (3) Mandating resumption tokens be long lifed (eg: can try again the
>     following day) is not acceptable
> 
> (4) In fact, mandating that resumption tokens be unique (allowing
>     a token to be reused twice in quick succession to get the same
>     data) is not acceptable
> 
> So any proposal needs to be optionally supported.
> 
> Question time:
> 
> Does anyone else think that this is a major hole in OAI? I personally
> do. After trying to crawl sites, things go wrong. The larger the site,
> the greater the probability that something will go wrong. The larger
> the site, the greater the pain of starting all over again. I do not
> think it is practical for anyone to harvest ETDCat if is really got
> 4,000,000 records. Any fault, and start downloading that 4gb again!
> So I feel strongly on this one. In fact, I think this is the most
> major problem OAI has.
> 
> Do people think its better to reuse resumption tokens for this purpose,
> or introduce a different sort of token? ETDCat for example I think
> allocates a session id in resumption tokens, meaning they cannot
> be reused when the session times out in the server (similar semantics
> anyway). This is a reasonable implementation decision to make.
> So maybe its better for servers to return an additional token,
> which is a <restartToken> which means a client can instead of
> specifying from= and to= again, specify restartToken= instead where
> the server then automatically works out whatever other parameters
> it needs, creates a new session etc internally. The new 'session'
> (ListXXX verb) then can use resumptionTokens to manage that new
> transfer.
> 
> The idea is for a <restartToken> to be long lifed. It may be less
> efficient to use than a resumptionToken, but its only purpose is
> if the client fails the download. If a server does not support
> restartToken, it simply never returns one. Large collections *should*
> support restartTokens.
> 
> For my harvester, I can then remember (to disk) the restartToken for
> every packet I get back, allowing me to recover much more easily
> if anything crashes. If restartToken's are too hard for someone
> to implement, then they don't. If you have a large data collection
> on the other hand, to reduce network load, I think its probably worth
> the extra effort of supporting restartTokens.
> 
> Any comments? Better suggesions?
> 
> Alan
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 

---
Michael L. Nelson
NASA Langley Research Center		m.l.nelson@larc.nasa.gov
MS 158, Hampton, VA 23681		http://www.ils.unc.edu/~mln/
+1 757 864 8511				+1 757 864 8342 (f)



From mln@ils.unc.edu  Tue Mar  5 15:29:36 2002
From: mln@ils.unc.edu (Michael L. Nelson)
Date: Tue, 5 Mar 2002 10:29:36 -0500 (EST)
Subject: [OAI-implementers] Inadequacy of Datestamps + repository IDs
In-Reply-To: <005a01c1c44b$0ba94b00$6400a8c0@Advocate>
Message-ID: <Pine.GSO.4.21.0203051005180.29139-100000@ruby.ils.unc.edu>

Tim,

datestamps:

2.0 will require at least second level granularity in the protocol
responses (decimal fractions of a second are optional).  aggregators
should then allow harvesting on at least second level granularity.  this
will result in the monotonically increasing values that you suggest below,
since its unlikely that a repository can load many thousands of records
in a single second (or 1/10th of a second).  and if it can load that many
per second, it shold slow down -- there's no rush ;-).

repositories will have the option of what level of time granularity they
support for harvesting, but obviously large volume federators will want to
support fine grained harvesting.  there will be mechanisms for determining
what granularity a repository supports.

repository ids:

I agree; an approach that piggy-backs on DNS names will probably be a good
one.  I'm not sure we can *require* people to use DNS names, but they will
probably migrate to it just to insure uniqueness.

regards,

Michael

On Tue, 5 Mar 2002, Tim Brody wrote:

> Dear all,
> 
> Datestamps:
> 
> Following on from Alan's email regarding datestamps, I would add another
> datestamp show-stopper:
> Federators
> 
> My current understanding of how to build hierachical harvesting is to change
> the record datestamp to the day of harvest. This means if a repository of
> 100,000 records is harvested, that will be 100,000 similarly dated records.
> 
> May I suggest the following (which would seem in keeping with OAI's aim of
> DP flexibility):
> Relax "datestamp" to be any positive number (not necessarily exclusive),
> which the repository must be capable of applying a "from" and "to" filter.
> New and changed records must have a number greater than the last record in
> the repository.
> 
> If a repository can be required to provide records in "datestamp" order,
> resuming from a broken response is simple (continue from last index
> inclusive).
> 
> As a datestamp is already an ordered number, this won't lose anything, but
> will enable repositories where a datestamp makes no sense to provide
> incremental harvesting.
> 
> repository IDs:
> 
> Following from my email a while back, it seems the discussion on UIDing
> repositories has come up with central vs. distributed listings. Simeon
> Warner responded with:
> 
> > I don't see harm in allowing also '-' and '.'. (I wouldn't want to make it
> > case insensitive.) However, without some enforceable policy about naming
> > (avoiding the need for OAI registration which currently solves the
> > uniqueness problem) does this really buy us anything? After all, I could
> > use identifiers "http://arXiv.org/abs/hep-th/9901001" and such for arXiv
> > but I choose to use the simpler oai scheme "oai:arXiv:hep-th/9901001".
> 
> I can't see a flat, short, centralised naming mechanism working in the
> long-run. XML namespaces are UIDed by URL (which is guaranteed to be unique
> in an honest system). Assuming that bandwidth isn't a problem, then it makes
> sense to have identifiers based on some kind of URL structure - why restrict
> the length of identifiers?:
> oai:arXiv.org/oai1:hep-th/0001001
> oai:cogprints.soton.ac.uk/perl/oai:cogprints/00001111
> 
> Another thought about naming authorities: What happens if someone sues for
> trademark infringement (a univeristy archive on microsoft legal proceedings
> called "oai:microsoft/...")? If archive's are based on DNS, they already
> have well-established mechanisms for coping with name conflicts.
> 
> 
> All the best,
> Tim Brody
> 
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 

---
Michael L. Nelson
NASA Langley Research Center		m.l.nelson@larc.nasa.gov
MS 158, Hampton, VA 23681		http://www.ils.unc.edu/~mln/
+1 757 864 8511				+1 757 864 8342 (f)


From simeon@cs.cornell.edu  Tue Mar  5 15:46:44 2002
From: simeon@cs.cornell.edu (Simeon Warner)
Date: Tue, 5 Mar 2002 10:46:44 -0500 (EST)
Subject: [OAI-implementers] Better resumption mechanism - more important
 than ever!
In-Reply-To: <Pine.GSO.4.21.0203050947320.29139-100000@ruby.ils.unc.edu>
Message-ID: <Pine.LNX.4.44.0203051023060.24770-100000@ice.cs.cornell.edu>

I agree with Michael. A repository such as ETDCat should be willing to put
the extra effort in to make harvesting easy and this can be done without
creating any additional barrier for small repositories. I suggest 
that ETDCat should use stateless and reusable resumptionTokens.

The question I see is whether we should come up with a standard way to say
"my resumptionTokens are stateless/reusable". Since this is clearly a
repository-level property the obvious place to put such information would
be in the Identify response. My feeling is that this probably shouldn't be
part of the core protocol so a <description> block would be the best
option. Alternatively it could just become 'standard practice' for
harvesters to attempt to restart a harvest by reusing a resumptionToken
and assuming all is okay if no error is returned.

Cheers,
Simeon.



[aside: in arXiv we have a few days with a significant number of updates,
something like 24k. I build resumptionTokens that just code the from, 
until, set and metadataPrefix parameters for a request to continue the 
harvest. Since I limit the number of records returned to about 1000, this 
means that I have to split the 24k-days. I do this by adding an index 
within the datestamp. e.g.:
- usual form: 1997-04-23_2002-01-01__oai_dc
- with index: 1997-11-13#23774_2002-01-01__oai_dc
]

On Tue, 5 Mar 2002, Michael L. Nelson wrote:
> actually, the way I see it is the protocol should not be complicated with
> additional tokens and such to enforce what ETDCat (and similarly
> large-sized DPs) should do:
> 
> 1.  partition their collection into sets
> 2.  use stateless (or very long lived) resumptionTokens
> 
> in 2.0, resumptionTokens will have optional attributes, including
> "expirationDate", so this will take the guess work out of knowing how long
> a resumptionToken will be valid.
> 
> IMO, introducing an optional restartToken is no different (from an
> implementer's point of view) than making the resumptionToken last a long
> time.  
> 
> at some point, you (as a harvester) are simply at the mercy of the
> repository.  new features in the protocol won't change that.
> 
> regards,
> 
> Michael
> 
> On Tue, 5 Mar 2002, 'Alan Kent' wrote:
> > I just got some mail from Jeff at OCLC talking about ETDCat (hope
> > you don't mind me quoting some of your mail Jeff). In particular,
> > he just told me
> > 
> >     ETDCat contains a lot of records (over 4 million), all of
> >     which currently have the exact same datestamp from the initial load.
> > 
> > He also told me that there were no sets. So basically, its all
> > or nothing for this site because OAI has no standard way to resume
> > if a transfer fails.
> > 
> > If this has happened already, I think its likely to occur again.
> > (That is, one very large database all with the same time stamp.)
> > So any comments about having a single large collection like this
> > is beside the point. The point is OAI does not handle it well.
> > 
> > So I would like to resurrect the discussion again if people don't
> > mind on how to do support restarts. My understanding of the general
> > feeling so far is
> > 
> > (1) Mandating support is not going to be acceptable
> > 
> > (2) Mandating format of resumption tokens is not going to be acceptable
> > 
> > (3) Mandating resumption tokens be long lifed (eg: can try again the
> >     following day) is not acceptable
> > 
> > (4) In fact, mandating that resumption tokens be unique (allowing
> >     a token to be reused twice in quick succession to get the same
> >     data) is not acceptable
> > 
> > So any proposal needs to be optionally supported.
> > 
> > Question time:
> > 
> > Does anyone else think that this is a major hole in OAI? I personally
> > do. After trying to crawl sites, things go wrong. The larger the site,
> > the greater the probability that something will go wrong. The larger
> > the site, the greater the pain of starting all over again. I do not
> > think it is practical for anyone to harvest ETDCat if is really got
> > 4,000,000 records. Any fault, and start downloading that 4gb again!
> > So I feel strongly on this one. In fact, I think this is the most
> > major problem OAI has.
> > 
> > Do people think its better to reuse resumption tokens for this purpose,
> > or introduce a different sort of token? ETDCat for example I think
> > allocates a session id in resumption tokens, meaning they cannot
> > be reused when the session times out in the server (similar semantics
> > anyway). This is a reasonable implementation decision to make.
> > So maybe its better for servers to return an additional token,
> > which is a <restartToken> which means a client can instead of
> > specifying from= and to= again, specify restartToken= instead where
> > the server then automatically works out whatever other parameters
> > it needs, creates a new session etc internally. The new 'session'
> > (ListXXX verb) then can use resumptionTokens to manage that new
> > transfer.
> > 
> > The idea is for a <restartToken> to be long lifed. It may be less
> > efficient to use than a resumptionToken, but its only purpose is
> > if the client fails the download. If a server does not support
> > restartToken, it simply never returns one. Large collections *should*
> > support restartTokens.
> > 
> > For my harvester, I can then remember (to disk) the restartToken for
> > every packet I get back, allowing me to recover much more easily
> > if anything crashes. If restartToken's are too hard for someone
> > to implement, then they don't. If you have a large data collection
> > on the other hand, to reduce network load, I think its probably worth
> > the extra effort of supporting restartTokens.
> > 
> > Any comments? Better suggesions?
> > 
> > Alan


From liu_x@cs.odu.edu  Tue Mar  5 16:36:23 2002
From: liu_x@cs.odu.edu (Xiaoming Liu)
Date: Tue, 5 Mar 2002 11:36:23 -0500 (EST)
Subject: [OAI-implementers] Better resumption mechanism - more important
 than ever!
In-Reply-To: <Pine.GSO.4.21.0203050947320.29139-100000@ruby.ils.unc.edu>
Message-ID: <Pine.SOL.4.10.10203051132170.1217-100000@defiant.cs.odu.edu>

On Tue, 5 Mar 2002, Michael L. Nelson wrote:

> 
> actually, the way I see it is the protocol should not be complicated with
> additional tokens and such to enforce what ETDCat (and similiarly
> large-sized DPs) should do:
> 
> 1.  partition their collection into sets


I agree all with Michael except the sets point. As OAI doesn't guarantee
that a harvester will get everything if it harvests by sets.

So the only possibility maybe stateless resumptionTokens ( implemented by
sorted fine-grained datestamp).

regards,
liu
 


> 2.  use stateless (or very long lived) resumptionTokens
> 
> in 2.0, resumptionTokens will have optional attributes, including
> "expirationDate", so this will take the guess work out of knowing how long
> a resumptionToken will be valid.
> 
> IMO, introducing an optional restartToken is no different (from an
> implementer's point of view) than making the resumptionToken last a long
> time.  
> 
> at some point, you (as a harvester) are simply at the mercy of the
> repository.  new features in the protocol won't change that.
> 
> regards,
> 
> Michael
> 
> On Tue, 5 Mar 2002, 'Alan Kent' wrote:
> 
> > I just got some mail from Jeff at OCLC talking about ETDCat (hope
> > you don't mind me quoting some of your mail Jeff). In particular,
> > he just told me
> > 
> >     ETDCat contains a lot of records (over 4 million), all of
> >     which currently have the exact same datestamp from the initial load.
> > 
> > He also told me that there were no sets. So basically, its all
> > or nothing for this site because OAI has no standard way to resume
> > if a transfer fails.
> > 
> > If this has happened already, I think its likely to occur again.
> > (That is, one very large database all with the same time stamp.)
> > So any comments about having a single large collection like this
> > is beside the point. The point is OAI does not handle it well.
> > 
> > So I would like to resurrect the discussion again if people don't
> > mind on how to do support restarts. My understanding of the general
> > feeling so far is
> > 
> > (1) Mandating support is not going to be acceptable
> > 
> > (2) Mandating format of resumption tokens is not going to be acceptable
> > 
> > (3) Mandating resumption tokens be long lifed (eg: can try again the
> >     following day) is not acceptable
> > 
> > (4) In fact, mandating that resumption tokens be unique (allowing
> >     a token to be reused twice in quick succession to get the same
> >     data) is not acceptable
> > 
> > So any proposal needs to be optionally supported.
> > 
> > Question time:
> > 
> > Does anyone else think that this is a major hole in OAI? I personally
> > do. After trying to crawl sites, things go wrong. The larger the site,
> > the greater the probability that something will go wrong. The larger
> > the site, the greater the pain of starting all over again. I do not
> > think it is practical for anyone to harvest ETDCat if is really got
> > 4,000,000 records. Any fault, and start downloading that 4gb again!
> > So I feel strongly on this one. In fact, I think this is the most
> > major problem OAI has.
> > 
> > Do people think its better to reuse resumption tokens for this purpose,
> > or introduce a different sort of token? ETDCat for example I think
> > allocates a session id in resumption tokens, meaning they cannot
> > be reused when the session times out in the server (similar semantics
> > anyway). This is a reasonable implementation decision to make.
> > So maybe its better for servers to return an additional token,
> > which is a <restartToken> which means a client can instead of
> > specifying from= and to= again, specify restartToken= instead where
> > the server then automatically works out whatever other parameters
> > it needs, creates a new session etc internally. The new 'session'
> > (ListXXX verb) then can use resumptionTokens to manage that new
> > transfer.
> > 
> > The idea is for a <restartToken> to be long lifed. It may be less
> > efficient to use than a resumptionToken, but its only purpose is
> > if the client fails the download. If a server does not support
> > restartToken, it simply never returns one. Large collections *should*
> > support restartTokens.
> > 
> > For my harvester, I can then remember (to disk) the restartToken for
> > every packet I get back, allowing me to recover much more easily
> > if anything crashes. If restartToken's are too hard for someone
> > to implement, then they don't. If you have a large data collection
> > on the other hand, to reduce network load, I think its probably worth
> > the extra effort of supporting restartTokens.
> > 
> > Any comments? Better suggesions?
> > 
> > Alan
> > _______________________________________________
> > OAI-implementers mailing list
> > OAI-implementers@oaisrv.nsdl.cornell.edu
> > http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> > 
> 
> ---
> Michael L. Nelson
> NASA Langley Research Center		m.l.nelson@larc.nasa.gov
> MS 158, Hampton, VA 23681		http://www.ils.unc.edu/~mln/
> +1 757 864 8511				+1 757 864 8342 (f)
> 
> 
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 


From jyoung@oclc.org  Tue Mar  5 18:36:49 2002
From: jyoung@oclc.org (Young,Jeff)
Date: Tue, 5 Mar 2002 13:36:49 -0500
Subject: [OAI-implementers] Better resumption mechanism - more importa
 nt than ever!
Message-ID: <E5431CF93E29F9478878F623E5B9CE9834255A@OA3-SERVER.oa.oclc.org>

I'd be very disappointed if ETDCat required custom and unique consideration
from harvesters merely because of its size. Partitioning the collection
would be a case in point. The implication seems to be that harvesters would
somehow know to query the list of sets and then loop through each of them.
How would an arbitrary harvester know to do that, and is their software even
capable of it without custom coding? It would also prevent me from using
sets for legitimate purposes since I couldn't distinguish between them.

I'd be happy to implement stateless resumptionTokens, but unless harvesters
know how to use them for recovery, why bother? How many harvesters today
could manage a recovery using stateless resumptionTokens? How many
harvesters will handle it tomorrow if OAI remains agnostic on the issue?

I'm sure ETDCat needs more stress testing to minimize future failures. The
fact that we've discussed this before, though, indicates a recognition that
problems can happen. I don't blame Alan if he doesn't want to negotiate
special rules for harvesting ETDCat merely because the risk is proportional
to the size of the repository.

Jeff

-----Original Message-----
From: Michael L. Nelson [mailto:mln@ils.unc.edu]
Sent: Tuesday, March 05, 2002 10:03 AM
To: 'Alan Kent'
Cc: OAI Implementors
Subject: Re: [OAI-implementers] Better resumption mechanism - more
important than ever!



actually, the way I see it is the protocol should not be complicated with
additional tokens and such to enforce what ETDCat (and similiarly
large-sized DPs) should do:

1.  partition their collection into sets
2.  use stateless (or very long lived) resumptionTokens

in 2.0, resumptionTokens will have optional attributes, including
"expirationDate", so this will take the guess work out of knowing how long
a resumptionToken will be valid.

IMO, introducing an optional restartToken is no different (from an
implementer's point of view) than making the resumptionToken last a long
time.  

at some point, you (as a harvester) are simply at the mercy of the
repository.  new features in the protocol won't change that.

regards,

Michael

On Tue, 5 Mar 2002, 'Alan Kent' wrote:

> I just got some mail from Jeff at OCLC talking about ETDCat (hope
> you don't mind me quoting some of your mail Jeff). In particular,
> he just told me
> 
>     ETDCat contains a lot of records (over 4 million), all of
>     which currently have the exact same datestamp from the initial load.
> 
> He also told me that there were no sets. So basically, its all
> or nothing for this site because OAI has no standard way to resume
> if a transfer fails.
> 
> If this has happened already, I think its likely to occur again.
> (That is, one very large database all with the same time stamp.)
> So any comments about having a single large collection like this
> is beside the point. The point is OAI does not handle it well.
> 
> So I would like to resurrect the discussion again if people don't
> mind on how to do support restarts. My understanding of the general
> feeling so far is
> 
> (1) Mandating support is not going to be acceptable
> 
> (2) Mandating format of resumption tokens is not going to be acceptable
> 
> (3) Mandating resumption tokens be long lifed (eg: can try again the
>     following day) is not acceptable
> 
> (4) In fact, mandating that resumption tokens be unique (allowing
>     a token to be reused twice in quick succession to get the same
>     data) is not acceptable
> 
> So any proposal needs to be optionally supported.
> 
> Question time:
> 
> Does anyone else think that this is a major hole in OAI? I personally
> do. After trying to crawl sites, things go wrong. The larger the site,
> the greater the probability that something will go wrong. The larger
> the site, the greater the pain of starting all over again. I do not
> think it is practical for anyone to harvest ETDCat if is really got
> 4,000,000 records. Any fault, and start downloading that 4gb again!
> So I feel strongly on this one. In fact, I think this is the most
> major problem OAI has.
> 
> Do people think its better to reuse resumption tokens for this purpose,
> or introduce a different sort of token? ETDCat for example I think
> allocates a session id in resumption tokens, meaning they cannot
> be reused when the session times out in the server (similar semantics
> anyway). This is a reasonable implementation decision to make.
> So maybe its better for servers to return an additional token,
> which is a <restartToken> which means a client can instead of
> specifying from= and to= again, specify restartToken= instead where
> the server then automatically works out whatever other parameters
> it needs, creates a new session etc internally. The new 'session'
> (ListXXX verb) then can use resumptionTokens to manage that new
> transfer.
> 
> The idea is for a <restartToken> to be long lifed. It may be less
> efficient to use than a resumptionToken, but its only purpose is
> if the client fails the download. If a server does not support
> restartToken, it simply never returns one. Large collections *should*
> support restartTokens.
> 
> For my harvester, I can then remember (to disk) the restartToken for
> every packet I get back, allowing me to recover much more easily
> if anything crashes. If restartToken's are too hard for someone
> to implement, then they don't. If you have a large data collection
> on the other hand, to reduce network load, I think its probably worth
> the extra effort of supporting restartTokens.
> 
> Any comments? Better suggesions?
> 
> Alan
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 

---
Michael L. Nelson
NASA Langley Research Center		m.l.nelson@larc.nasa.gov
MS 158, Hampton, VA 23681		http://www.ils.unc.edu/~mln/
+1 757 864 8511				+1 757 864 8342 (f)


_______________________________________________
OAI-implementers mailing list
OAI-implementers@oaisrv.nsdl.cornell.edu
http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers

From mln@ils.unc.edu  Tue Mar  5 18:59:22 2002
From: mln@ils.unc.edu (Michael L. Nelson)
Date: Tue, 5 Mar 2002 13:59:22 -0500 (EST)
Subject: [OAI-implementers] Better resumption mechanism - more importa
 nt than ever!
In-Reply-To: <E5431CF93E29F9478878F623E5B9CE9834255A@OA3-SERVER.oa.oclc.org>
Message-ID: <Pine.GSO.4.21.0203051344110.29139-100000@ruby.ils.unc.edu>

On Tue, 5 Mar 2002, Young,Jeff wrote:

> I'd be very disappointed if ETDCat required custom and unique consideration
> from harvesters merely because of its size. Partitioning the collection
> would be a case in point. The implication seems to be that harvesters would
> somehow know to query the list of sets and then loop through each of them.
> How would an arbitrary harvester know to do that, and is their software even
> capable of it without custom coding? It would also prevent me from using
> sets for legitimate purposes since I couldn't distinguish between them.

my original point about sets was not intended to be the primary point.  if
ETDCat already uses sets, then Alan's harvester should investigate
harvesting by set, since it will naturally partion the collection (modulo
Liu's point about not all records are guaranteed to be in sets).

> 
> I'd be happy to implement stateless resumptionTokens, but unless harvesters
> know how to use them for recovery, why bother? How many harvesters today
> could manage a recovery using stateless resumptionTokens? How many
> harvesters will handle it tomorrow if OAI remains agnostic on the issue?
> 

I'm not sure, but I would guess this would be the default behaivor.  If
the harvester chokes, I would start again where it left off.  If
successful harvesting continues, then there was a transient error.  If it
fails again, then maybe the repository has a problem.

In 2.0, it will be even easier to determine:  

- when the resumptionToken expires
- how big the result set is
- and how many records the repository has transmitted so far

Any harvester writers our there care to comment?  Liu?  Hussein?

> I'm sure ETDCat needs more stress testing to minimize future failures. The
> fact that we've discussed this before, though, indicates a recognition that
> problems can happen. I don't blame Alan if he doesn't want to negotiate
> special rules for harvesting ETDCat merely because the risk is proportional
> to the size of the repository.

(I hope no one interprets my comments as beating up on ETDCat)

These are interesting points:  to the best of my knowledge, if ETDCat has
4M records, its by and far the biggest OAI repository out there.  My point
is that 4M of anything is a big number, and repositories that large need
to make sure they implement features that facilitate fault-tolerant
harvesting.  Stateless (or very long lived) resumptionTokens would appear
to be one of those features.  Also, if you have 4M records all with the
same datestamp, this would seem to be an ideal candidate for some response
caching techniques, which tied with very long lived (2-3 days?)
resumptionTokens would seem to make for an efficent load on your end.


regards,

Michael

> 
> Jeff
> 
> -----Original Message-----
> From: Michael L. Nelson [mailto:mln@ils.unc.edu]
> Sent: Tuesday, March 05, 2002 10:03 AM
> To: 'Alan Kent'
> Cc: OAI Implementors
> Subject: Re: [OAI-implementers] Better resumption mechanism - more
> important than ever!
> 
> 
> 
> actually, the way I see it is the protocol should not be complicated with
> additional tokens and such to enforce what ETDCat (and similiarly
> large-sized DPs) should do:
> 
> 1.  partition their collection into sets
> 2.  use stateless (or very long lived) resumptionTokens
> 
> in 2.0, resumptionTokens will have optional attributes, including
> "expirationDate", so this will take the guess work out of knowing how long
> a resumptionToken will be valid.
> 
> IMO, introducing an optional restartToken is no different (from an
> implementer's point of view) than making the resumptionToken last a long
> time.  
> 
> at some point, you (as a harvester) are simply at the mercy of the
> repository.  new features in the protocol won't change that.
> 
> regards,
> 
> Michael
> 
> On Tue, 5 Mar 2002, 'Alan Kent' wrote:
> 
> > I just got some mail from Jeff at OCLC talking about ETDCat (hope
> > you don't mind me quoting some of your mail Jeff). In particular,
> > he just told me
> > 
> >     ETDCat contains a lot of records (over 4 million), all of
> >     which currently have the exact same datestamp from the initial load.
> > 
> > He also told me that there were no sets. So basically, its all
> > or nothing for this site because OAI has no standard way to resume
> > if a transfer fails.
> > 
> > If this has happened already, I think its likely to occur again.
> > (That is, one very large database all with the same time stamp.)
> > So any comments about having a single large collection like this
> > is beside the point. The point is OAI does not handle it well.
> > 
> > So I would like to resurrect the discussion again if people don't
> > mind on how to do support restarts. My understanding of the general
> > feeling so far is
> > 
> > (1) Mandating support is not going to be acceptable
> > 
> > (2) Mandating format of resumption tokens is not going to be acceptable
> > 
> > (3) Mandating resumption tokens be long lifed (eg: can try again the
> >     following day) is not acceptable
> > 
> > (4) In fact, mandating that resumption tokens be unique (allowing
> >     a token to be reused twice in quick succession to get the same
> >     data) is not acceptable
> > 
> > So any proposal needs to be optionally supported.
> > 
> > Question time:
> > 
> > Does anyone else think that this is a major hole in OAI? I personally
> > do. After trying to crawl sites, things go wrong. The larger the site,
> > the greater the probability that something will go wrong. The larger
> > the site, the greater the pain of starting all over again. I do not
> > think it is practical for anyone to harvest ETDCat if is really got
> > 4,000,000 records. Any fault, and start downloading that 4gb again!
> > So I feel strongly on this one. In fact, I think this is the most
> > major problem OAI has.
> > 
> > Do people think its better to reuse resumption tokens for this purpose,
> > or introduce a different sort of token? ETDCat for example I think
> > allocates a session id in resumption tokens, meaning they cannot
> > be reused when the session times out in the server (similar semantics
> > anyway). This is a reasonable implementation decision to make.
> > So maybe its better for servers to return an additional token,
> > which is a <restartToken> which means a client can instead of
> > specifying from= and to= again, specify restartToken= instead where
> > the server then automatically works out whatever other parameters
> > it needs, creates a new session etc internally. The new 'session'
> > (ListXXX verb) then can use resumptionTokens to manage that new
> > transfer.
> > 
> > The idea is for a <restartToken> to be long lifed. It may be less
> > efficient to use than a resumptionToken, but its only purpose is
> > if the client fails the download. If a server does not support
> > restartToken, it simply never returns one. Large collections *should*
> > support restartTokens.
> > 
> > For my harvester, I can then remember (to disk) the restartToken for
> > every packet I get back, allowing me to recover much more easily
> > if anything crashes. If restartToken's are too hard for someone
> > to implement, then they don't. If you have a large data collection
> > on the other hand, to reduce network load, I think its probably worth
> > the extra effort of supporting restartTokens.
> > 
> > Any comments? Better suggesions?
> > 
> > Alan
> > _______________________________________________
> > OAI-implementers mailing list
> > OAI-implementers@oaisrv.nsdl.cornell.edu
> > http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> > 
> 
> ---
> Michael L. Nelson
> NASA Langley Research Center		m.l.nelson@larc.nasa.gov
> MS 158, Hampton, VA 23681		http://www.ils.unc.edu/~mln/
> +1 757 864 8511				+1 757 864 8342 (f)
> 
> 
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 

---
Michael L. Nelson
NASA Langley Research Center		m.l.nelson@larc.nasa.gov
MS 158, Hampton, VA 23681		http://www.ils.unc.edu/~mln/
+1 757 864 8511				+1 757 864 8342 (f)



From liu_x@cs.odu.edu  Tue Mar  5 19:40:58 2002
From: liu_x@cs.odu.edu (Xiaoming Liu)
Date: Tue, 5 Mar 2002 14:40:58 -0500 (EST)
Subject: [OAI-implementers] Better resumption mechanism - more importa
 nt than ever!
In-Reply-To: <E5431CF93E29F9478878F623E5B9CE9834255A@OA3-SERVER.oa.oclc.org>
Message-ID: <Pine.SOL.4.10.10203051432340.1302-100000@defiant.cs.odu.edu>

On Tue, 5 Mar 2002, Young,Jeff wrote:

> 
> I'd be happy to implement stateless resumptionTokens, but unless harvesters
> know how to use them for recovery, why bother? How many harvesters today
> could manage a recovery using stateless resumptionTokens? How many
> harvesters will handle it tomorrow if OAI remains agnostic on the issue?

I guess this is a major issue that harvester should follow a certain
policy. In current implementation in Arc, for each failed request, the
harvester will try at most three times using the same http request. And it
will give up after that. This policy really helps several times, but not
too often ;-)

Ideally, I guess a harvester could use exponential backoff algorithm to
keep trying until the resumptionToken is expired (Considering a
time-to-live parameter will be added in 2.0). And if we implment the
harvester in a multiple process/thread way, the system should scale well
for several resumptionToken errors.

I think something like "implementation guide" or "reference
implementation" will help harvester and DP understand each other well
beyond the core protocol.

regards,
liu



 

> 
> I'm sure ETDCat needs more stress testing to minimize future failures. The
> fact that we've discussed this before, though, indicates a recognition that
> problems can happen. I don't blame Alan if he doesn't want to negotiate
> special rules for harvesting ETDCat merely because the risk is proportional
> to the size of the repository.
> 
> Jeff
> 
> -----Original Message-----
> From: Michael L. Nelson [mailto:mln@ils.unc.edu]
> Sent: Tuesday, March 05, 2002 10:03 AM
> To: 'Alan Kent'
> Cc: OAI Implementors
> Subject: Re: [OAI-implementers] Better resumption mechanism - more
> important than ever!
> 
> 
> 
> actually, the way I see it is the protocol should not be complicated with
> additional tokens and such to enforce what ETDCat (and similiarly
> large-sized DPs) should do:
> 
> 1.  partition their collection into sets
> 2.  use stateless (or very long lived) resumptionTokens
> 
> in 2.0, resumptionTokens will have optional attributes, including
> "expirationDate", so this will take the guess work out of knowing how long
> a resumptionToken will be valid.
> 
> IMO, introducing an optional restartToken is no different (from an
> implementer's point of view) than making the resumptionToken last a long
> time.  
> 
> at some point, you (as a harvester) are simply at the mercy of the
> repository.  new features in the protocol won't change that.
> 
> regards,
> 
> Michael
> 
> On Tue, 5 Mar 2002, 'Alan Kent' wrote:
> 
> > I just got some mail from Jeff at OCLC talking about ETDCat (hope
> > you don't mind me quoting some of your mail Jeff). In particular,
> > he just told me
> > 
> >     ETDCat contains a lot of records (over 4 million), all of
> >     which currently have the exact same datestamp from the initial load.
> > 
> > He also told me that there were no sets. So basically, its all
> > or nothing for this site because OAI has no standard way to resume
> > if a transfer fails.
> > 
> > If this has happened already, I think its likely to occur again.
> > (That is, one very large database all with the same time stamp.)
> > So any comments about having a single large collection like this
> > is beside the point. The point is OAI does not handle it well.
> > 
> > So I would like to resurrect the discussion again if people don't
> > mind on how to do support restarts. My understanding of the general
> > feeling so far is
> > 
> > (1) Mandating support is not going to be acceptable
> > 
> > (2) Mandating format of resumption tokens is not going to be acceptable
> > 
> > (3) Mandating resumption tokens be long lifed (eg: can try again the
> >     following day) is not acceptable
> > 
> > (4) In fact, mandating that resumption tokens be unique (allowing
> >     a token to be reused twice in quick succession to get the same
> >     data) is not acceptable
> > 
> > So any proposal needs to be optionally supported.
> > 
> > Question time:
> > 
> > Does anyone else think that this is a major hole in OAI? I personally
> > do. After trying to crawl sites, things go wrong. The larger the site,
> > the greater the probability that something will go wrong. The larger
> > the site, the greater the pain of starting all over again. I do not
> > think it is practical for anyone to harvest ETDCat if is really got
> > 4,000,000 records. Any fault, and start downloading that 4gb again!
> > So I feel strongly on this one. In fact, I think this is the most
> > major problem OAI has.
> > 
> > Do people think its better to reuse resumption tokens for this purpose,
> > or introduce a different sort of token? ETDCat for example I think
> > allocates a session id in resumption tokens, meaning they cannot
> > be reused when the session times out in the server (similar semantics
> > anyway). This is a reasonable implementation decision to make.
> > So maybe its better for servers to return an additional token,
> > which is a <restartToken> which means a client can instead of
> > specifying from= and to= again, specify restartToken= instead where
> > the server then automatically works out whatever other parameters
> > it needs, creates a new session etc internally. The new 'session'
> > (ListXXX verb) then can use resumptionTokens to manage that new
> > transfer.
> > 
> > The idea is for a <restartToken> to be long lifed. It may be less
> > efficient to use than a resumptionToken, but its only purpose is
> > if the client fails the download. If a server does not support
> > restartToken, it simply never returns one. Large collections *should*
> > support restartTokens.
> > 
> > For my harvester, I can then remember (to disk) the restartToken for
> > every packet I get back, allowing me to recover much more easily
> > if anything crashes. If restartToken's are too hard for someone
> > to implement, then they don't. If you have a large data collection
> > on the other hand, to reduce network load, I think its probably worth
> > the extra effort of supporting restartTokens.
> > 
> > Any comments? Better suggesions?
> > 
> > Alan
> > _______________________________________________
> > OAI-implementers mailing list
> > OAI-implementers@oaisrv.nsdl.cornell.edu
> > http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> > 
> 
> ---
> Michael L. Nelson
> NASA Langley Research Center		m.l.nelson@larc.nasa.gov
> MS 158, Hampton, VA 23681		http://www.ils.unc.edu/~mln/
> +1 757 864 8511				+1 757 864 8342 (f)
> 
> 
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 


From simeon@cs.cornell.edu  Tue Mar  5 19:48:43 2002
From: simeon@cs.cornell.edu (Simeon Warner)
Date: Tue, 5 Mar 2002 14:48:43 -0500 (EST)
Subject: [OAI-implementers] Better resumption mechanism - more importa
 nt than ever!
In-Reply-To: <Pine.SOL.4.10.10203051432340.1302-100000@defiant.cs.odu.edu>
Message-ID: <Pine.LNX.4.44.0203051442390.26364-100000@ice.cs.cornell.edu>

On Tue, 5 Mar 2002, Xiaoming Liu wrote:
> On Tue, 5 Mar 2002, Young,Jeff wrote:
> > I'd be happy to implement stateless resumptionTokens, but unless harvesters
> > know how to use them for recovery, why bother? How many harvesters today
> > could manage a recovery using stateless resumptionTokens? How many
> > harvesters will handle it tomorrow if OAI remains agnostic on the issue?
> 
> I guess this is a major issue that harvester should follow a certain
> policy. In current implementation in Arc, for each failed request, the
> harvester will try at most three times using the same http request. And it
> will give up after that. This policy really helps several times, but not
> too often ;-)

Liu, your policy is the sort of thing I had imagined. However, I'm curious
about how frequently you find that a sequence of harvests fails. When I
last did an extensive harvest (last summer) I found that, provided
repositories had implemented the protocol properly, I rarely had problems
getting successful responses to complete a List request. Can you give us
some (approximate) statistics?
 
> Ideally, I guess a harvester could use exponential backoff algorithm to
> keep trying until the resumptionToken is expired (Considering a
> time-to-live parameter will be added in 2.0). And if we implment the
> harvester in a multiple process/thread way, the system should scale well
> for several resumptionToken errors.
> 
> I think something like "implementation guide" or "reference
> implementation" will help harvester and DP understand each other well
> beyond the core protocol.

Yes, this should certainly be covered in the implementation guidelines.

Cheers,
Simeon.
 

> regards,
> liu
> > 
> > I'm sure ETDCat needs more stress testing to minimize future failures. The
> > fact that we've discussed this before, though, indicates a recognition that
> > problems can happen. I don't blame Alan if he doesn't want to negotiate
> > special rules for harvesting ETDCat merely because the risk is proportional
> > to the size of the repository.
> > 
> > Jeff
> > 
> > -----Original Message-----
> > From: Michael L. Nelson [mailto:mln@ils.unc.edu]
> > Sent: Tuesday, March 05, 2002 10:03 AM
> > To: 'Alan Kent'
> > Cc: OAI Implementors
> > Subject: Re: [OAI-implementers] Better resumption mechanism - more
> > important than ever!
> > 
> > actually, the way I see it is the protocol should not be complicated with
> > additional tokens and such to enforce what ETDCat (and similiarly
> > large-sized DPs) should do:
> > 
> > 1.  partition their collection into sets
> > 2.  use stateless (or very long lived) resumptionTokens
> > 
> > in 2.0, resumptionTokens will have optional attributes, including
> > "expirationDate", so this will take the guess work out of knowing how long
> > a resumptionToken will be valid.
> > 
> > IMO, introducing an optional restartToken is no different (from an
> > implementer's point of view) than making the resumptionToken last a long
> > time.  
> > 
> > at some point, you (as a harvester) are simply at the mercy of the
> > repository.  new features in the protocol won't change that.
> > 
> > regards,
> > 
> > Michael
> > 
> > On Tue, 5 Mar 2002, 'Alan Kent' wrote:
> > 
> > > I just got some mail from Jeff at OCLC talking about ETDCat (hope
> > > you don't mind me quoting some of your mail Jeff). In particular,
> > > he just told me
> > > 
> > >     ETDCat contains a lot of records (over 4 million), all of
> > >     which currently have the exact same datestamp from the initial load.
> > > 
> > > He also told me that there were no sets. So basically, its all
> > > or nothing for this site because OAI has no standard way to resume
> > > if a transfer fails.
> > > 
> > > If this has happened already, I think its likely to occur again.
> > > (That is, one very large database all with the same time stamp.)
> > > So any comments about having a single large collection like this
> > > is beside the point. The point is OAI does not handle it well.
> > > 
> > > So I would like to resurrect the discussion again if people don't
> > > mind on how to do support restarts. My understanding of the general
> > > feeling so far is
> > > 
> > > (1) Mandating support is not going to be acceptable
> > > 
> > > (2) Mandating format of resumption tokens is not going to be acceptable
> > > 
> > > (3) Mandating resumption tokens be long lifed (eg: can try again the
> > >     following day) is not acceptable
> > > 
> > > (4) In fact, mandating that resumption tokens be unique (allowing
> > >     a token to be reused twice in quick succession to get the same
> > >     data) is not acceptable
> > > 
> > > So any proposal needs to be optionally supported.
> > > 
> > > Question time:
> > > 
> > > Does anyone else think that this is a major hole in OAI? I personally
> > > do. After trying to crawl sites, things go wrong. The larger the site,
> > > the greater the probability that something will go wrong. The larger
> > > the site, the greater the pain of starting all over again. I do not
> > > think it is practical for anyone to harvest ETDCat if is really got
> > > 4,000,000 records. Any fault, and start downloading that 4gb again!
> > > So I feel strongly on this one. In fact, I think this is the most
> > > major problem OAI has.
> > > 
> > > Do people think its better to reuse resumption tokens for this purpose,
> > > or introduce a different sort of token? ETDCat for example I think
> > > allocates a session id in resumption tokens, meaning they cannot
> > > be reused when the session times out in the server (similar semantics
> > > anyway). This is a reasonable implementation decision to make.
> > > So maybe its better for servers to return an additional token,
> > > which is a <restartToken> which means a client can instead of
> > > specifying from= and to= again, specify restartToken= instead where
> > > the server then automatically works out whatever other parameters
> > > it needs, creates a new session etc internally. The new 'session'
> > > (ListXXX verb) then can use resumptionTokens to manage that new
> > > transfer.
> > > 
> > > The idea is for a <restartToken> to be long lifed. It may be less
> > > efficient to use than a resumptionToken, but its only purpose is
> > > if the client fails the download. If a server does not support
> > > restartToken, it simply never returns one. Large collections *should*
> > > support restartTokens.
> > > 
> > > For my harvester, I can then remember (to disk) the restartToken for
> > > every packet I get back, allowing me to recover much more easily
> > > if anything crashes. If restartToken's are too hard for someone
> > > to implement, then they don't. If you have a large data collection
> > > on the other hand, to reduce network load, I think its probably worth
> > > the extra effort of supporting restartTokens.
> > > 
> > > Any comments? Better suggesions?
> > > 
> > > Alan
> > > _______________________________________________
> > > OAI-implementers mailing list
> > > OAI-implementers@oaisrv.nsdl.cornell.edu
> > > http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> > > 
> > 
> > ---
> > Michael L. Nelson
> > NASA Langley Research Center		m.l.nelson@larc.nasa.gov
> > MS 158, Hampton, VA 23681		http://www.ils.unc.edu/~mln/
> > +1 757 864 8511				+1 757 864 8342 (f)
> > 
> > 
> > _______________________________________________
> > OAI-implementers mailing list
> > OAI-implementers@oaisrv.nsdl.cornell.edu
> > http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> > _______________________________________________
> > OAI-implementers mailing list
> > OAI-implementers@oaisrv.nsdl.cornell.edu
> > http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> > 
> 
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 


From hussein@vt.edu  Tue Mar  5 20:24:16 2002
From: hussein@vt.edu (Hussein Suleman)
Date: Tue, 05 Mar 2002 15:24:16 -0500
Subject: [OAI-implementers] Better resumption mechanism - more importa nt than ever!
References: <Pine.GSO.4.21.0203051344110.29139-100000@ruby.ils.unc.edu>
Message-ID: <3C852970.1020705@vt.edu>

hi

this harvesting discussion is quite fascinating. here are some thoughts 
on how i approach harvesting:

personally, my harvesters do not reissue resumptionTokens if something 
goes wrong - rather, an error report is filed (usually by sending me 
email and/or logging the problem) and harvesting is aborted. i will 
someday include automatic exponential backoff/retry but the need hasn't 
been great enough to warrant that just yet.

a single harvesting operation is considered to be a sequence of 
requests, including as many resumptions as is necessary. the "last 
harvested date" is only updated when a single harvesting operations is 
completed successfully. this ensures that if any archive uses internal 
state and the network fails, i will not lose any records. while speed 
and recovery are important to me, integrity of the data is more so since 
most of my harvesters are part of hierarchical systems and i cannot 
afford to have data go missing in the early stages.

as far as scheduling goes, i run independent processes for each archive 
with a 2-level scheduling system (firstly, how often to check the global 
schedule, and secondly, how often to harvest the individual archives). 
failures are signalled by a fault report and a persistent lock on the 
archive. after i investigate what caused the failure i remove the lock - 
(it should be trivial to have the retry mechanism do this in future).

to state the stateless resumption problem a little differently:
   is the OAI protocol idempotent ?
if a request is submitted twice, will the responses be identical ? 
obviously not, since new records could have been added. maybe we need a 
weaker condition - like, if req2 is issued after req1, then the response 
res2 must contain at least all of the contents of the response res1. 
would this work ? i don't think so - if we update a record, its 
datestamp would cause it to move out of range of a from/until 
specification. is this the only case where the weaker condition fails ? 
if so, what if we ditch the until parameter ?

it would be nice to have a rigorous mathematical framework within which 
we can reason about the stability of algorithms related to the OAI-PMH. 
until i come up with one ;), i'm just sticking with my best judgement 
(no repeated resumptiontokens and no until parameters).

ttfn
----hussein

Michael L. Nelson wrote:

> On Tue, 5 Mar 2002, Young,Jeff wrote:
> 
> 
>>I'd be very disappointed if ETDCat required custom and unique consideration
>>from harvesters merely because of its size. Partitioning the collection
>>would be a case in point. The implication seems to be that harvesters would
>>somehow know to query the list of sets and then loop through each of them.
>>How would an arbitrary harvester know to do that, and is their software even
>>capable of it without custom coding? It would also prevent me from using
>>sets for legitimate purposes since I couldn't distinguish between them.
>>
> 
> my original point about sets was not intended to be the primary point.  if
> ETDCat already uses sets, then Alan's harvester should investigate
> harvesting by set, since it will naturally partion the collection (modulo
> Liu's point about not all records are guaranteed to be in sets).
> 
> 
>>I'd be happy to implement stateless resumptionTokens, but unless harvesters
>>know how to use them for recovery, why bother? How many harvesters today
>>could manage a recovery using stateless resumptionTokens? How many
>>harvesters will handle it tomorrow if OAI remains agnostic on the issue?
>>
>>
> 
> I'm not sure, but I would guess this would be the default behaivor.  If
> the harvester chokes, I would start again where it left off.  If
> successful harvesting continues, then there was a transient error.  If it
> fails again, then maybe the repository has a problem.
> 
> In 2.0, it will be even easier to determine:  
> 
> - when the resumptionToken expires
> - how big the result set is
> - and how many records the repository has transmitted so far
> 
> Any harvester writers our there care to comment?  Liu?  Hussein?
> 
> 
>>I'm sure ETDCat needs more stress testing to minimize future failures. The
>>fact that we've discussed this before, though, indicates a recognition that
>>problems can happen. I don't blame Alan if he doesn't want to negotiate
>>special rules for harvesting ETDCat merely because the risk is proportional
>>to the size of the repository.
>>
> 
> (I hope no one interprets my comments as beating up on ETDCat)
> 
> These are interesting points:  to the best of my knowledge, if ETDCat has
> 4M records, its by and far the biggest OAI repository out there.  My point
> is that 4M of anything is a big number, and repositories that large need
> to make sure they implement features that facilitate fault-tolerant
> harvesting.  Stateless (or very long lived) resumptionTokens would appear
> to be one of those features.  Also, if you have 4M records all with the
> same datestamp, this would seem to be an ideal candidate for some response
> caching techniques, which tied with very long lived (2-3 days?)
> resumptionTokens would seem to make for an efficent load on your end.
> 
> 
> regards,
> 
> Michael
> 
> 
>>Jeff
>>
>>-----Original Message-----
>>From: Michael L. Nelson [mailto:mln@ils.unc.edu]
>>Sent: Tuesday, March 05, 2002 10:03 AM
>>To: 'Alan Kent'
>>Cc: OAI Implementors
>>Subject: Re: [OAI-implementers] Better resumption mechanism - more
>>important than ever!
>>
>>
>>
>>actually, the way I see it is the protocol should not be complicated with
>>additional tokens and such to enforce what ETDCat (and similiarly
>>large-sized DPs) should do:
>>
>>1.  partition their collection into sets
>>2.  use stateless (or very long lived) resumptionTokens
>>
>>in 2.0, resumptionTokens will have optional attributes, including
>>"expirationDate", so this will take the guess work out of knowing how long
>>a resumptionToken will be valid.
>>
>>IMO, introducing an optional restartToken is no different (from an
>>implementer's point of view) than making the resumptionToken last a long
>>time.  
>>
>>at some point, you (as a harvester) are simply at the mercy of the
>>repository.  new features in the protocol won't change that.
>>
>>regards,
>>
>>Michael
>>
>>On Tue, 5 Mar 2002, 'Alan Kent' wrote:
>>
>>
>>>I just got some mail from Jeff at OCLC talking about ETDCat (hope
>>>you don't mind me quoting some of your mail Jeff). In particular,
>>>he just told me
>>>
>>>    ETDCat contains a lot of records (over 4 million), all of
>>>    which currently have the exact same datestamp from the initial load.
>>>
>>>He also told me that there were no sets. So basically, its all
>>>or nothing for this site because OAI has no standard way to resume
>>>if a transfer fails.
>>>
>>>If this has happened already, I think its likely to occur again.
>>>(That is, one very large database all with the same time stamp.)
>>>So any comments about having a single large collection like this
>>>is beside the point. The point is OAI does not handle it well.
>>>
>>>So I would like to resurrect the discussion again if people don't
>>>mind on how to do support restarts. My understanding of the general
>>>feeling so far is
>>>
>>>(1) Mandating support is not going to be acceptable
>>>
>>>(2) Mandating format of resumption tokens is not going to be acceptable
>>>
>>>(3) Mandating resumption tokens be long lifed (eg: can try again the
>>>    following day) is not acceptable
>>>
>>>(4) In fact, mandating that resumption tokens be unique (allowing
>>>    a token to be reused twice in quick succession to get the same
>>>    data) is not acceptable
>>>
>>>So any proposal needs to be optionally supported.
>>>
>>>Question time:
>>>
>>>Does anyone else think that this is a major hole in OAI? I personally
>>>do. After trying to crawl sites, things go wrong. The larger the site,
>>>the greater the probability that something will go wrong. The larger
>>>the site, the greater the pain of starting all over again. I do not
>>>think it is practical for anyone to harvest ETDCat if is really got
>>>4,000,000 records. Any fault, and start downloading that 4gb again!
>>>So I feel strongly on this one. In fact, I think this is the most
>>>major problem OAI has.
>>>
>>>Do people think its better to reuse resumption tokens for this purpose,
>>>or introduce a different sort of token? ETDCat for example I think
>>>allocates a session id in resumption tokens, meaning they cannot
>>>be reused when the session times out in the server (similar semantics
>>>anyway). This is a reasonable implementation decision to make.
>>>So maybe its better for servers to return an additional token,
>>>which is a <restartToken> which means a client can instead of
>>>specifying from= and to= again, specify restartToken= instead where
>>>the server then automatically works out whatever other parameters
>>>it needs, creates a new session etc internally. The new 'session'
>>>(ListXXX verb) then can use resumptionTokens to manage that new
>>>transfer.
>>>
>>>The idea is for a <restartToken> to be long lifed. It may be less
>>>efficient to use than a resumptionToken, but its only purpose is
>>>if the client fails the download. If a server does not support
>>>restartToken, it simply never returns one. Large collections *should*
>>>support restartTokens.
>>>
>>>For my harvester, I can then remember (to disk) the restartToken for
>>>every packet I get back, allowing me to recover much more easily
>>>if anything crashes. If restartToken's are too hard for someone
>>>to implement, then they don't. If you have a large data collection
>>>on the other hand, to reduce network load, I think its probably worth
>>>the extra effort of supporting restartTokens.
>>>
>>>Any comments? Better suggesions?
>>>
>>>Alan
>>>_______________________________________________
>>>OAI-implementers mailing list
>>>OAI-implementers@oaisrv.nsdl.cornell.edu
>>>http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
>>>
>>>
>>---
>>Michael L. Nelson
>>NASA Langley Research Center		m.l.nelson@larc.nasa.gov
>>MS 158, Hampton, VA 23681		http://www.ils.unc.edu/~mln/
>>+1 757 864 8511				+1 757 864 8342 (f)
>>
>>
>>_______________________________________________
>>OAI-implementers mailing list
>>OAI-implementers@oaisrv.nsdl.cornell.edu
>>http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
>>
>>
> 
> ---
> Michael L. Nelson
> NASA Langley Research Center		m.l.nelson@larc.nasa.gov
> MS 158, Hampton, VA 23681		http://www.ils.unc.edu/~mln/
> +1 757 864 8511				+1 757 864 8342 (f)
> 
> 
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 


-- 
======================================================================
hussein suleman - hussein@vt.edu - vtcs - http://www.husseinsspace.com
======================================================================


From liu_x@cs.odu.edu  Tue Mar  5 22:04:18 2002
From: liu_x@cs.odu.edu (Xiaoming Liu)
Date: Tue, 5 Mar 2002 17:04:18 -0500 (EST)
Subject: [OAI-implementers] Better resumption mechanism - more importa
 nt than ever!
In-Reply-To: <Pine.LNX.4.44.0203051442390.26364-100000@ice.cs.cornell.edu>
Message-ID: <Pine.SOL.4.10.10203051656350.1460-100000@defiant.cs.odu.edu>

On Tue, 5 Mar 2002, Simeon Warner wrote:

> 
> Liu, your policy is the sort of thing I had imagined. However, I'm curious
> about how frequently you find that a sequence of harvests fails. When I
> last did an extensive harvest (last summer) I found that, provided
> repositories had implemented the protocol properly, I rarely had problems
> getting successful responses to complete a List request. Can you give us
> some (approximate) statistics?


I did the last extensive (historical) harvest from Feb,5. Usually this is
not a problem in our daily (fresh) harvesting. The result may be the
problem in my side.

Five archives failed in a sequence of harvesting (using resumptionToken),
including: cimi, etdcat, conoze, dlpscoll, hsss. I did not dig into the
details however. See harvest log below:

etdcat: 
http://alcme.oclc.org/etdcat/servlet/OAIHandler?verb=ListRecords&resumptionToken=1013006344644%3A22000%3Aoai_dc
Wed Feb 06 09:40:20 EST 2002
java.net.SocketException: errno: 101, error: Network is unreachable for
fd: 11
java.net.SocketException: errno: 101, error: Network is unreachable for
fd: 11/

hsss:

http://hsss.slub-dresden.de/hsss/servlet/hsss.oai.OAIServlet?verb=ListRecords&resumptionToken=rt10130378386093615p9
Wed Feb 06 18:19:51 EST 2002
java.io.IOException: Server returned HTTP response code: 400 for URL:
http://hsss.slub-dresden.de/hsss/servlet/hsss.oai.OAIServlet?verb=ListRecords&resumptionToken=rt10130378386093615p9

cimi:

http://www.cimi.org/servlet/oai?verb=ListRecords&resumptionToken=5fs9fuzq3mcj471dredxulbv2rjfihmg
Thu Feb 07 01:00:56 EST 2002
java.io.IOException: Server returned HTTP response code: 400 for URL:
http://www.cimi.org/servlet/oai?verb=ListRecords&resumptionToken=5fs9fuzq3mcj471dredxulbv2rjfihmg
responsecode:400

dlpscoll:
http://www.hti.umich.edu/cgi/b/broker/broker?verb=ListRecords&resumptionToken=evd-bib,2200-01-01,1800-01-01,oai_dc,evd-bib,evd-bib.dd,200,
Thu Feb 07 06:38:05 EST 2002
java.io.IOException: Server returned HTTP response code: 500 for URL:
http://www.hti.umich.edu/cgi/b/broker/broker?verb=ListRecords&resumptionToken=evd-bib,2200-01-01,1800-01-01,oai_dc,evd-bib,evd-bib.dd,200,
responsecode:500

conoze:
http://www.conoze.com/interfaz/oai/index.php?verb=ListRecords&resumptionToken=50
Sun Mar 03 07:34:50 EST 2002
java.io.IOException: Server returned HTTP response code: 400 for URL:
http://www.conoze.com/interfaz/oai/index.php?verb=ListRecords&resumptionToken=50


regards,
liu



>  
> > Ideally, I guess a harvester could use exponential backoff algorithm to
> > keep trying until the resumptionToken is expired (Considering a
> > time-to-live parameter will be added in 2.0). And if we implment the
> > harvester in a multiple process/thread way, the system should scale well
> > for several resumptionToken errors.
> > 
> > I think something like "implementation guide" or "reference
> > implementation" will help harvester and DP understand each other well
> > beyond the core protocol.
> 
> Yes, this should certainly be covered in the implementation guidelines.
> 
> Cheers,
> Simeon.
>  
> 
> > regards,
> > liu
> > > 
> > > I'm sure ETDCat needs more stress testing to minimize future failures. The
> > > fact that we've discussed this before, though, indicates a recognition that
> > > problems can happen. I don't blame Alan if he doesn't want to negotiate
> > > special rules for harvesting ETDCat merely because the risk is proportional
> > > to the size of the repository.
> > > 
> > > Jeff
> > > 
> > > -----Original Message-----
> > > From: Michael L. Nelson [mailto:mln@ils.unc.edu]
> > > Sent: Tuesday, March 05, 2002 10:03 AM
> > > To: 'Alan Kent'
> > > Cc: OAI Implementors
> > > Subject: Re: [OAI-implementers] Better resumption mechanism - more
> > > important than ever!
> > > 
> > > actually, the way I see it is the protocol should not be complicated with
> > > additional tokens and such to enforce what ETDCat (and similiarly
> > > large-sized DPs) should do:
> > > 
> > > 1.  partition their collection into sets
> > > 2.  use stateless (or very long lived) resumptionTokens
> > > 
> > > in 2.0, resumptionTokens will have optional attributes, including
> > > "expirationDate", so this will take the guess work out of knowing how long
> > > a resumptionToken will be valid.
> > > 
> > > IMO, introducing an optional restartToken is no different (from an
> > > implementer's point of view) than making the resumptionToken last a long
> > > time.  
> > > 
> > > at some point, you (as a harvester) are simply at the mercy of the
> > > repository.  new features in the protocol won't change that.
> > > 
> > > regards,
> > > 
> > > Michael
> > > 
> > > On Tue, 5 Mar 2002, 'Alan Kent' wrote:
> > > 
> > > > I just got some mail from Jeff at OCLC talking about ETDCat (hope
> > > > you don't mind me quoting some of your mail Jeff). In particular,
> > > > he just told me
> > > > 
> > > >     ETDCat contains a lot of records (over 4 million), all of
> > > >     which currently have the exact same datestamp from the initial load.
> > > > 
> > > > He also told me that there were no sets. So basically, its all
> > > > or nothing for this site because OAI has no standard way to resume
> > > > if a transfer fails.
> > > > 
> > > > If this has happened already, I think its likely to occur again.
> > > > (That is, one very large database all with the same time stamp.)
> > > > So any comments about having a single large collection like this
> > > > is beside the point. The point is OAI does not handle it well.
> > > > 
> > > > So I would like to resurrect the discussion again if people don't
> > > > mind on how to do support restarts. My understanding of the general
> > > > feeling so far is
> > > > 
> > > > (1) Mandating support is not going to be acceptable
> > > > 
> > > > (2) Mandating format of resumption tokens is not going to be acceptable
> > > > 
> > > > (3) Mandating resumption tokens be long lifed (eg: can try again the
> > > >     following day) is not acceptable
> > > > 
> > > > (4) In fact, mandating that resumption tokens be unique (allowing
> > > >     a token to be reused twice in quick succession to get the same
> > > >     data) is not acceptable
> > > > 
> > > > So any proposal needs to be optionally supported.
> > > > 
> > > > Question time:
> > > > 
> > > > Does anyone else think that this is a major hole in OAI? I personally
> > > > do. After trying to crawl sites, things go wrong. The larger the site,
> > > > the greater the probability that something will go wrong. The larger
> > > > the site, the greater the pain of starting all over again. I do not
> > > > think it is practical for anyone to harvest ETDCat if is really got
> > > > 4,000,000 records. Any fault, and start downloading that 4gb again!
> > > > So I feel strongly on this one. In fact, I think this is the most
> > > > major problem OAI has.
> > > > 
> > > > Do people think its better to reuse resumption tokens for this purpose,
> > > > or introduce a different sort of token? ETDCat for example I think
> > > > allocates a session id in resumption tokens, meaning they cannot
> > > > be reused when the session times out in the server (similar semantics
> > > > anyway). This is a reasonable implementation decision to make.
> > > > So maybe its better for servers to return an additional token,
> > > > which is a <restartToken> which means a client can instead of
> > > > specifying from= and to= again, specify restartToken= instead where
> > > > the server then automatically works out whatever other parameters
> > > > it needs, creates a new session etc internally. The new 'session'
> > > > (ListXXX verb) then can use resumptionTokens to manage that new
> > > > transfer.
> > > > 
> > > > The idea is for a <restartToken> to be long lifed. It may be less
> > > > efficient to use than a resumptionToken, but its only purpose is
> > > > if the client fails the download. If a server does not support
> > > > restartToken, it simply never returns one. Large collections *should*
> > > > support restartTokens.
> > > > 
> > > > For my harvester, I can then remember (to disk) the restartToken for
> > > > every packet I get back, allowing me to recover much more easily
> > > > if anything crashes. If restartToken's are too hard for someone
> > > > to implement, then they don't. If you have a large data collection
> > > > on the other hand, to reduce network load, I think its probably worth
> > > > the extra effort of supporting restartTokens.
> > > > 
> > > > Any comments? Better suggesions?
> > > > 
> > > > Alan
> > > > _______________________________________________
> > > > OAI-implementers mailing list
> > > > OAI-implementers@oaisrv.nsdl.cornell.edu
> > > > http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> > > > 
> > > 
> > > ---
> > > Michael L. Nelson
> > > NASA Langley Research Center		m.l.nelson@larc.nasa.gov
> > > MS 158, Hampton, VA 23681		http://www.ils.unc.edu/~mln/
> > > +1 757 864 8511				+1 757 864 8342 (f)
> > > 
> > > 
> > > _______________________________________________
> > > OAI-implementers mailing list
> > > OAI-implementers@oaisrv.nsdl.cornell.edu
> > > http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> > > _______________________________________________
> > > OAI-implementers mailing list
> > > OAI-implementers@oaisrv.nsdl.cornell.edu
> > > http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> > > 
> > 
> > _______________________________________________
> > OAI-implementers mailing list
> > OAI-implementers@oaisrv.nsdl.cornell.edu
> > http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> > 
> 
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 


From ajk@mds.rmit.edu.au  Tue Mar  5 23:03:45 2002
From: ajk@mds.rmit.edu.au ('Alan Kent')
Date: Wed, 6 Mar 2002 10:03:45 +1100
Subject: [OAI-implementers] Better resumption mechanism - more importa nt than ever!
In-Reply-To: <Pine.SOL.4.10.10203051432340.1302-100000@defiant.cs.odu.edu>; from Xiaoming Liu on Tue, Mar 05, 2002 at 02:40:58PM -0500
References: <E5431CF93E29F9478878F623E5B9CE9834255A@OA3-SERVER.oa.oclc.org> <Pine.SOL.4.10.10203051432340.1302-100000@defiant.cs.odu.edu>
Message-ID: <20020306100345.A9210@io.mds.rmit.edu.au>

On Tue, Mar 05, 2002 at 02:40:58PM -0500, Xiaoming Liu wrote:
> I guess this is a major issue that harvester should follow a certain
> policy. In current implementation in Arc, for each failed request, the
> harvester will try at most three times using the same http request...

The problem I have with this is in the current OAI spec as I read
it, this is not a safe thing to do. OAI does not mandate that
resumptionToken's change. That is, resumptionToken *could* be
a session id, with no cursor information in it. Each request
using the same resumptionToken therefore is permitted to return
the next N records.

So I could implement a data provider where the above algorithm is
wrong and will silently deliver incorrect results.

Does OAI 2.0 say that resumptionToken's must be unique within
a download? And that reusing an old resumptionToken must be
supported (or rejected with an error)? If not guaranteed by
the spec, then I would not want to write a harvester relying
on it. I would rather spend the effort and get the spec right
rather than having to come to agreements with individual data
providers.

One approach discussed was of course to add something to the
Identify response allowing servers to advertise 'its safe
to reuse resumptionTokens'. More in a later mail.

(I am not proposing anything in this mail - just saying I believe
the above retry algorithm is theoretically unsafe.)

Alan

From ajk@mds.rmit.edu.au  Tue Mar  5 23:50:53 2002
From: ajk@mds.rmit.edu.au ('Alan Kent')
Date: Wed, 6 Mar 2002 10:50:53 +1100
Subject: [OAI-implementers] Better resumption mechanism - more important than ever!
In-Reply-To: <Pine.GSO.4.21.0203050947320.29139-100000@ruby.ils.unc.edu>; from Michael L. Nelson on Tue, Mar 05, 2002 at 10:02:43AM -0500
References: <20020305200021.R1467@io.mds.rmit.edu.au> <Pine.GSO.4.21.0203050947320.29139-100000@ruby.ils.unc.edu>
Message-ID: <20020306105053.B9210@io.mds.rmit.edu.au>

On Tue, Mar 05, 2002 at 10:02:43AM -0500, Michael L. Nelson wrote:
> actually, the way I see it is the protocol should not be complicated with
> additional tokens and such to enforce what ETDCat (and similiarly
> large-sized DPs) should do:
> 
> 1.  partition their collection into sets

I am sorry, but I agree with other's here that sets are not the
solution. How are the sets going to be created? Are they going
to have any semantics (or just 1,000 records per set)? What if I
do want semantics for my sets, but one set does have a 1,000,000
records? What happens when people start creating even bigger
collections? Etc. I think sets can be useful, but I would not
*rely* on them as solving the problem.

> 2.  use stateless (or very long lived) resumptionTokens
> 
> in 2.0, resumptionTokens will have optional attributes, including
> "expirationDate", so this will take the guess work out of knowing how long
> a resumptionToken will be valid.
> 
> IMO, introducing an optional restartToken is no different (from an
> implementer's point of view) than making the resumptionToken last a long
> time.  

I am going to play devil's advocate a bit here - I think its worth
teasing out arguments a bit more to make sure they are solid.

There is a difference, but is the difference worth the complexity
to the protocol? That is a different question.

For example, if I was going to build a data supplier implementation
(I am actually thinking about how it would be done), then I would
layer it on top of Z39.50 - because that is what our database server
uses. Z39.50 has a result set concept. So I would do a search,
then the resumptionToken would be the result set name. If I had
to make resumptionTokens unique (not currently required I believe),
then I would add the offset into the result set. Since result
sets are stored in the server, I might use a timeout of 10 minutes,
maybe an hour, certainly not a few days. Each result set uses up
memory in the server! Note that because I have a Z39.50 result,
I don't need to worry about updates of data in the server.
My result set won't change in size during the transfer, so I can
implement idempotent resumptionTokens easily.

So how to support restarting if something goes wrong? Well, I could
implement a restartToken which encoded the original request and
the OAI record identifier I was up to. Note, I would not store the
result set index. I have to redo the query, the database may have
changed, so the old index is no longer guaranteed to be correct.
(I would sort the result sets in the server to make my implementation
easy). My restart query would be the old from/until stuff, plus an
addtional 'id >= id-from-restartToken' so the new result set would
be smaller.

How long would my restartToken be valid for? I could say months
or years. How long would my resumptionToken be valid for? minutes
or hours, not days. Remember that if a transfer fails, my data
provider code is not sure how long before (or if) the client is
going to retry. If the harvester says 'help! I need human
intervension', then the delay could be significant.

So my *personal* feeling is restartToken's should have a life in
terms of at least a week. Certainly multiple days. I think this
might be too much of a restriction on resumption tokens.


Some other points worth noting:

* If a server does support long term resumption tokens, then they
  can return exactly the same string for both resumption tokens
  and restart tokens. So implementation is not that much harder.

* It is reasonable for a request using a restart token to return
  a different set of records (due to database updates) than the
  old request. It is also reasonable for a server not to return
  a restart token for every response - it could, for example,
  return a restart token every time the day or year changes in
  returned records (if the implementation returns them in order)
  allowing the harvester to avoid doing *all* the work again,
  even if some effort is repeated. (ie: more flexibility).

* Is enhancing the Identify verb response (in a standard way) a
  good model to move to? It is a real option, and a reasonable one.
  But so far OAI has not required harvesters to do this sort of
  look into what the server provides. Do people want to start now?
  (Phylisophical question here worth asking.) Using restartToken
  does not require usage of examining Identify responses.

* For small servers, they do not have to implement restartToken at
  all. In that case, harvesters just redo the whole request.
  So this is not mandatory additional code to write.

* For people who have written code to implement a data provider,
  how much of a burden is there for resumptionToken's to be valid
  for a long period of time? (eg: a week). Would a separate
  restartToken be any use?

* For data provider programmers again, if the data provider server
  goes down (eg: shut down nightly for backups or something), will
  it be easy to make resumption tokens survive across such events?

* Has OAI 2.0 decreed that resumptionToken's can be reused? (Idempotent)
  If not, then they cannot be used to recover - unless again something
  is added to Identify for harvesters to say 'oh, I can try a reload'.


Taking my horns off for a moment, I also agree that keeping the protocol
simple is a very good thing.

But I am not (yet) convinced (oh dear, those horns don't come off that
easily do they >;^) that that forcing resumptionTokens to have a longer
life is actually simplifying the job of implementors. And I don't think
short life resumptionTokens (less than a few days at least) will solve
the restart problem.  Semantically, to me resumptionToken's are used as
a protocol mechanism to link multiple packets into a single request.
RestartTokens are used to recover after a failure by starting a
completely new request.

> at some point, you (as a harvester) are simply at the mercy of the
> repository.  new features in the protocol won't change that.

That is true, but that does not mean to me that the protocol cannot
be improved to make the protocol more robust. With OAI as it is,
I am not going to try and crawl ETDCat any more. Even with more
precise date stamps (lets say every ETDCat record has a different
stamp), because results are not guaranteed to come back in sorted
order, I cannot restart using from=. I must start again from scratch.

I think the real question is will data provider implementers be
happy with resumptonToken's lasting for a week. For me *personally*,
it will be easier having two separate tokens. But I think its wrong
to design the protocol around my intended implementation (which does
not even - and may never - exist! :-)

Alan

From liu_x@cs.odu.edu  Wed Mar  6 02:53:54 2002
From: liu_x@cs.odu.edu (Xiaoming Liu)
Date: Tue, 5 Mar 2002 21:53:54 -0500 (EST)
Subject: [OAI-implementers] Better resumption mechanism - more important
 than ever!
In-Reply-To: <20020306105053.B9210@io.mds.rmit.edu.au>
Message-ID: <Pine.SOL.4.10.10203052055270.27255-100000@dilbert.cs.odu.edu>

Alan,

I think there are two questions here.

1) Could the resumptionToken (in your case restartToken) be re-used? 

I agree the retry algorithem is theoretically unsafe in current protocol,
thanks. However, the same question also exists in "restartToken" and 
must be addressed before we talk about question 2. If they can not be
re-used, the harvester has to start from scratch. It looks like the OAI
1.1 doesn't give clear answer to this question. Hopefully it
could be answsered in 2.0

2) If it is legal to re-use, should we introduce a restartToken concept?

My personal opinion is restartToken will bring too much complexity, and
it's not necessary.

In your case, I could imagine it can be done by current OAI 
resumptionToken: assume the proposed tokens in your suggestion are called
alan_restartToken and  alan_resumptionToken respectively. 

oai_resumptionToken=alan_restartToken + alan_resumptionToken

So data provider (DP) can always parse the oai_resumptionToken, in most
case, the session is valid and DP just uses alan_resumptionToken; if
anything goes wrong, DP need redo the query, DP have the freedom to use
the alan_restartToken. The harvester should not know what happens behind
the scene. At this scenario, the time-to-live could be month, year ;-)

Another small doubt.

>precise date stamps (lets say every ETDCat record has a different
>stamp), because results are not guaranteed to come back in sorted
>order, I cannot restart using from=. I must start again from scratch.

I think it's perfectly correct if you restart using from=. For example, if
you finished everything <=1980, and when you are doing something
"from=1981", system crashed, I think it's correct if you do "from=1981"
again. Because the protocol guarantees you have everything before 1981.

regards,
liu









On Wed, 6 Mar 2002, 'Alan Kent' wrote:

> On Tue, Mar 05, 2002 at 10:02:43AM -0500, Michael L. Nelson wrote:
> > actually, the way I see it is the protocol should not be complicated with
> > additional tokens and such to enforce what ETDCat (and similiarly
> > large-sized DPs) should do:
> > 
> > 1.  partition their collection into sets
> 
> I am sorry, but I agree with other's here that sets are not the
> solution. How are the sets going to be created? Are they going
> to have any semantics (or just 1,000 records per set)? What if I
> do want semantics for my sets, but one set does have a 1,000,000
> records? What happens when people start creating even bigger
> collections? Etc. I think sets can be useful, but I would not
> *rely* on them as solving the problem.
> 
> > 2.  use stateless (or very long lived) resumptionTokens
> > 
> > in 2.0, resumptionTokens will have optional attributes, including
> > "expirationDate", so this will take the guess work out of knowing how long
> > a resumptionToken will be valid.
> > 
> > IMO, introducing an optional restartToken is no different (from an
> > implementer's point of view) than making the resumptionToken last a long
> > time.  
> 
> I am going to play devil's advocate a bit here - I think its worth
> teasing out arguments a bit more to make sure they are solid.
> 
> There is a difference, but is the difference worth the complexity
> to the protocol? That is a different question.
> 
> For example, if I was going to build a data supplier implementation
> (I am actually thinking about how it would be done), then I would
> layer it on top of Z39.50 - because that is what our database server
> uses. Z39.50 has a result set concept. So I would do a search,
> then the resumptionToken would be the result set name. If I had
> to make resumptionTokens unique (not currently required I believe),
> then I would add the offset into the result set. Since result
> sets are stored in the server, I might use a timeout of 10 minutes,
> maybe an hour, certainly not a few days. Each result set uses up
> memory in the server! Note that because I have a Z39.50 result,
> I don't need to worry about updates of data in the server.
> My result set won't change in size during the transfer, so I can
> implement idempotent resumptionTokens easily.
> 
> So how to support restarting if something goes wrong? Well, I could
> implement a restartToken which encoded the original request and
> the OAI record identifier I was up to. Note, I would not store the
> result set index. I have to redo the query, the database may have
> changed, so the old index is no longer guaranteed to be correct.
> (I would sort the result sets in the server to make my implementation
> easy). My restart query would be the old from/until stuff, plus an
> addtional 'id >= id-from-restartToken' so the new result set would
> be smaller.
> 
> How long would my restartToken be valid for? I could say months
> or years. How long would my resumptionToken be valid for? minutes
> or hours, not days. Remember that if a transfer fails, my data
> provider code is not sure how long before (or if) the client is
> going to retry. If the harvester says 'help! I need human
> intervension', then the delay could be significant.
> 
> So my *personal* feeling is restartToken's should have a life in
> terms of at least a week. Certainly multiple days. I think this
> might be too much of a restriction on resumption tokens.
> 
> 
> Some other points worth noting:
> 
> * If a server does support long term resumption tokens, then they
>   can return exactly the same string for both resumption tokens
>   and restart tokens. So implementation is not that much harder.
> 
> * It is reasonable for a request using a restart token to return
>   a different set of records (due to database updates) than the
>   old request. It is also reasonable for a server not to return
>   a restart token for every response - it could, for example,
>   return a restart token every time the day or year changes in
>   returned records (if the implementation returns them in order)
>   allowing the harvester to avoid doing *all* the work again,
>   even if some effort is repeated. (ie: more flexibility).
> 
> * Is enhancing the Identify verb response (in a standard way) a
>   good model to move to? It is a real option, and a reasonable one.
>   But so far OAI has not required harvesters to do this sort of
>   look into what the server provides. Do people want to start now?
>   (Phylisophical question here worth asking.) Using restartToken
>   does not require usage of examining Identify responses.
> 
> * For small servers, they do not have to implement restartToken at
>   all. In that case, harvesters just redo the whole request.
>   So this is not mandatory additional code to write.
> 
> * For people who have written code to implement a data provider,
>   how much of a burden is there for resumptionToken's to be valid
>   for a long period of time? (eg: a week). Would a separate
>   restartToken be any use?
> 
> * For data provider programmers again, if the data provider server
>   goes down (eg: shut down nightly for backups or something), will
>   it be easy to make resumption tokens survive across such events?
> 
> * Has OAI 2.0 decreed that resumptionToken's can be reused? (Idempotent)
>   If not, then they cannot be used to recover - unless again something
>   is added to Identify for harvesters to say 'oh, I can try a reload'.
> 
> 
> Taking my horns off for a moment, I also agree that keeping the protocol
> simple is a very good thing.
> 
> But I am not (yet) convinced (oh dear, those horns don't come off that
> easily do they >;^) that that forcing resumptionTokens to have a longer
> life is actually simplifying the job of implementors. And I don't think
> short life resumptionTokens (less than a few days at least) will solve
> the restart problem.  Semantically, to me resumptionToken's are used as
> a protocol mechanism to link multiple packets into a single request.
> RestartTokens are used to recover after a failure by starting a
> completely new request.
> 
> > at some point, you (as a harvester) are simply at the mercy of the
> > repository.  new features in the protocol won't change that.
> 
> That is true, but that does not mean to me that the protocol cannot
> be improved to make the protocol more robust. With OAI as it is,
> I am not going to try and crawl ETDCat any more. Even with more
> precise date stamps (lets say every ETDCat record has a different
> stamp), because results are not guaranteed to come back in sorted
> order, I cannot restart using from=. I must start again from scratch.
> 
> I think the real question is will data provider implementers be
> happy with resumptonToken's lasting for a week. For me *personally*,
> it will be easier having two separate tokens. But I think its wrong
> to design the protocol around my intended implementation (which does
> not even - and may never - exist! :-)
> 
> Alan
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 


From mln@ils.unc.edu  Wed Mar  6 03:42:26 2002
From: mln@ils.unc.edu (Michael L. Nelson)
Date: Tue, 5 Mar 2002 22:42:26 -0500 (EST)
Subject: [OAI-implementers] Better resumption mechanism - more important
 than ever!
In-Reply-To: <20020306105053.B9210@io.mds.rmit.edu.au>
Message-ID: <Pine.GSO.4.21.0203052149210.7821-100000@ruby.ils.unc.edu>

(combining the last 2 emails into 1 respone)

>> I guess this is a major issue that harvester should follow a certain
>> policy. In current implementation in Arc, for each failed request, the
>> harvester will try at most three times using the same http request...

>The problem I have with this is in the current OAI spec as I read
>it, this is not a safe thing to do. OAI does not mandate that
>resumptionToken's change. That is, resumptionToken *could* be
>a session id, with no cursor information in it. Each request
>using the same resumptionToken therefore is permitted to return
>the next N records.

>So I could implement a data provider where the above algorithm is
>wrong and will silently deliver incorrect results.

in OAI 2.0, resumptionTokens are slated to have the optional attributes
of:

- resumeAfter (how long to wait before issuing the next request)
- expirationDate (TTL for the resumptionToken)
- completeListSize (total # of records in the complete list)
- cursor (number of items returned so far)

granted, a repository does not have to implement these, but it the
concerns we are talking about behooves them to.

so if you re-issue a resumptionToken, and completeListSize or cursor are
not what you expect, then something has gone wrong.  and if the
resumptionToken is past its expiration, the repository will respond with a
badResumptionToken error.

>Does OAI 2.0 say that resumptionToken's must be unique within
>a download? And that reusing an old resumptionToken must be
>supported (or rejected with an error)? If not guaranteed by
>the spec, then I would not want to write a harvester relying
>on it. I would rather spend the effort and get the spec right
>rather than having to come to agreements with individual data
>providers.

I don't think the spec currently requires that a repository reject expired
resumptionTokens, but a harvester would be wise not to use them if they
are expired.  its like drinking milk a day or two after the expiration
date:  its *probably* ok, but you gotta be pretty thirsty to do it.

>One approach discussed was of course to add something to the
>Identify response allowing servers to advertise 'its safe
>to reuse resumptionTokens'. More in a later mail.

agreed, below in my mesg also.

>(I am not proposing anything in this mail - just saying I believe
>the above retry algorithm is theoretically unsafe.)

> On Tue, Mar 05, 2002 at 10:02:43AM -0500, Michael L. Nelson wrote:
> > actually, the way I see it is the protocol should not be complicated with
> > additional tokens and such to enforce what ETDCat (and similiarly
> > large-sized DPs) should do:
> > 
> > 1.  partition their collection into sets
> 
> I am sorry, but I agree with other's here that sets are not the
> solution. How are the sets going to be created? Are they going
> to have any semantics (or just 1,000 records per set)? What if I
> do want semantics for my sets, but one set does have a 1,000,000
> records? 

1M < 4M

> What happens when people start creating even bigger
> collections? Etc. I think sets can be useful, but I would not
> *rely* on them as solving the problem.
> 

I never meant to imply that they should be relied on, only that if they
exist, they can make harvesting easier.  I'll defer further set discussion
since the main topic is resumptionToken mechanics (sets can chew up a
whole other thread ;-)

> > 2.  use stateless (or very long lived) resumptionTokens
> > 
> > in 2.0, resumptionTokens will have optional attributes, including
> > "expirationDate", so this will take the guess work out of knowing how long
> > a resumptionToken will be valid.
> > 
> > IMO, introducing an optional restartToken is no different (from an
> > implementer's point of view) than making the resumptionToken last a long
> > time.  
> 
> I am going to play devil's advocate a bit here - I think its worth
> teasing out arguments a bit more to make sure they are solid.
> 
> There is a difference, but is the difference worth the complexity
> to the protocol? That is a different question.

I'll rephrase my answer:  the repository can implement it so there is no
difference.

> 
> For example, if I was going to build a data supplier implementation
> (I am actually thinking about how it would be done), then I would
> layer it on top of Z39.50 - because that is what our database server
> uses. Z39.50 has a result set concept. So I would do a search,
> then the resumptionToken would be the result set name. If I had
> to make resumptionTokens unique (not currently required I believe),
> then I would add the offset into the result set. Since result
> sets are stored in the server, I might use a timeout of 10 minutes,
> maybe an hour, certainly not a few days. Each result set uses up
> memory in the server! Note that because I have a Z39.50 result,

this is an artifact of your implementation... write the result set out to
disk and set the expirationDate to a few days.  add a reasonable response
caching algorithm, and you could end up with a huge performance
win.  Depending on the DP accession rate, harvesting patterns, etc., your
mileage could vary, but I suspect it would be very good.

> I don't need to worry about updates of data in the server.
> My result set won't change in size during the transfer, so I can
> implement idempotent resumptionTokens easily.
> 
> So how to support restarting if something goes wrong? Well, I could
> implement a restartToken which encoded the original request and
> the OAI record identifier I was up to. Note, I would not store the
> result set index. I have to redo the query, the database may have
> changed, so the old index is no longer guaranteed to be correct.
> (I would sort the result sets in the server to make my implementation
> easy). My restart query would be the old from/until stuff, plus an
> addtional 'id >= id-from-restartToken' so the new result set would
> be smaller.

I think this is all doable with resumptionTokens.  As Liu suggest in his
email, you can embed your restartTokens in your resumptionTokens.

> 
> How long would my restartToken be valid for? I could say months
> or years. How long would my resumptionToken be valid for? minutes
> or hours, not days. Remember that if a transfer fails, my data
> provider code is not sure how long before (or if) the client is
> going to retry. If the harvester says 'help! I need human
> intervension', then the delay could be significant.
> 
> So my *personal* feeling is restartToken's should have a life in
> terms of at least a week. Certainly multiple days. I think this
> might be too much of a restriction on resumption tokens.
> 
> 
> Some other points worth noting:
> 
> * If a server does support long term resumption tokens, then they
>   can return exactly the same string for both resumption tokens
>   and restart tokens. So implementation is not that much harder.
> 
> * It is reasonable for a request using a restart token to return
>   a different set of records (due to database updates) than the
>   old request. It is also reasonable for a server not to return
>   a restart token for every response - it could, for example,
>   return a restart token every time the day or year changes in
>   returned records (if the implementation returns them in order)
>   allowing the harvester to avoid doing *all* the work again,
>   even if some effort is repeated. (ie: more flexibility).
> 
> * Is enhancing the Identify verb response (in a standard way) a
>   good model to move to? It is a real option, and a reasonable one.
>   But so far OAI has not required harvesters to do this sort of
>   look into what the server provides. Do people want to start now?
>   (Phylisophical question here worth asking.) Using restartToken
>   does not require usage of examining Identify responses.

2.0 will already have more machine processable information in the Identify
response.  I'm not sure there is a good way around it, and since that
door is already open, if you want to provide hints about how your
resumptionTokens are used/implemented, that's surely ok.

> 
> * For small servers, they do not have to implement restartToken at
>   all. In that case, harvesters just redo the whole request.
>   So this is not mandatory additional code to write.
> 
> * For people who have written code to implement a data provider,
>   how much of a burden is there for resumptionToken's to be valid
>   for a long period of time? (eg: a week). Would a separate
>   restartToken be any use?

my DPs use stateless resumptionTokens, resuming and restarting are the
similar.

> 
> * For data provider programmers again, if the data provider server
>   goes down (eg: shut down nightly for backups or something), will
>   it be easy to make resumption tokens survive across such events?

I would implement stateful resumptionTokens in a disk cache, so recovery
would not be a problem.

> 
> * Has OAI 2.0 decreed that resumptionToken's can be reused? (Idempotent)
>   If not, then they cannot be used to recover - unless again something
>   is added to Identify for harvesters to say 'oh, I can try a reload'.
> 

good point...  perhaps if a DP sets an expirationDate, it should
idempotenly (?!) honor the resumptionToken until that date...  hmm...  

this would be a good thing when the response to the harvester is lost, and
the harvester reissues a request with the same resumptionToken...  hmm...

> 
> Taking my horns off for a moment, I also agree that keeping the protocol
> simple is a very good thing.
> 
> But I am not (yet) convinced (oh dear, those horns don't come off that
> easily do they >;^) that that forcing resumptionTokens to have a longer
> life is actually simplifying the job of implementors. And I don't think
> short life resumptionTokens (less than a few days at least) will solve
> the restart problem.  Semantically, to me resumptionToken's are used as
> a protocol mechanism to link multiple packets into a single request.
> RestartTokens are used to recover after a failure by starting a
> completely new request.

We allow the DP to choose the syntax of the resumptionToken, and as this
discussion has revealed, there is some wiggle room in allowing them to
choose (perhaps "nudge" is better) the semantics as well.  Ultimately, I
think this is a good thing.  Most DPs won't need all of this.  But the
ones that want to implement a certain policy or effect can do so.

> 
> > at some point, you (as a harvester) are simply at the mercy of the
> > repository.  new features in the protocol won't change that.
> 
> That is true, but that does not mean to me that the protocol cannot
> be improved to make the protocol more robust. With OAI as it is,
> I am not going to try and crawl ETDCat any more. Even with more
> precise date stamps (lets say every ETDCat record has a different
> stamp), because results are not guaranteed to come back in sorted
> order, I cannot restart using from=. I must start again from scratch.

but if their resumptionTokens had a long life, and were idempotent within
that lifetime you would not have to start from scratch.  2.0 will allow
the specification of the former, and we should probably discuss the latter
some more.

> 
> I think the real question is will data provider implementers be
> happy with resumptonToken's lasting for a week. For me *personally*,
> it will be easier having two separate tokens. But I think its wrong
> to design the protocol around my intended implementation (which does
> not even - and may never - exist! :-)

you better build your system after all this!  ;-)

seriously, you bring up a lot of good points.  a lot of this exchange
should probably be reflected in the implementation guide that will
accompany the protocol doc.

regards,

Michael

> 
> Alan
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 

---
Michael L. Nelson
NASA Langley Research Center		m.l.nelson@larc.nasa.gov
MS 158, Hampton, VA 23681		http://www.ils.unc.edu/~mln/
+1 757 864 8511				+1 757 864 8342 (f)




From ajk@mds.rmit.edu.au  Wed Mar  6 06:44:12 2002
From: ajk@mds.rmit.edu.au ('Alan Kent')
Date: Wed, 6 Mar 2002 17:44:12 +1100
Subject: [OAI-implementers] Better resumption mechanism - more important than ever!
In-Reply-To: <Pine.SOL.4.10.10203052055270.27255-100000@dilbert.cs.odu.edu>; from Xiaoming Liu on Tue, Mar 05, 2002 at 09:53:54PM -0500
References: <20020306105053.B9210@io.mds.rmit.edu.au> <Pine.SOL.4.10.10203052055270.27255-100000@dilbert.cs.odu.edu>
Message-ID: <20020306174412.F9210@io.mds.rmit.edu.au>

On Tue, Mar 05, 2002 at 09:53:54PM -0500, Xiaoming Liu wrote:
> 1) Could the resumptionToken (in your case restartToken) be re-used? 
> 
> I agree the retry algorithem is theoretically unsafe in current protocol,
> thanks. However, the same question also exists in "restartToken" and 
> must be addressed before we talk about question 2. If they can not be
> re-used, the harvester has to start from scratch. It looks like the OAI
> 1.1 doesn't give clear answer to this question. Hopefully it
> could be answsered in 2.0

*If* restartToken was introduced, it would be idempotent-ish. Its whole
purpose would be to allow it to be reissued and old results come back.
I would define it as "returning all of the records not returned so
far in the current transfer, possibly including other records that
have already been transferred".

> 2) If it is legal to re-use, should we introduce a restartToken concept?
> 
> My personal opinion is restartToken will bring too much complexity, and
> it's not necessary.

I agree the concensus is to stick to resumptionToken's. That's fine.
Just pushing restartToken to see what problems/issues arrise.

I would therefore instead propose that there be a standard way in
the Identify response to say 'resumptionToken's are idempotent'
and also 'resumptionToken's can be rerequested' in case of network
failure. DP implementors *should* also try to make them long lifed
(days to weeks) for large repositories.

> In your case, I could imagine it can be done by current OAI 
> resumptionToken: assume the proposed tokens in your suggestion are called
> alan_restartToken and  alan_resumptionToken respectively. 
> 
> oai_resumptionToken=alan_restartToken + alan_resumptionToken
> 
> So data provider (DP) can always parse the oai_resumptionToken, in most
> case, the session is valid and DP just uses alan_resumptionToken; if
> anything goes wrong, DP need redo the query, DP have the freedom to use
> the alan_restartToken. The harvester should not know what happens behind
> the scene. At this scenario, the time-to-live could be month, year ;-)

That works. I can use it with my Z39.50 result set/query. If the
result set is still around, reuse it. If it has timed out, redo the
query. I would not be able to return the number of records (when
redoing the query, the number might change), but overall things
would work.

Alan

From ajk@mds.rmit.edu.au  Wed Mar  6 07:11:38 2002
From: ajk@mds.rmit.edu.au (Alan Kent)
Date: Wed, 6 Mar 2002 18:11:38 +1100
Subject: [OAI-implementers] Better resumption mechanism - more important than ever!
In-Reply-To: <Pine.GSO.4.21.0203052149210.7821-100000@ruby.ils.unc.edu>; from Michael L. Nelson on Tue, Mar 05, 2002 at 10:42:26PM -0500
References: <20020306105053.B9210@io.mds.rmit.edu.au> <Pine.GSO.4.21.0203052149210.7821-100000@ruby.ils.unc.edu>
Message-ID: <20020306181138.G9210@io.mds.rmit.edu.au>

On Tue, Mar 05, 2002 at 10:42:26PM -0500, Michael L. Nelson wrote:
> >Does OAI 2.0 say that resumptionToken's must be unique within
> >a download? And that reusing an old resumptionToken must be
> >supported (or rejected with an error)? If not guaranteed by
> >the spec, then I would not want to write a harvester relying
> >on it. I would rather spend the effort and get the spec right
> >rather than having to come to agreements with individual data
> >providers.
> 
> I don't think the spec currently requires that a repository reject expired
> resumptionTokens, but a harvester would be wise not to use them if they
> are expired.  its like drinking milk a day or two after the expiration
> date:  its *probably* ok, but you gotta be pretty thirsty to do it.

This is not quite what I meant. Its more if my resumption token is
just a result set name and the DP remembers where the transfer is
up to, the reusing the same token would get the next N records.
The current spec allows this.

I think OAI 2.0 should allow a DP to advertise (via Identify?) that
it resumptionToken's can be reused (are idempotent) to retry.
That would satisfy me.

> > There is a difference, but is the difference worth the complexity
> > to the protocol? That is a different question.
> 
> I'll rephrase my answer:  the repository can implement it so there is no
> difference.

I agree that a DP can implement (idempotent resumption tokens), but how
does a harvester know that the DP has implemented it? Either OAI 2.0
must mandate it (possibly overly restrictive for smaller repositories),
or DP's must be able advertise it in a standard way, such as in the
Identify response.

So not much needs to be done, but something does need to be done.
The 1.1 spec at present is not enough.

> this is an artifact of your implementation... write the result set out to
> disk and set the expirationDate to a few days.  add a reasonable response
> caching algorithm, and you could end up with a huge performance
> win.  Depending on the DP accession rate, harvesting patterns, etc., your
> mileage could vary, but I suspect it would be very good.

I would never write the result set out to disk. For a very large
result set (eg: 10,000,000 records), I would have to fetch all the
records (lots of disk accesses) get their OAI-id's, then start
transferring. Then how long to keep the temporary file around for?
How many people might be doing transfers at the same time?
(A Z39.50 result set is not a client side data structure, but
a server side data structure by the way.)

But Liu had a good solution - just store both what I called resumptionToken
and restartToken in the resumptionToken. Ie: the result set name and
query. If the result set has timed out, use the query part and build
it up again. Its up to me to get it correct. I personally would
have problems with cursors and list sizes (I would not support them
because if I redid the query, the result set size may change and
so both the size and cursor would be invalidated). But I can munge
my own DP implementation stuff in there to do something pretty similar
(my own internal concept of a 'cursor').

> 2.0 will already have more machine processable information in the Identify
> response.  I'm not sure there is a good way around it, and since that
> door is already open, if you want to provide hints about how your
> resumptionTokens are used/implemented, that's surely ok.

Ok, then I think advertise a little more about idempotency of
resumptionToken's and everything is fine. Implementors for large
repositories should try to have long time-to-live for resumption
tokens, but no protocol change is required.

> but if their resumptionTokens had a long life, and were idempotent within
> that lifetime you would not have to start from scratch.  2.0 will allow
> the specification of the former, and we should probably discuss the latter
> some more.

Agreed. The simplest solution is (as above) to allow a server to advertise
its resumptionToken's are idempotent.

> you better build your system after all this!  ;-)

*-)

One problem is I dont have any data to export - only data that other
people have made available. The other problem relates to number of
hours in the day. I still want to put my harvested collection up
for public access to if I can scrounge up the disk space.

> seriously, you bring up a lot of good points.  a lot of this exchange
> should probably be reflected in the implementation guide that will
> accompany the protocol doc.

I think the conclusions, such as 'advertise idempontency, and make resumption
tokens long lifed to handle where a harvester hits a problem and waits
for a humam to try and keep going' are worth documenting, not the rest.
There are always the mail archives.

Alan

From jak@library.ucsf.edu  Tue Mar  5 15:56:44 2002
From: jak@library.ucsf.edu (John A. Kunze)
Date: Tue, 5 Mar 2002 07:56:44 -0800 (PST)
Subject: [OAI-implementers] many records, same datestamp [was: Better resumption
 mechanism - more important than ever!]
In-Reply-To: <20020305200021.R1467@io.mds.rmit.edu.au>
Message-ID: <Pine.GSO.4.44.0203050737530.20797-100000@tweety.ckm.ucsf.edu>

I can't comment on the suggestion (I haven't studied the problem
enough), but we are facing this awkward situation too.

We have a collection of about 4 million records (internal tobacco
industry documents made available to the tobacco control community),
all loaded at about the same time.

-John


--- On Tue, 5 Mar 2002, 'Alan Kent' wrote:

Date: Tue, 5 Mar 2002 20:00:22 +1100
From: 'Alan Kent' <ajk@mds.rmit.edu.au>
To: OAI Implementors <oai-implementers@oaisrv.nsdl.cornell.edu>
Subject: [OAI-implementers] Better resumption mechanism - more important
    than ever!

I just got some mail from Jeff at OCLC talking about ETDCat (hope
you don't mind me quoting some of your mail Jeff). In particular,
he just told me

    ETDCat contains a lot of records (over 4 million), all of
    which currently have the exact same datestamp from the initial load.

He also told me that there were no sets. So basically, its all
or nothing for this site because OAI has no standard way to resume
if a transfer fails.

If this has happened already, I think its likely to occur again.
(That is, one very large database all with the same time stamp.)
So any comments about having a single large collection like this
is beside the point. The point is OAI does not handle it well.

So I would like to resurrect the discussion again if people don't
mind on how to do support restarts. My understanding of the general
feeling so far is

(1) Mandating support is not going to be acceptable

(2) Mandating format of resumption tokens is not going to be acceptable

(3) Mandating resumption tokens be long lifed (eg: can try again the
    following day) is not acceptable

(4) In fact, mandating that resumption tokens be unique (allowing
    a token to be reused twice in quick succession to get the same
    data) is not acceptable

So any proposal needs to be optionally supported.

Question time:

Does anyone else think that this is a major hole in OAI? I personally
do. After trying to crawl sites, things go wrong. The larger the site,
the greater the probability that something will go wrong. The larger
the site, the greater the pain of starting all over again. I do not
think it is practical for anyone to harvest ETDCat if is really got
4,000,000 records. Any fault, and start downloading that 4gb again!
So I feel strongly on this one. In fact, I think this is the most
major problem OAI has.

Do people think its better to reuse resumption tokens for this purpose,
or introduce a different sort of token? ETDCat for example I think
allocates a session id in resumption tokens, meaning they cannot
be reused when the session times out in the server (similar semantics
anyway). This is a reasonable implementation decision to make.
So maybe its better for servers to return an additional token,
which is a <restartToken> which means a client can instead of
specifying from= and to= again, specify restartToken= instead where
the server then automatically works out whatever other parameters
it needs, creates a new session etc internally. The new 'session'
(ListXXX verb) then can use resumptionTokens to manage that new
transfer.

The idea is for a <restartToken> to be long lifed. It may be less
efficient to use than a resumptionToken, but its only purpose is
if the client fails the download. If a server does not support
restartToken, it simply never returns one. Large collections *should*
support restartTokens.

For my harvester, I can then remember (to disk) the restartToken for
every packet I get back, allowing me to recover much more easily
if anything crashes. If restartToken's are too hard for someone
to implement, then they don't. If you have a large data collection
on the other hand, to reduce network load, I think its probably worth
the extra effort of supporting restartTokens.

Any comments? Better suggesions?

Alan
_______________________________________________
OAI-implementers mailing list
OAI-implementers@oaisrv.nsdl.cornell.edu
http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers


From jak@ckm.ucsf.edu  Tue Mar  5 23:46:23 2002
From: jak@ckm.ucsf.edu (John A. Kunze)
Date: Tue, 5 Mar 2002 15:46:23 -0800 (PST)
Subject: [OAI-implementers] many records, same datestamp [was: Better resumption
 mechanism - more important than ever!]
In-Reply-To: <Pine.GSO.4.44.0203050737530.20797-100000@tweety.ckm.ucsf.edu>
Message-ID: <Pine.GSO.4.44.0203051545370.20866-100000@tweety.ckm.ucsf.edu>

I can't comment on the suggestion (I haven't studied the problem
enough), but we are facing this awkward situation too.

We have a collection of about 4 million records (internal tobacco
industry documents made available to the tobacco control community),
all loaded at about the same time.

-John


--- On Tue, 5 Mar 2002, 'Alan Kent' wrote:

Date: Tue, 5 Mar 2002 20:00:22 +1100
From: 'Alan Kent' <ajk@mds.rmit.edu.au>
To: OAI Implementors <oai-implementers@oaisrv.nsdl.cornell.edu>
Subject: [OAI-implementers] Better resumption mechanism - more important
    than ever!

I just got some mail from Jeff at OCLC talking about ETDCat (hope
you don't mind me quoting some of your mail Jeff). In particular,
he just told me

    ETDCat contains a lot of records (over 4 million), all of
    which currently have the exact same datestamp from the initial load.

He also told me that there were no sets. So basically, its all
or nothing for this site because OAI has no standard way to resume
if a transfer fails.

If this has happened already, I think its likely to occur again.
(That is, one very large database all with the same time stamp.)
So any comments about having a single large collection like this
is beside the point. The point is OAI does not handle it well.

So I would like to resurrect the discussion again if people don't
mind on how to do support restarts. My understanding of the general
feeling so far is

(1) Mandating support is not going to be acceptable

(2) Mandating format of resumption tokens is not going to be acceptable

(3) Mandating resumption tokens be long lifed (eg: can try again the
    following day) is not acceptable

(4) In fact, mandating that resumption tokens be unique (allowing
    a token to be reused twice in quick succession to get the same
    data) is not acceptable

So any proposal needs to be optionally supported.

Question time:

Does anyone else think that this is a major hole in OAI? I personally
do. After trying to crawl sites, things go wrong. The larger the site,
the greater the probability that something will go wrong. The larger
the site, the greater the pain of starting all over again. I do not
think it is practical for anyone to harvest ETDCat if is really got
4,000,000 records. Any fault, and start downloading that 4gb again!
So I feel strongly on this one. In fact, I think this is the most
major problem OAI has.

Do people think its better to reuse resumption tokens for this purpose,
or introduce a different sort of token? ETDCat for example I think
allocates a session id in resumption tokens, meaning they cannot
be reused when the session times out in the server (similar semantics
anyway). This is a reasonable implementation decision to make.
So maybe its better for servers to return an additional token,
which is a <restartToken> which means a client can instead of
specifying from= and to= again, specify restartToken= instead where
the server then automatically works out whatever other parameters
it needs, creates a new session etc internally. The new 'session'
(ListXXX verb) then can use resumptionTokens to manage that new
transfer.

The idea is for a <restartToken> to be long lifed. It may be less
efficient to use than a resumptionToken, but its only purpose is
if the client fails the download. If a server does not support
restartToken, it simply never returns one. Large collections *should*
support restartTokens.

For my harvester, I can then remember (to disk) the restartToken for
every packet I get back, allowing me to recover much more easily
if anything crashes. If restartToken's are too hard for someone
to implement, then they don't. If you have a large data collection
on the other hand, to reduce network load, I think its probably worth
the extra effort of supporting restartTokens.

Any comments? Better suggesions?

Alan
_______________________________________________
OAI-implementers mailing list
OAI-implementers@oaisrv.nsdl.cornell.edu
http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers


From wunder@inktomi.com  Wed Mar  6 19:33:53 2002
From: wunder@inktomi.com (Walter Underwood)
Date: Wed, 06 Mar 2002 11:33:53 -0800
Subject: [OAI-implementers] Better resumption mechanism - more importa nt than ever!
In-Reply-To: <3C852970.1020705@vt.edu>
References: <Pine.GSO.4.21.0203051344110.29139-100000@ruby.ils.unc.edu> <3C852970.1020705@vt.edu>
Message-ID: <8972885.1015414433@[0.0.0.0]>

--On Tuesday, March 5, 2002 3:24 PM -0500 Hussein Suleman <hussein@vt.edu> wrote:
>
> to state the stateless resumption problem a little differently:
>    is the OAI protocol idempotent ?
> if a request is submitted twice, will the responses be identical ?
> obviously not, since new records could have been added.

If a request has two time bounds, both in the past, then the protocol
should be idempotent. The protocol would need to outlaw back-dated
changes (put in this record today and date it yesterday).

With only a beginning bound, the protocol is not idempotent.
Requests with "until now" or "until next week" have other problems,
like clock skew. It is better to get the current server time, then
use that.

> if we update a record, its datestamp would cause it to move out of
> range of a from/until specification. is this the only case where
> the weaker condition fails ? if so, what if we ditch the until parameter ?

Requiring "until" might be a better path to statelessness. If a
record falls outside the bounds, that isn't a tragedy. It will be
picked up on the next harvest. There is always a window where
the read is before the write. The only question is how big.

As for EDTcat, this protocol needs a set of use cases. Then we can
separate kinds of problems: the protocol doesn't meet its intended
uses, or the intended uses missed an important case.

If anyone needs mood music for working on this, the Lene Lovich
album "Stateless" has been released on CD.

wunder
--
Walter R. Underwood
Senior Staff Engineer
Inktomi Enterprise Search
http://search.inktomi.com/

From ajk@mds.rmit.edu.au  Wed Mar  6 23:29:50 2002
From: ajk@mds.rmit.edu.au (Alan Kent)
Date: Thu, 7 Mar 2002 10:29:50 +1100
Subject: [OAI-implementers] Better resumption mechanism - more importa nt than ever!
In-Reply-To: <8972885.1015414433@[0.0.0.0]>; from Walter Underwood on Wed, Mar 06, 2002 at 11:33:53AM -0800
References: <Pine.GSO.4.21.0203051344110.29139-100000@ruby.ils.unc.edu> <3C852970.1020705@vt.edu> <8972885.1015414433@[0.0.0.0]>
Message-ID: <20020307102949.B26577@io.mds.rmit.edu.au>

On Wed, Mar 06, 2002 at 11:33:53AM -0800, Walter Underwood wrote:
> If a request has two time bounds, both in the past, then the protocol
> should be idempotent. The protocol would need to outlaw back-dated
> changes (put in this record today and date it yesterday).

Is this true? (If so it will change my *possible* implementation ;-)
I agree that new records being added should never be back dated,
but what if an old record is updated?

For example, if a record is inserted in May then updated in July and I
do a query from Jan to June (ie, including May but not July), then
should I get back the old May copy of the record? Or because the record
has been updated (in July), does that mean it is valid for me no longer
to return the May version of my record and only return the July
version (when appropriate)?

If I don't need to return the May version, the the protocol would not
be idempotent. This was my current understanding from old postings on
the list. If I must/should return the May version of the record still,
then I will need to rethink how I implement my harvester (which might
turn into an aggregator). Currently I keep the most recent record
version and throw away old versions (by doing updates on the old
record).

Doing a search service would also be different because I doubt people
want to do a search and find the old version of the record.

Alan

From ajk@mds.rmit.edu.au  Wed Mar  6 23:35:47 2002
From: ajk@mds.rmit.edu.au (Alan Kent)
Date: Thu, 7 Mar 2002 10:35:47 +1100
Subject: [OAI-implementers] Aggregators and querying by repository identifier?
Message-ID: <20020307103547.C26577@io.mds.rmit.edu.au>

My understanding of OAI currently is that you can query by from/until
and by set, but can you query by repository identifier?

With the advent of aggregators, I am wondering if people are going
to want to be able to harvest from an aggregator selectivly based
on repository identifiers. Before aggregators it was not an issue
(a repository contained records of that repository). With aggregators,
my understanding is records will retain the OAI-identifier (and
hence repository identifier?) of the original repository, so a
aggregator will hold records from lots of different repositories.

I was thinking that sets should not be used, as the aggregator should
probably try to preserve the sets of the original data (have not
looked into if this is possible yet).

Anyone have any thoughts on this? Is there a need, or is it an overkill?
Just thinking about how I might try to implement the DP part of OAI
(on leave next week, so don't expect anything soon!), and if I did
so, I would probably make it dish up the OAI data that I have harvested
from other sites (ie: and aggregator).

Alan

From hussein@vt.edu  Wed Mar  6 23:55:35 2002
From: hussein@vt.edu (Hussein Suleman)
Date: Wed, 06 Mar 2002 18:55:35 -0500
Subject: [OAI-implementers] Aggregators and querying by repository identifier?
References: <20020307103547.C26577@io.mds.rmit.edu.au>
Message-ID: <3C86AC77.6040707@vt.edu>

hi

i don't know how other people do this, but i prepend the repository id 
to all set names. so, for example, if i harvest the set "All" from VTETD 
and everything from CaltechETD, i end up with the set structure:
    VTETD/All
    CaltechETD

i interpret sets as being a method of structuring the collection, and 
with a plain vanilla union collection (as opposed to one that does 
merging of categorical sets) there is probably no better structure than 
the individual sub-collections

of course this is not a general solution, but it works for us :) and our 
"aggregator" has been running for about 9 months now. see the ODL 
website (http://oai.dlib.vt.edu/odl) for more details - specifically, 
read the tech report.

ttfn
----hussein

Alan Kent wrote:

> My understanding of OAI currently is that you can query by from/until
> and by set, but can you query by repository identifier?
> 
> With the advent of aggregators, I am wondering if people are going
> to want to be able to harvest from an aggregator selectivly based
> on repository identifiers. Before aggregators it was not an issue
> (a repository contained records of that repository). With aggregators,
> my understanding is records will retain the OAI-identifier (and
> hence repository identifier?) of the original repository, so a
> aggregator will hold records from lots of different repositories.
> 
> I was thinking that sets should not be used, as the aggregator should
> probably try to preserve the sets of the original data (have not
> looked into if this is possible yet).
> 
> Anyone have any thoughts on this? Is there a need, or is it an overkill?
> Just thinking about how I might try to implement the DP part of OAI
> (on leave next week, so don't expect anything soon!), and if I did
> so, I would probably make it dish up the OAI data that I have harvested
> from other sites (ie: and aggregator).
> 
> Alan
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 


-- 
======================================================================
hussein suleman - hussein@vt.edu - vtcs - http://www.husseinsspace.com
======================================================================


From liu_x@cs.odu.edu  Thu Mar  7 02:17:21 2002
From: liu_x@cs.odu.edu (Xiaoming Liu)
Date: Wed, 6 Mar 2002 21:17:21 -0500 (EST)
Subject: [OAI-implementers] Better resumption mechanism - more importa
 nt than ever!
In-Reply-To: <20020307102949.B26577@io.mds.rmit.edu.au>
Message-ID: <Pine.SOL.4.10.10203062049570.8699-100000@dilbert.cs.odu.edu>

On Thu, 7 Mar 2002, Alan Kent wrote:

> On Wed, Mar 06, 2002 at 11:33:53AM -0800, Walter Underwood wrote:
> > If a request has two time bounds, both in the past, then the protocol
> > should be idempotent. The protocol would need to outlaw back-dated
> > changes (put in this record today and date it yesterday).
> 
> Is this true? (If so it will change my *possible* implementation ;-)
> I agree that new records being added should never be back dated,
> but what if an old record is updated?

I don't think the protocol is idempotent. If a record is changed, it's
datestamp should be updated too, so a query to past (from->until) may be 
quite different. And the old record should not be kept. (Except in special
case like CVS, you will give it another OAI id, that's a different problem
however.)


> 
> For example, if a record is inserted in May then updated in July and I
> do a query from Jan to June (ie, including May but not July), then
> should I get back the old May copy of the record? Or because the record
> has been updated (in July), does that mean it is valid for me no longer
> to return the May version of my record and only return the July
> version (when appropriate)?
> 

The objective, as I understand, is to reach a consistent view for
different harvesters, based on frequent fresh harvesting. Let's assume
two harvesters, say H1 and H2. Like your case, we have a record X inserted
in May and updated in July. If H1 starts harvesting in April, it will get
an old version of your record in May, but if H1 is doing a regular fresh
harvesting, it will pickup the changes sometimes in  July; and H2 starts
harvesting in Aug, it will get a new version of X and never know what's
happended to X before. By this way H1 and H2 will have the same view of
the repository.

regards,
liu


 


> If I don't need to return the May version, the the protocol would not
> be idempotent. This was my current understanding from old postings on
> the list. If I must/should return the May version of the record still,
> then I will need to rethink how I implement my harvester (which might
> turn into an aggregator). Currently I keep the most recent record
> version and throw away old versions (by doing updates on the old
> record).
> 
> Doing a search service would also be different because I doubt people
> want to do a search and find the old version of the record.
> 
> Alan
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 


From ajk@mds.rmit.edu.au  Thu Mar  7 03:16:38 2002
From: ajk@mds.rmit.edu.au (Alan Kent)
Date: Thu, 7 Mar 2002 14:16:38 +1100
Subject: [OAI-implementers] XML records and RDF
Message-ID: <20020307141638.F26577@io.mds.rmit.edu.au>

I was looking at the feasibility of putting up a collection of
records using a scheme which is an extension of Dublin Core. However,
they have encoded the records using RDF rather than the simpler
OAI schema as used for Dublin Core records. (ie: lots of
<rdf:description> and <rdf:value> elements all over the place.)

The DC records included in appendex of the OAI 1.1 spec however
does not use RDF.

Just wondering if there was any reason for me not to return my
records using the RDF based schema? I could try and convert them
to the OAI DC schema as well (so available in both formats).
Just wondering if there was any wisdom here. (Eg: "we chose not
to use RDF based DC XML because...").

I am not an RDF expert by the way, so don't understand all the
subtlties that may be involved.

Alan
-- 
Alan Kent (mailto:ajk@mds.rmit.edu.au, http://www.mds.rmit.edu.au)
Postal: Multimedia Database Systems, RMIT, GPO Box 2476V, Melbourne 3001.
Where: RMIT MDS, Bld 91, Level 3, 110 Victoria St, Carlton 3053, VIC Australia.
Phone: +61 3 9925 4114  Reception: +61 3 9925 4099  Fax: +61 3 9925 4098 

From paxinos.mathew.m@edumail.vic.gov.au  Thu Mar  7 04:59:25 2002
From: paxinos.mathew.m@edumail.vic.gov.au (Paxinos, Mathew M)
Date: Thu, 7 Mar 2002 15:59:25 +1100
Subject: [OAI-implementers] XML records and RDF
Message-ID: <7542BA003187DC43B21A8021F6B17BF302067366@edu002ms003.education.vic.gov.au>

This message is in MIME format. Since your mail reader does not understand
this format, some or all of this message may not be legible.

------_=_NextPart_001_01C1C594.DA417880
Content-Type: text/plain

Alan,

My interpretation of the schema was that you can provide the records in whatever
format you wish, it could conform to the DC schema as outlined in the OAI 1.1
spec, or you could specify your own.  The OAI 1.1 only specified that you must
provide an instance of your data using the DC schema.

Our system provides records in DC as per the schema for general consumption or
using our own VEMS standard, which provides the same information but with more
descriptive detail as to what things are.  The ListMetadataFormats verb provides
a list of the schemas that the repository can provide the data in.

Regards,

Mathew Paxinos
System Developer
Victorian Education Channel
Department of Education & Training
Phone: (03) 9637 2256
www.education.vic.gov.au
 

-----Original Message-----
From: Alan Kent [mailto:ajk@mds.rmit.edu.au] 
Sent: Thursday, 7 March 2002 14:17
To: OAI Implementors
Subject: [OAI-implementers] XML records and RDF

I was looking at the feasibility of putting up a collection of
records using a scheme which is an extension of Dublin Core. However,
they have encoded the records using RDF rather than the simpler
OAI schema as used for Dublin Core records. (ie: lots of
<rdf:description> and <rdf:value> elements all over the place.)

The DC records included in appendex of the OAI 1.1 spec however
does not use RDF.

Just wondering if there was any reason for me not to return my
records using the RDF based schema? I could try and convert them
to the OAI DC schema as well (so available in both formats).
Just wondering if there was any wisdom here. (Eg: "we chose not
to use RDF based DC XML because...").

I am not an RDF expert by the way, so don't understand all the
subtlties that may be involved.

Alan
-- 
Alan Kent (mailto:ajk@mds.rmit.edu.au, http://www.mds.rmit.edu.au)
Postal: Multimedia Database Systems, RMIT, GPO Box 2476V, Melbourne 3001.
Where: RMIT MDS, Bld 91, Level 3, 110 Victoria St, Carlton 3053, VIC Australia.
Phone: +61 3 9925 4114  Reception: +61 3 9925 4099  Fax: +61 3 9925 4098 
_______________________________________________
OAI-implementers mailing list
OAI-implementers@oaisrv.nsdl.cornell.edu
http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers



*******************************************************************************
Important - This email and any attachments may be confidential. If received in
error, please contact us and delete all copies. Before opening or using
attachments check them for viruses and defects. Regardless of any loss, damage
or consequence, whether caused by the negligence of the sender or not, resulting
directly or indirectly from the use of any attached files our liability is
limited to resupplying any affected attachments. Any representations or opinions
expressed are those of the individual sender, and not necessarily those of the
Department of Education Employment and Training.

------_=_NextPart_001_01C1C594.DA417880
Content-Type: text/html
Content-Transfer-Encoding: quoted-printable

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
<HEAD>
<META HTTP-EQUIV=3D"Content-Type" CONTENT=3D"text/html; =
charset=3Dus-ascii">
<META NAME=3D"Generator" CONTENT=3D"MS Exchange Server version =
5.5.2653.12">
<TITLE>RE: [OAI-implementers] XML records and RDF</TITLE>
</HEAD>
<BODY>

<P><FONT SIZE=3D2>Alan,</FONT>
</P>

<P><FONT SIZE=3D2>My interpretation of the schema was that you can =
provide the records in whatever format you wish, it could conform to =
the DC schema as outlined in the OAI 1.1 spec, or you could specify =
your own.&nbsp; The OAI 1.1 only specified that you must provide an =
instance of your data using the DC schema.</FONT></P>

<P><FONT SIZE=3D2>Our system provides records in DC as per the schema =
for general consumption or using our own VEMS standard, which provides =
the same information but with more descriptive detail as to what things =
are.&nbsp; The ListMetadataFormats verb provides a list of the schemas =
that the repository can provide the data in.</FONT></P>

<P><FONT SIZE=3D2>Regards,</FONT>
</P>

<P><FONT SIZE=3D2>Mathew Paxinos</FONT>
<BR><FONT SIZE=3D2>System Developer</FONT>
<BR><FONT SIZE=3D2>Victorian Education Channel</FONT>
<BR><FONT SIZE=3D2>Department of Education &amp; Training</FONT>
<BR><FONT SIZE=3D2>Phone: (03) 9637 2256</FONT>
<BR><FONT SIZE=3D2>www.education.vic.gov.au</FONT>
<BR><FONT SIZE=3D2>&nbsp;</FONT>
</P>

<P><FONT SIZE=3D2>-----Original Message-----</FONT>
<BR><FONT SIZE=3D2>From: Alan Kent [<A =
HREF=3D"mailto:ajk@mds.rmit.edu.au">mailto:ajk@mds.rmit.edu.au</A>] =
</FONT>
<BR><FONT SIZE=3D2>Sent: Thursday, 7 March 2002 14:17</FONT>
<BR><FONT SIZE=3D2>To: OAI Implementors</FONT>
<BR><FONT SIZE=3D2>Subject: [OAI-implementers] XML records and =
RDF</FONT>
</P>

<P><FONT SIZE=3D2>I was looking at the feasibility of putting up a =
collection of</FONT>
<BR><FONT SIZE=3D2>records using a scheme which is an extension of =
Dublin Core. However,</FONT>
<BR><FONT SIZE=3D2>they have encoded the records using RDF rather than =
the simpler</FONT>
<BR><FONT SIZE=3D2>OAI schema as used for Dublin Core records. (ie: =
lots of</FONT>
<BR><FONT SIZE=3D2>&lt;rdf:description&gt; and &lt;rdf:value&gt; =
elements all over the place.)</FONT>
</P>

<P><FONT SIZE=3D2>The DC records included in appendex of the OAI 1.1 =
spec however</FONT>
<BR><FONT SIZE=3D2>does not use RDF.</FONT>
</P>

<P><FONT SIZE=3D2>Just wondering if there was any reason for me not to =
return my</FONT>
<BR><FONT SIZE=3D2>records using the RDF based schema? I could try and =
convert them</FONT>
<BR><FONT SIZE=3D2>to the OAI DC schema as well (so available in both =
formats).</FONT>
<BR><FONT SIZE=3D2>Just wondering if there was any wisdom here. (Eg: =
&quot;we chose not</FONT>
<BR><FONT SIZE=3D2>to use RDF based DC XML because...&quot;).</FONT>
</P>

<P><FONT SIZE=3D2>I am not an RDF expert by the way, so don't =
understand all the</FONT>
<BR><FONT SIZE=3D2>subtlties that may be involved.</FONT>
</P>

<P><FONT SIZE=3D2>Alan</FONT>
<BR><FONT SIZE=3D2>-- </FONT>
<BR><FONT SIZE=3D2>Alan Kent (<A =
HREF=3D"mailto:ajk@mds.rmit.edu.au">mailto:ajk@mds.rmit.edu.au</A>, <A =
HREF=3D"http://www.mds.rmit.edu.au" =
TARGET=3D"_blank">http://www.mds.rmit.edu.au</A>)</FONT>
<BR><FONT SIZE=3D2>Postal: Multimedia Database Systems, RMIT, GPO Box =
2476V, Melbourne 3001.</FONT>
<BR><FONT SIZE=3D2>Where: RMIT MDS, Bld 91, Level 3, 110 Victoria St, =
Carlton 3053, VIC Australia.</FONT>
<BR><FONT SIZE=3D2>Phone: +61 3 9925 4114&nbsp; Reception: +61 3 9925 =
4099&nbsp; Fax: +61 3 9925 4098 </FONT>
<BR><FONT =
SIZE=3D2>_______________________________________________</FONT>
<BR><FONT SIZE=3D2>OAI-implementers mailing list</FONT>
<BR><FONT SIZE=3D2>OAI-implementers@oaisrv.nsdl.cornell.edu</FONT>
<BR><FONT SIZE=3D2><A =
HREF=3D"http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers=
" =
TARGET=3D"_blank">http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-im=
plementers</A></FONT>
</P>
<BR>
<BR>

<P><FONT =
SIZE=3D2>***************************************************************=
****************</FONT>
<BR><B><FONT SIZE=3D2>Important</FONT></B><FONT SIZE=3D2> - This email =
and any attachments may be confidential. If received in error, please =
contact us and delete all copies. Before opening or using attachments =
check them for viruses and defects. Regardless of any loss, damage or =
consequence, whether caused by the negligence of the sender or not, =
resulting directly or indirectly from the use of any attached files our =
liability is limited to resupplying any affected attachments. Any =
representations or opinions expressed are those of the individual =
sender, and not necessarily those of the Department of Education =
Employment and Training.</FONT></P>

</BODY>
</HTML>
------_=_NextPart_001_01C1C594.DA417880--

From lagoze@cs.cornell.edu  Thu Mar  7 12:38:21 2002
From: lagoze@cs.cornell.edu (Carl Lagoze)
Date: Thu, 7 Mar 2002 07:38:21 -0500
Subject: [OAI-implementers] XML records and RDF
Message-ID: <706871B20764CD449DB0E8E3D81C4D4302A20436@opus.cs.cornell.edu>

This is a multi-part message in MIME format.

------_=_NextPart_001_01C1C5D4.F6F7AB08
Content-Type: text/plain;
	charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

Alan,=20
=20
Mathew's response is pretty complete.  To restate in slightly different =
language: the OAI-PMH mandates a very simple and basic DC syntax for the =
purpose of a lingua franca amongst all data providers.  The protocol =
allows (and encourages, if a protocol can do such!) data providers to =
provide parallel forms of metadata that are more expressive than the =
required dc format.
=20
Carl
=20
Department of Computer Science
Cornell University
Ithaca, NY 14853 USA
Voice: +1-607-255-6046
FAX: +1-607-255-4428
EMail: lagoze@cs.cornell.edu
WWW: http://www.cs.cornell.edu/lagoze=20

-----Original Message-----
From: Paxinos, Mathew M [mailto:paxinos.mathew.m@edumail.vic.gov.au]
Sent: Wednesday, March 06, 2002 11:59 PM
To: 'Alan Kent'; OAI Implementors
Subject: RE: [OAI-implementers] XML records and RDF



Alan,=20

My interpretation of the schema was that you can provide the records in =
whatever format you wish, it could conform to the DC schema as outlined =
in the OAI 1.1 spec, or you could specify your own.  The OAI 1.1 only =
specified that you must provide an instance of your data using the DC =
schema.

Our system provides records in DC as per the schema for general =
consumption or using our own VEMS standard, which provides the same =
information but with more descriptive detail as to what things are.  The =
ListMetadataFormats verb provides a list of the schemas that the =
repository can provide the data in.

Regards,=20

Mathew Paxinos=20
System Developer=20
Victorian Education Channel=20
Department of Education & Training=20
Phone: (03) 9637 2256=20
www.education.vic.gov.au=20
 =20

-----Original Message-----=20
From: Alan Kent [ mailto:ajk@mds.rmit.edu.au]=20
Sent: Thursday, 7 March 2002 14:17=20
To: OAI Implementors=20
Subject: [OAI-implementers] XML records and RDF=20

I was looking at the feasibility of putting up a collection of=20
records using a scheme which is an extension of Dublin Core. However,=20
they have encoded the records using RDF rather than the simpler=20
OAI schema as used for Dublin Core records. (ie: lots of=20
<rdf:description> and <rdf:value> elements all over the place.)=20

The DC records included in appendex of the OAI 1.1 spec however=20
does not use RDF.=20

Just wondering if there was any reason for me not to return my=20
records using the RDF based schema? I could try and convert them=20
to the OAI DC schema as well (so available in both formats).=20
Just wondering if there was any wisdom here. (Eg: "we chose not=20
to use RDF based DC XML because...").=20

I am not an RDF expert by the way, so don't understand all the=20
subtlties that may be involved.=20

Alan=20
--=20
Alan Kent ( mailto:ajk@mds.rmit.edu.au, http://www.mds.rmit.edu.au)=20
Postal: Multimedia Database Systems, RMIT, GPO Box 2476V, Melbourne =
3001.=20
Where: RMIT MDS, Bld 91, Level 3, 110 Victoria St, Carlton 3053, VIC =
Australia.=20
Phone: +61 3 9925 4114  Reception: +61 3 9925 4099  Fax: +61 3 9925 4098 =

_______________________________________________=20
OAI-implementers mailing list=20
OAI-implementers@oaisrv.nsdl.cornell.edu=20
http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers=20



*************************************************************************=
******=20
Important - This email and any attachments may be confidential. If =
received in error, please contact us and delete all copies. Before =
opening or using attachments check them for viruses and defects. =
Regardless of any loss, damage or consequence, whether caused by the =
negligence of the sender or not, resulting directly or indirectly from =
the use of any attached files our liability is limited to resupplying =
any affected attachments. Any representations or opinions expressed are =
those of the individual sender, and not necessarily those of the =
Department of Education Employment and Training.


------_=_NextPart_001_01C1C5D4.F6F7AB08
Content-Type: text/html;
	charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML><HEAD>
<META HTTP-EQUIV=3D"Content-Type" CONTENT=3D"text/html; =
charset=3Diso-8859-1">
<TITLE>RE: [OAI-implementers] XML records and RDF</TITLE>

<META content=3D"MSHTML 6.00.2713.1100" name=3DGENERATOR></HEAD>
<BODY>
<DIV><SPAN class=3D306333512-07032002><FONT color=3D#0000ff =
size=3D2>Alan,=20
</FONT></SPAN></DIV>
<DIV><SPAN class=3D306333512-07032002><FONT color=3D#0000ff=20
size=3D2></FONT></SPAN>&nbsp;</DIV>
<DIV><SPAN class=3D306333512-07032002><FONT color=3D#0000ff =
size=3D2>Mathew's response=20
is pretty complete.&nbsp; To restate in slightly different language: the =
OAI-PMH=20
mandates a very simple and basic DC syntax for the purpose of a lingua =
franca=20
amongst all data providers.&nbsp; The protocol allows (and encourages, =
if a=20
protocol can do such!) data providers to provide parallel forms of =
metadata that=20
are more expressive than the required dc format.</FONT></SPAN></DIV>
<DIV><SPAN class=3D306333512-07032002><FONT color=3D#0000ff=20
size=3D2></FONT></SPAN>&nbsp;</DIV>
<DIV><SPAN class=3D306333512-07032002><FONT color=3D#0000ff=20
size=3D2>Carl</FONT></SPAN></DIV>
<DIV><SPAN class=3D306333512-07032002><FONT color=3D#0000ff=20
size=3D2></FONT></SPAN>&nbsp;</DIV>
<DIV><SPAN class=3D306333512-07032002>
<P><FONT size=3D2>Department of Computer Science<BR>Cornell =
University<BR>Ithaca,=20
NY 14853 USA<BR>Voice: +1-607-255-6046<BR>FAX: +1-607-255-4428<BR>EMail: =

lagoze@cs.cornell.edu<BR>WWW: <A =
href=3D"http://www.cs.cornell.edu/lagoze"=20
target=3D_blank>http://www.cs.cornell.edu/lagoze</A> =
</FONT></P></SPAN></DIV>
<BLOCKQUOTE dir=3Dltr=20
style=3D"PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORDER-LEFT: #0000ff 2px =
solid; MARGIN-RIGHT: 0px">
  <DIV class=3DOutlookMessageHeader dir=3Dltr align=3Dleft><FONT =
face=3DTahoma=20
  size=3D2>-----Original Message-----<BR><B>From:</B> Paxinos, Mathew M=20
  [mailto:paxinos.mathew.m@edumail.vic.gov.au]<BR><B>Sent:</B> =
Wednesday, March=20
  06, 2002 11:59 PM<BR><B>To:</B> 'Alan Kent'; OAI=20
  Implementors<BR><B>Subject:</B> RE: [OAI-implementers] XML records and =

  RDF<BR><BR></FONT></DIV>
  <P><FONT size=3D2>Alan,</FONT> </P>
  <P><FONT size=3D2>My interpretation of the schema was that you can =
provide the=20
  records in whatever format you wish, it could conform to the DC schema =
as=20
  outlined in the OAI 1.1 spec, or you could specify your own.&nbsp; The =
OAI 1.1=20
  only specified that you must provide an instance of your data using =
the DC=20
  schema.</FONT></P>
  <P><FONT size=3D2>Our system provides records in DC as per the schema =
for=20
  general consumption or using our own VEMS standard, which provides the =
same=20
  information but with more descriptive detail as to what things =
are.&nbsp; The=20
  ListMetadataFormats verb provides a list of the schemas that the =
repository=20
  can provide the data in.</FONT></P>
  <P><FONT size=3D2>Regards,</FONT> </P>
  <P><FONT size=3D2>Mathew Paxinos</FONT> <BR><FONT size=3D2>System =
Developer</FONT>=20
  <BR><FONT size=3D2>Victorian Education Channel</FONT> <BR><FONT=20
  size=3D2>Department of Education &amp; Training</FONT> <BR><FONT =
size=3D2>Phone:=20
  (03) 9637 2256</FONT> <BR><FONT =
size=3D2>www.education.vic.gov.au</FONT>=20
  <BR><FONT size=3D2>&nbsp;</FONT> </P>
  <P><FONT size=3D2>-----Original Message-----</FONT> <BR><FONT =
size=3D2>From: Alan=20
  Kent [<A =
href=3D"mailto:ajk@mds.rmit.edu.au">mailto:ajk@mds.rmit.edu.au</A>]=20
  </FONT><BR><FONT size=3D2>Sent: Thursday, 7 March 2002 14:17</FONT> =
<BR><FONT=20
  size=3D2>To: OAI Implementors</FONT> <BR><FONT size=3D2>Subject:=20
  [OAI-implementers] XML records and RDF</FONT> </P>
  <P><FONT size=3D2>I was looking at the feasibility of putting up a =
collection=20
  of</FONT> <BR><FONT size=3D2>records using a scheme which is an =
extension of=20
  Dublin Core. However,</FONT> <BR><FONT size=3D2>they have encoded the =
records=20
  using RDF rather than the simpler</FONT> <BR><FONT size=3D2>OAI schema =
as used=20
  for Dublin Core records. (ie: lots of</FONT> <BR><FONT=20
  size=3D2>&lt;rdf:description&gt; and &lt;rdf:value&gt; elements all =
over the=20
  place.)</FONT> </P>
  <P><FONT size=3D2>The DC records included in appendex of the OAI 1.1 =
spec=20
  however</FONT> <BR><FONT size=3D2>does not use RDF.</FONT> </P>
  <P><FONT size=3D2>Just wondering if there was any reason for me not to =
return=20
  my</FONT> <BR><FONT size=3D2>records using the RDF based schema? I =
could try and=20
  convert them</FONT> <BR><FONT size=3D2>to the OAI DC schema as well =
(so=20
  available in both formats).</FONT> <BR><FONT size=3D2>Just wondering =
if there=20
  was any wisdom here. (Eg: "we chose not</FONT> <BR><FONT size=3D2>to =
use RDF=20
  based DC XML because...").</FONT> </P>
  <P><FONT size=3D2>I am not an RDF expert by the way, so don't =
understand all=20
  the</FONT> <BR><FONT size=3D2>subtlties that may be involved.</FONT> =
</P>
  <P><FONT size=3D2>Alan</FONT> <BR><FONT size=3D2>-- </FONT><BR><FONT =
size=3D2>Alan=20
  Kent (<A =
href=3D"mailto:ajk@mds.rmit.edu.au">mailto:ajk@mds.rmit.edu.au</A>, <A=20
  href=3D"http://www.mds.rmit.edu.au"=20
  target=3D_blank>http://www.mds.rmit.edu.au</A>)</FONT> <BR><FONT =
size=3D2>Postal:=20
  Multimedia Database Systems, RMIT, GPO Box 2476V, Melbourne =
3001.</FONT>=20
  <BR><FONT size=3D2>Where: RMIT MDS, Bld 91, Level 3, 110 Victoria St, =
Carlton=20
  3053, VIC Australia.</FONT> <BR><FONT size=3D2>Phone: +61 3 9925 =
4114&nbsp;=20
  Reception: +61 3 9925 4099&nbsp; Fax: +61 3 9925 4098 </FONT><BR><FONT =

  size=3D2>_______________________________________________</FONT> =
<BR><FONT=20
  size=3D2>OAI-implementers mailing list</FONT> <BR><FONT=20
  size=3D2>OAI-implementers@oaisrv.nsdl.cornell.edu</FONT> <BR><FONT =
size=3D2><A=20
  =
href=3D"http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers"=
=20
  =
target=3D_blank>http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-imple=
menters</A></FONT>=20
  </P><BR><BR>
  <P><FONT=20
  =
size=3D2>****************************************************************=
***************</FONT>=20
  <BR><B><FONT size=3D2>Important</FONT></B><FONT size=3D2> - This email =
and any=20
  attachments may be confidential. If received in error, please contact =
us and=20
  delete all copies. Before opening or using attachments check them for =
viruses=20
  and defects. Regardless of any loss, damage or consequence, whether =
caused by=20
  the negligence of the sender or not, resulting directly or indirectly =
from the=20
  use of any attached files our liability is limited to resupplying any =
affected=20
  attachments. Any representations or opinions expressed are those of =
the=20
  individual sender, and not necessarily those of the Department of =
Education=20
  Employment and Training.</FONT></P></BLOCKQUOTE></BODY></HTML>

------_=_NextPart_001_01C1C5D4.F6F7AB08--

From leop@engr.arizona.edu  Thu Mar  7 14:58:00 2002
From: leop@engr.arizona.edu (Leo Przybylski)
Date: Thu, 7 Mar 2002 07:58:00 -0700
Subject: [OAI-implementers] XML records and RDF
In-Reply-To: <20020307141638.F26577@io.mds.rmit.edu.au>
References: <20020307141638.F26577@io.mds.rmit.edu.au>
Message-ID: <200203071503.g27F3LD310721@intermix.engr.arizona.edu>

Hello,

I'm not an RDF expert either, but I do consider myself to have well 
researched DC and RDF together. I have also been looking for a way to extend 
DC.

RDF is very extensible. Their Schema can easily be added to using RDF. 
Unfortunately, RDF isn't just XML. It's almost a parallel data model to XML. 
The RDF community has developed their own processor for it. It has its own 
Schema model.

The subject of RDF and DC gets into "web semantics". You may have already 
read this, but I'll send it anyway: 
http://talad.sis.pitt.edu/marut/soa/SemanticInteroperability.pdf

Personally, I am leaning (though still undecided) not to go with RDF. My 
concern is not with semantics, I don't want to run the risk that someone else 
is not using RDF, and I don't want to chase down an API for using an RDF 
processor. 

You may have noticed that the DC RDF Schema isn't all RDF. That's right. They 
have a plain old XML schema. It looks like they're encapsulating their XML 
inside an RDF description. (Correct me if I'm wrong on this. Again, I'm not 
an expert). If you really want to extend DC, I've found you can easily do it 
right through the XML Schema into anything you want (doesn't have to be RDF 
at all).

Hope this helps,
-Leo

On Wednesday 06 March 2002 20:16, Alan Kent wrote:
> I was looking at the feasibility of putting up a collection of
> records using a scheme which is an extension of Dublin Core. However,
> they have encoded the records using RDF rather than the simpler
> OAI schema as used for Dublin Core records. (ie: lots of
> <rdf:description> and <rdf:value> elements all over the place.)
>
> The DC records included in appendex of the OAI 1.1 spec however
> does not use RDF.
>
> Just wondering if there was any reason for me not to return my
> records using the RDF based schema? I could try and convert them
> to the OAI DC schema as well (so available in both formats).
> Just wondering if there was any wisdom here. (Eg: "we chose not
> to use RDF based DC XML because...").
>
> I am not an RDF expert by the way, so don't understand all the
> subtlties that may be involved.
>
> Alan

From ajk@mds.rmit.edu.au  Fri Mar  8 09:38:54 2002
From: ajk@mds.rmit.edu.au (Alan Kent)
Date: Fri, 8 Mar 2002 20:38:54 +1100
Subject: [OAI-implementers] OAI harvested database up for searching
Message-ID: <20020308203854.M11510@io.mds.rmit.edu.au>

I have created a database of all the OAI data I have harvested on
the machine outside the firewall here. Its still building as I type
but I will probably go home before it finishes.

I have put it up using a default search interface we have. Its not
the prettiest, or most user friendly, but it can exercise things
pretty well (and it was quick for me to do).

If you want to play, try http://z3950.simdb.com:8080/
then click on the "Servers" link in the top menu bar. It has
some instructions (normally this page displays other Z39.50
servers you can connect to, but for security reasons I have
disabled this). Log on as 'anonymous' (no password) and then
select the OAI-Data database (feel free to play with the other
databases if you want to, but they are pretty boring).

You can enter various searches (single terms, phrases), drop
down lists of searchable fields, scan indexes, etc. The indexes
defined may be silly (please suggest improvements!) For example
what do people normally want to search on?

The database it is in is also available for direct Z39.50 access.
Try z3950.simdb.com:210. It supports the GRS-1 record syntax by
default, but should be able to return records in SUTRS and XML
record syntaxes too. But I have not tested in much.

By the way, I am on leave much of next week, so if you hit problems
I might not respond immediately. But I think its always best to
release a first version of something then immediately go on
holidays so you don't have to worry about supporting it for
a while! :-) :-) :-)

Anyway, enjoy (if it works!)

Alan

From khage@umich.edu  Fri Mar  8 17:36:07 2002
From: khage@umich.edu (Kat Hagedorn)
Date: Fri, 8 Mar 2002 12:36:07 -0500
Subject: [OAI-implementers] restricted and free records
Message-ID: <F8FF00DE-32BA-11D6-A2F5-0003934CA344@umich.edu>

Hi all,

You're probably familiar with our OAIster project (I sent around a 
message last month). If not, check us out at 
http://oaister.umdl.umich.edu/.

Besides developing a search service with OAI harvested records, we're 
also a data provider. We've been discussing in-house how to best make 
our OAI records available for harvest by others.

In particular, we're wrestling with the issue of restricted records vs. 
free records. We have a number of sets that contain free records, but 
other sets are restricted to UM or institutions that carry licenses for 
materials we have. We're OAI-enabling both types of records, since 
restricted materials are valuable for certain audiences (e.g., our 
partners).

To make it clear what is restricted and what is free, we developed the 
concept of groups. One group is "dlpsall" which contains the sets that 
are both restricted and free. Another group is "dlpsfree" which contains 
only those sets that are free. We're serving up groups as sets. The 
repository explorer at http://oai.dlib.vt.edu/cgi-
bin/Explorer/oai1.1/testoai gives a more complete view of what we're 
doing.

My question is: wouldn't it be nice if we could indicate whether a 
record is restricted or free not at the set level but within the record? 
A harvester could then easily choose one type over another without 
resorting to parsing sets info (which would probably take human 
intervention). Is this something that could be made available in v2 of 
the protocol?

Regards,
Kat

-------------------
Kat Hagedorn
OAIster Librarian
Digital Library Production Service
University of Michigan
khage@umich.edu
734-647-8000 (not a direct line)


From hussein@vt.edu  Tue Mar 12 16:53:07 2002
From: hussein@vt.edu (Hussein Suleman)
Date: Tue, 12 Mar 2002 11:53:07 -0500
Subject: [OAI-implementers] restricted and free records
References: <F8FF00DE-32BA-11D6-A2F5-0003934CA344@umich.edu>
Message-ID: <3C8E3273.4070401@vt.edu>

hi

some thoughts ...

the mechanisms for doing what you propose are to some degree already in 
place. some additional work needs to be done within specific communities 
however ... to elaborate,

firstly, i am not sure if you mean "restricted resources" or "restricted 
records". if the former then intellectual property and usage information 
could go into the Dublin Core 'metadata/dc/rights' field. if the latter, 
then the same information pertains to the record rather than to the 
resource and the information should ideally go into the 
'about/dc/rights' field.

now, some things to consider:

- what if the the metadata is not dc only ? other metadata formats have 
places in which to add such information - IMS, for example, has a 
'rights' category

- the problem with most metadata formats is that there is no 
machine-readable vocabulary. that is a separate problem and can be 
resolved independently of making the relevant containers available. 
there are a couple of projects out there - try http://odrl.net/. i have 
seen a simpler scheme but cannot remember who is working on that - maybe 
someone on this mailing list can help ...

- if you really want to protect your data, what is there to stop a 
service provider from ignoring the rights information ? especially if 
the sp does not understand the rights language you are using ... maybe 
an access control list in the data provider would be better ?

- what if you are using the about container for something else already ? 
in v2, "about" will be repeatable!

- in most metadata formats, rights are human-readable strings ... on the 
other hand, most rights management schemes are structured - hence, for 
example, i do not believe you can encode the ODRL scheme in DC ... so 
either you do not use DC, you use extensions, you serialize (aka fudge) 
the structured rights data, or we completely revisit the notion of simple dc

the bottom line is unfortunately that until there is a requirement for 
service providers to respect rights statements (and ignore records if 
they do not understand the statements), nobody is going to do it

a quick question to anyone who harvests data: do you read the "rights" 
field of every record you harvest ? e.g., does anyone read the stuff we 
put into the VTETD dc.rights fields ?

ttfn
----hussein

Kat Hagedorn wrote:

> Hi all,
> 
> You're probably familiar with our OAIster project (I sent around a 
> message last month). If not, check us out at 
> http://oaister.umdl.umich.edu/.
> 
> Besides developing a search service with OAI harvested records, we're 
> also a data provider. We've been discussing in-house how to best make 
> our OAI records available for harvest by others.
> 
> In particular, we're wrestling with the issue of restricted records vs. 
> free records. We have a number of sets that contain free records, but 
> other sets are restricted to UM or institutions that carry licenses for 
> materials we have. We're OAI-enabling both types of records, since 
> restricted materials are valuable for certain audiences (e.g., our 
> partners).
> 
> To make it clear what is restricted and what is free, we developed the 
> concept of groups. One group is "dlpsall" which contains the sets that 
> are both restricted and free. Another group is "dlpsfree" which contains 
> only those sets that are free. We're serving up groups as sets. The 
> repository explorer at http://oai.dlib.vt.edu/cgi-
> bin/Explorer/oai1.1/testoai gives a more complete view of what we're doing.
> 
> My question is: wouldn't it be nice if we could indicate whether a 
> record is restricted or free not at the set level but within the record? 
> A harvester could then easily choose one type over another without 
> resorting to parsing sets info (which would probably take human 
> intervention). Is this something that could be made available in v2 of 
> the protocol?
> 
> Regards,
> Kat
> 
> -------------------
> Kat Hagedorn
> OAIster Librarian
> Digital Library Production Service
> University of Michigan
> khage@umich.edu
> 734-647-8000 (not a direct line)
> 
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers


-- 
======================================================================
hussein suleman - hussein@vt.edu - vtcs - http://www.husseinsspace.com
======================================================================


From francois@fsconsult.com  Tue Mar 19 04:03:01 2002
From: francois@fsconsult.com (Francois Schiettecatte)
Date: Mon, 18 Mar 2002 23:03:01 -0500
Subject: [OAI-implementers] <no subject>
Message-ID: <B8BC22A5.3C81%francois@fsconsult.com>

Hi

I am putting together a search engine for a selected number of OAI metadata
databases. Below is the news item I posted to the OAI web site:

my.OAI is a full-featured search engine to a selected list of metadata
databases from the Open Archives Initiative project. my.OAI can be
tailored by the user to suit individual interests and provides the
following features: Forms based query formulation; Automatic display of
summaries when viewing search results; Automatic display of similar
document when viewing a document; Automatic mark-up of retrieved
document with search links; Search history; Saving searches for later
re-use (with an SDI option); Saving document in folders. The user
interfaces is not yet complete, user account creation still needs to be
added in, but we are making it available now so that we can get feedback
on the functionality from users.

The URL to the search engine is:

    http://www.myoai.com/

I am releasing this before I have fully finished the functionality to get
feedback/comments/suggestions from people. Hopefully I will have the account
creation done in a couple of week's time.

Cheers

Francois

========================================================================
Franois Schiettecatte                               FS Consulting, Inc.
Phone : (410) 732-4839                          1000 Fell Street, # 618,
                                                    Baltimore, MD, 21231
Email : francois@fsconsult.com           URL : http://www.fsconsult.com/
========================================================================



From ajk@mds.rmit.edu.au  Thu Mar 21 03:02:21 2002
From: ajk@mds.rmit.edu.au (Alan Kent)
Date: Thu, 21 Mar 2002 14:02:21 +1100
Subject: [OAI-implementers] Dublin Core XML and OAI
Message-ID: <20020321140221.E5046@io.mds.rmit.edu.au>

Hi,

I was wondering what the exact position was with respect to OAI and
Dublin Core. For example, the OAI spec defines oai_dc and an XML Schema
specification for it. My question relates to other (non-Dublin Core)
elements that may be added.

For example, various groups have defined their own additional elements
and what they mean. They put these additional elements in their own
namespace. I am guessing it is not correct for me to include these
additional elements if oai_dc is requested (I should prune them out).
Each group should then define their own oai_agls etc identifier
(probably without the oai_ prefix, if that is reserved for use by
the protocol).

Alan

From ldodds@ingenta.com  Thu Mar 21 13:01:38 2002
From: ldodds@ingenta.com (Leigh Dodds)
Date: Thu, 21 Mar 2002 13:01:38 -0000
Subject: [OAI-implementers] Dublin Core XML and OAI
In-Reply-To: <20020321140221.E5046@io.mds.rmit.edu.au>
Message-ID: <NCBBKFMJCLIMOBIGKFMJGEBMHGAA.ldodds@ingenta.com>

Hi,

I've got some related questions/thoughts on this topic...

> I was wondering what the exact position was with respect to OAI and
> Dublin Core. For example, the OAI spec defines oai_dc and an XML Schema
> specification for it. My question relates to other (non-Dublin Core)
> elements that may be added.
> 
> For example, various groups have defined their own additional elements
> and what they mean. They put these additional elements in their own
> namespace. I am guessing it is not correct for me to include these
> additional elements if oai_dc is requested (I should prune them out).

I guess this depends on how authoritative the XML Schema for the DC 
metadata is deemed to be?

The main schema defines the metadata block for a record as being 
optional (minoccurs="0") so you can't guarantee that metadata 
will be provided with a GetRecord response. When it is included, the 
oai:metadata element can contain *any* number of of elements, 
from any namespace. So one might assume that any amount of metadata 
is permissible within this element.

The DC schema allows for any amount of metadata from a specific 
list of elements (subject, creator, etc), although including none of them is 
also valid. So from this one might assume that it is only legal to 
include elements within the oai:metadata/dc:dc elements that are explicitly 
given in this schema.

Although as the above shows, other elements e.g. oai:metadata/foo:foo 
would be legal -- this is probably not the intent though.

> Each group should then define their own oai_agls etc identifier

I personally think this is the better option. If the oai_dc metadata 
response is guaranteed to contain a well-defined metadata format, 
then the goal of interoperability seems to be met.

The intent of the spec seems to be that additional, custom metadata formats 
should be only delivered in response to requests for an explicit 
metadataPrefix.

> (probably without the oai_ prefix, if that is reserved for use by
> the protocol).

I think the spec only restricts oai_dc rather than oai_*

On a related note, I'm curious whether service providers are routinely 
validating the records they harvest from repositories?

Also are there any standard subject classifications being used, or 
are data providers creating their own?

Cheers,

L.

-- 
Leigh Dodds, Research Group, Ingenta | "Pluralitas non est ponenda
http://weblogs.userland.com/eclectic |    sine necessitate"
http://www.xml.com/pub/xmldeviant    |     -- William of Ockham

From mmcclelland@eduprise.com  Thu Mar 21 16:46:54 2002
From: mmcclelland@eduprise.com (Marty McClelland)
Date: Thu, 21 Mar 2002 11:46:54 -0500
Subject: [OAI-implementers] Dublin Core XML and OAI
Message-ID: <937B9574F4A2A344812CB180F5B5D25E18A7EB@corpmail.eduprise.com>

> Also are there any standard subject classifications being used, or 
> are data providers creating their own?

I am also curious about standard subject classifications.  In iLumina,
the browse and advanced search features give views on the data by
discipline classification.  For example - Computer Science objects have
been cataloged based on ACM/IEEE Computing Curricula 2001 Classification
Scheme.  Metadata has been imported via OAI for Computer Science
learning resources using ACM Computing Classification System.

marty 

From lagoze@cs.cornell.edu  Thu Mar 21 19:05:44 2002
From: lagoze@cs.cornell.edu (Carl Lagoze)
Date: Thu, 21 Mar 2002 14:05:44 -0500
Subject: [OAI-implementers] Dublin Core XML and OAI
Message-ID: <706871B20764CD449DB0E8E3D81C4D4302A204F1@opus.cs.cornell.edu>

Alan,

I haven't looked at the 1.x oai_dc schema.  But, having spent some time
on the 2.0 oai_dc schema I can say that it dos not permit tags from any
other name space.  The intent here is that a harvester should not have
to prune out the dc elements from other arbitrary stuff.  Of course, a
data provider is permited to create another schema that contains other
stuff (mixed with dc elements) and export that as another metadata
format.

Carl

Carl Lagoze
Department of Computer Science
Cornell University
Ithaca, NY 14853 USA
Voice: +1-607-255-6046
FAX: +1-607-255-4428
EMail: lagoze@cs.cornell.edu
WWW: http://www.cs.cornell.edu/lagoze 

> -----Original Message-----
> From: Alan Kent [mailto:ajk@mds.rmit.edu.au] 
> Sent: Wednesday, March 20, 2002 10:02 PM
> To: OAI Implementors
> Subject: [OAI-implementers] Dublin Core XML and OAI
> 
> 
> Hi,
> 
> I was wondering what the exact position was with respect to 
> OAI and Dublin Core. For example, the OAI spec defines oai_dc 
> and an XML Schema specification for it. My question relates 
> to other (non-Dublin Core) elements that may be added.
> 
> For example, various groups have defined their own additional 
> elements and what they mean. They put these additional 
> elements in their own namespace. I am guessing it is not 
> correct for me to include these additional elements if oai_dc 
> is requested (I should prune them out). Each group should 
> then define their own oai_agls etc identifier (probably 
> without the oai_ prefix, if that is reserved for use by the protocol).
> 
> Alan
> _______________________________________________
> OAI-implementers mailing list OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 

From mln@ils.unc.edu  Thu Mar 21 19:33:49 2002
From: mln@ils.unc.edu (Michael L. Nelson)
Date: Thu, 21 Mar 2002 14:33:49 -0500 (EST)
Subject: [OAI-implementers] Dublin Core XML and OAI
In-Reply-To: <937B9574F4A2A344812CB180F5B5D25E18A7EB@corpmail.eduprise.com>
Message-ID: <Pine.GSO.4.21.0203211422010.17360-100000@ruby.ils.unc.edu>

On Thu, 21 Mar 2002, Marty McClelland wrote:

> > Also are there any standard subject classifications being used, or 
> > are data providers creating their own?
> 
> I am also curious about standard subject classifications.  In iLumina,
> the browse and advanced search features give views on the data by
> discipline classification.  For example - Computer Science objects have
> been cataloged based on ACM/IEEE Computing Curricula 2001 Classification
> Scheme.  Metadata has been imported via OAI for Computer Science
> learning resources using ACM Computing Classification System.

a quick look through:

http://arc.cs.odu.edu:8080/dp9/index.jsp
(for each repository: click on info, click on "ListSets")

shows that maybe 1/2 of the 60 DPs listed employ sets, and many of the
sets have the appearance of subject classification systems.

how many of them are "standard" is another question...  

to my end, http://techreports.larc.nasa.gov/ltrs/oai/ uses the NASA STI
Subject Categories:

ftp://ftp.sti.nasa.gov/pub/scan/SCAN-TOPICS

on the other hand, http://naca.larc.nasa.gov/oai/ has no subject
information, since NACA predates the NASA STI categories.  I've been
wondering if I should just declare all the NACA records in category 
"01 - Aeronautics (General)"; I'm not sure if that helps or hurts.

regards,

Michael

> 
> marty 
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 

---
Michael L. Nelson
NASA Langley Research Center		m.l.nelson@larc.nasa.gov
MS 158, Hampton, VA 23681		http://www.ils.unc.edu/~mln/
+1 757 864 8511				+1 757 864 8342 (f)


From mneedlem@dra.com  Thu Mar 21 17:09:02 2002
From: mneedlem@dra.com (Mark Needleman - DRA)
Date: Thu, 21 Mar 2002 11:09:02 -0600 (CST)
Subject: [OAI-implementers] Dublin Core XML and OAI
In-Reply-To: <937B9574F4A2A344812CB180F5B5D25E18A7EB@corpmail.eduprise.com>
Message-ID: <Pine.OSF.3.95.1020321110624.21142A-100000@tourist.dra.com>

Marty

A pretty standard set in the library world is the Library of Congress
Subject Headings (LCSH) - its widely used all over - the National Library
of Medicine has MESH (Medical Subject Headings) - and there are others out
there as well from national libraries around the world

these2 may be online try

www.loc.gov -LC

www.nlm.gov - NLM

Mark H Needleman
Product Manager - Standards
1276 North Warson Road
P.O. Box 8495
St Louis, MO 63132-1806
USA

Phone: 800 325-0888 (US/Canada)
       314 432-1100 x318
Fax: 314 993-8927
 
Email: markn@sirsi.com
 

On Thu, 21 Mar 2002, Marty McClelland wrote:

> > Also are there any standard subject classifications being used, or 
> > are data providers creating their own?
> 
> I am also curious about standard subject classifications.  In iLumina,
> the browse and advanced search features give views on the data by
> discipline classification.  For example - Computer Science objects have
> been cataloged based on ACM/IEEE Computing Curricula 2001 Classification
> Scheme.  Metadata has been imported via OAI for Computer Science
> learning resources using ACM Computing Classification System.
> 
> marty 
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 


From lagoze@cs.cornell.edu  Thu Mar 21 19:35:52 2002
From: lagoze@cs.cornell.edu (Carl Lagoze)
Date: Thu, 21 Mar 2002 14:35:52 -0500
Subject: [OAI-implementers] Progress update on Open Archives Initiative Protocol for Metadata Harvesting version 2.0
Message-ID: <706871B20764CD449DB0E8E3D81C4D4302A204F8@opus.cs.cornell.edu>

Dear all:

We'd like to bring you up-to-date on progress towards release of version
2.0 of the Open Archives Initiative Protocol for Metadata Harvesting
(OAI-PMH).  This is a follow-on to a previous message, archived at [1],
summarizing changes in the protocol.

As scheduled we completed the first alpha of the protocol document on
March 1.  This document was released to a group of alpha testers and the
OAI technical committee.  The last few weeks have been busy with
discussions among this group with the goal of fine tuning the revised
protocol.

The nature of the discussions in the alpha and technical groups and our
commitment to make version 2.0 a stable protocol version have led us to
a decision to push our protocol release date to June 3, 2002.  You may
recall from the earlier email [1] our decisions that:

1. Version 2.0 of the protocol will not be backwards compatible with
1.1.  Since the release of version 1.0, we have characterized the 1.x
releases as 'experimental'.  Results from the considerable amount of
experimentation over the past year mandate a number of changes that make
backward compatibility impractical. 2. Version 2.0 of the protocol is
NOT experimental and is meant to be a STABLE base of implementation.  

We feel that ensuring the credibility of these commitments is of primary
importance leading to the conclusion that extending the alpha test
period for version 2.0 is the proper decision.  An added benefit of
extending the alpha testing period is that there will be sufficient time
for important tools such as the Repository Explorer to be version 2.0
compatible by the time of the production release.

To summarize the release and migration schedule:

>>>>June 1, 2002

Production release of the following:

1. OAI-PMH version 2.0
2. Annotated OAI-PMH version 2.0 detailing changes from version 1.1 3.
OAI-PMH version 2.0 Usage Guidelines document. 4. OAI-PMH version 2.0
Registration and conformance testing software

>>>>September 1, 2002

Registered sites that have not upgraded will be sent a reminder that
they must upgrade.  As of this date, attempts to register at the OAI
registration site [2] MUST conform to OAI-PMH version 2.0

>>>>December 1, 2002

Sites that have not upgraded will be removed from the OAI registration
site.

As always thanks for your participation and support of the OAI
community.  

Carl Lagoze
Herbert Van de Sompel


[1]
http://www.openarchives.org/pipermail/oai-general/2002-February/000130.h
tml
[2] http://www.openarchives.org/data/registerasprovider.html



From liu_x@cs.odu.edu  Thu Mar 21 21:46:26 2002
From: liu_x@cs.odu.edu (Xiaoming Liu)
Date: Thu, 21 Mar 2002 16:46:26 -0500 (EST)
Subject: [OAI-implementers] Dublin Core XML and OAI
In-Reply-To: <937B9574F4A2A344812CB180F5B5D25E18A7EB@corpmail.eduprise.com>
Message-ID: <Pine.SOL.4.10.10203211621180.7923-100000@ren.cs.odu.edu>

Hi,

I have exported a preliminary statistic of metadata usage in
http://arc.cs.odu.edu/stat/index.html 

One might conclude from these data that although controlled vocabularies
are widely used, the schemas used are extremely variant.

regards,
liu



On Thu, 21 Mar 2002, Marty McClelland wrote:

> > Also are there any standard subject classifications being used, or 
> > are data providers creating their own?
> 
> I am also curious about standard subject classifications.  In iLumina,
> the browse and advanced search features give views on the data by
> discipline classification.  For example - Computer Science objects have
> been cataloged based on ACM/IEEE Computing Curricula 2001 Classification
> Scheme.  Metadata has been imported via OAI for Computer Science
> learning resources using ACM Computing Classification System.
> 
> marty 
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 


From hussein@vt.edu  Wed Mar 27 16:09:13 2002
From: hussein@vt.edu (Hussein Suleman)
Date: Wed, 27 Mar 2002 11:09:13 -0500
Subject: [OAI-implementers] list of rights statements
Message-ID: <3CA1EEA9.3010604@vt.edu>

hi

i think i saw someone post a list of rights statements extracted from 
OAI archives either here or somewhere else. does anybody remember where 
this was ? any pointers will be most appreciated.

thanx,
----hussein

-- 
======================================================================
hussein suleman - hussein@vt.edu - vtcs - http://www.husseinsspace.com
======================================================================


From liu_x@cs.odu.edu  Wed Mar 27 19:40:10 2002
From: liu_x@cs.odu.edu (Xiaoming Liu)
Date: Wed, 27 Mar 2002 14:40:10 -0500 (EST)
Subject: [OAI-implementers] list of rights statements
In-Reply-To: <3CA1EEA9.3010604@vt.edu>
Message-ID: <Pine.SOL.4.10.10203271437100.12225-100000@defiant.cs.odu.edu>

On Wed, 27 Mar 2002, Hussein Suleman wrote:

> hi
> 
> i think i saw someone post a list of rights statements extracted from 
> OAI archives either here or somewhere else. does anybody remember where 
> this was ? any pointers will be most appreciated.

Hussein, 

I extracted several metadata fields from Arc database sometimes ago, I am
not sure whether it's what you talked about. I therefore added rights
field and make it available at

http://arc.cs.odu.edu/stat/

regards,
liu
 

> 
> thanx,
> ----hussein
> 
> -- 
> ======================================================================
> hussein suleman - hussein@vt.edu - vtcs - http://www.husseinsspace.com
> ======================================================================
> 
> _______________________________________________
> OAI-implementers mailing list
> OAI-implementers@oaisrv.nsdl.cornell.edu
> http://oaisrv.nsdl.cornell.edu/mailman/listinfo/oai-implementers
> 


From Steven Bird <sb@ldc.upenn.edu>  Thu Mar 28 01:52:39 2002
From: Steven Bird <sb@ldc.upenn.edu> (Steven Bird)
Date: Wed, 27 Mar 2002 20:52:39 EST
Subject: [OAI-implementers] list of rights statements
In-Reply-To: Your mail dated Wednesday 27 March, 2002.
Message-ID: <200203280152.g2S1qdb21654@unagi.cis.upenn.edu>

Hussein,

> i think i saw someone post a list of rights statements extracted from
> OAI archives either here or somewhere else. does anybody remember where
> this was ? any pointers will be most appreciated.

Perhaps this was it:
  http://www.ldc.upenn.edu/sb/oai/rights.html

Yours,
Steven Bird

--
Steven.Bird@ldc.upenn.edu  http://www.ldc.upenn.edu/sb
Assoc Director, LDC; Adj Assoc Prof, CIS & Linguistics
Linguistic Data Consortium, University of Pennsylvania
3615 Market St, Suite 200, Philadelphia, PA 19104-2608



